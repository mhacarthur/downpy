{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit, minimize, fsolve\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "from scipy.special import gamma\n",
    "from scipy.integrate import dblquad, nquad\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "'''----------------------------------------------------------------------------\n",
    "Functions for downscaling gridded precipitation data.\n",
    "The main code is:\n",
    "\n",
    "Functions included here:\n",
    "-------------------------------------------------------------------------------\n",
    "matplotlib_update_settings\n",
    "haversine\n",
    "area_lat_long\n",
    "\n",
    "downscale_pwet\n",
    "compute_pwet_xr\n",
    "Taylor_beta\n",
    "\n",
    "corr\n",
    "str_exp_fun\n",
    "epl_fun\n",
    "myacf_2d\n",
    "grid_corr\n",
    "nabla_2d\n",
    "fast_corla_2d\n",
    "myfun_sse\n",
    "\n",
    "bin_ave_corr\n",
    "down_corr\n",
    "int_corr\n",
    "\n",
    "fit_wei\n",
    "fit_yearly_weibull\n",
    "vrf\n",
    "down_wei\n",
    "downscale\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "----------------------------------------------------------------------------'''\n",
    "\n",
    "\n",
    "def matplotlib_update_settings():\n",
    "    # http://wiki.scipy.org/Cookbook/Matplotlib/LaTeX_Examples\n",
    "    # this is a latex constant, don't change it.\n",
    "    pts_per_inch = 72.27\n",
    "    # write \"\\the\\textwidth\" (or \"\\showthe\\columnwidth\" for a 2 collumn text)\n",
    "    text_width_in_pts = 300.0\n",
    "    # inside a figure environment in latex, the result will be on the\n",
    "    # dvi/pdf next to the figure. See url above.\n",
    "    text_width_in_inches = text_width_in_pts / pts_per_inch\n",
    "    # make rectangles with a nice proportion\n",
    "    golden_ratio = 0.618\n",
    "    # figure.png or figure.eps will be intentionally larger, because it is prettier\n",
    "    inverse_latex_scale = 2\n",
    "    # when compiling latex code, use\n",
    "    # \\includegraphics[scale=(1/inverse_latex_scale)]{figure}\n",
    "    # we want the figure to occupy 2/3 (for example) of the text width\n",
    "    fig_proportion = (3.0 / 3.0)\n",
    "    csize = inverse_latex_scale * fig_proportion * text_width_in_inches\n",
    "    # always 1.0 on the first argument\n",
    "    fig_size = (1.0 * csize, 0.8 * csize)\n",
    "    # find out the fontsize of your latex text, and put it here\n",
    "    text_size = inverse_latex_scale * 12\n",
    "    tick_size = inverse_latex_scale * 8\n",
    "\n",
    "    # learn how to configure:\n",
    "    # http://matplotlib.sourceforge.net/users/customizing.html\n",
    "    params = {\n",
    "        'axes.labelsize': text_size,\n",
    "        'legend.fontsize': tick_size,\n",
    "        'legend.handlelength': 2.5,\n",
    "        'legend.borderaxespad': 0,\n",
    "        'xtick.labelsize': tick_size,\n",
    "        'ytick.labelsize': tick_size,\n",
    "        'font.size': text_size,\n",
    "        'text.usetex': True,\n",
    "        'figure.figsize': fig_size,\n",
    "        # include here any neede package for latex\n",
    "        'text.latex.preamble': [r'\\usepackage{amsmath}',\n",
    "                                ],\n",
    "    }\n",
    "    plt.rcParams.update(params)\n",
    "    return\n",
    "\n",
    "\n",
    "def haversine(lat1, lat2, lon1, lon2,\n",
    "              convert_to_rad=True):\n",
    "    '''compute haversine distance btw 2 points.\n",
    "    by default provide angles in degrees.\n",
    "    Return distance in km'''\n",
    "\n",
    "    def torad(theta):\n",
    "        # convert angle to radiants\n",
    "        return theta*np.pi/180.0\n",
    "\n",
    "    if convert_to_rad:\n",
    "        lat1 = torad(lat1)\n",
    "        lat2 = torad(lat2)\n",
    "        lon1 = torad(lon1)\n",
    "        lon2 = torad(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    R = 6371.0 # km\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1\n",
    "               )*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    dist = 2*R*np.arctan2( np.sqrt(a), np.sqrt(1-a))\n",
    "    return dist\n",
    "\n",
    "\n",
    "def area_lat_long(lat_c, lon_c, dlat, dlon):\n",
    "    \"\"\"----------------------------------------------------------\n",
    "    INPUTS: latitude, longitude of pixel center\n",
    "    and delta lat, delta long of pixel size.\n",
    "    Everything in degrees\n",
    "    RETURNS::\n",
    "    my_edge: characteristic pixel size ( L = (L1*L2)**0.5 ) [km]\n",
    "    my_area: pixel area A = L1*L2 [km**2]\n",
    "    hor_size: horizontal pixel size, L1 [km]\n",
    "    vert_size: vertical pixel size, L2 [km]\n",
    "    --------------------------------------------------------------\"\"\"\n",
    "    lat1 = lat_c - dlat/2\n",
    "    lat2 = lat_c + dlat/2\n",
    "    lon1 = lon_c - dlon/2\n",
    "    lon2 = lon_c + dlon/2\n",
    "    hor_size = haversine(lat1, lat1, lon1, lon2)\n",
    "    vert_size = haversine(lat1, lat2, lon1, lon1)\n",
    "    my_area = hor_size*vert_size\n",
    "    my_edge = np.sqrt(my_area)\n",
    "    return my_edge, my_area, hor_size, vert_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def downscale_pwet(xdata, *, thresh=1, dt=3, L1=25,\n",
    "                   target_x=0.0001, target_t=24,\n",
    "                   origin_x=25, origin_t=24, ninterp=1000, plot=False):\n",
    "    '''------------------------------------------------------------------------\n",
    "    Use a Taylor hypothesis to trade space for time and give an estimate\n",
    "    of the wet fraction above a certain threshold at a spatial scale smaller\n",
    "    that the grid cell size.\n",
    "\n",
    "    INPUT:\n",
    "        xdata: X-ARRAY with the precipitation ACCUMULATIONS at scale dt\n",
    "               must already be loaded in memory\n",
    "               negative values must be already removed and set to np.nan\n",
    "               has dimensions (lat, lon, time)\n",
    "               time step MUST BE between 0.5 and 3 HRS (IMERG / TMPA)\n",
    "               must be square (same dimension in x and y)\n",
    "               ***SEE FUNCTION CALLED BELOW FOR MORE INFORMATION***\n",
    "\n",
    "        thresh: threshold for computing wet fraction (default 1 prcp unit)\n",
    "        dt: time scale of the precipitation product [HOURS] (default 3 hours)\n",
    "        L1: linear spatial scale of a grid cell [km] (default is 25 km)\n",
    "        target_x: subgrid spatial scale we want pwet at [km](default 0.0001 km)\n",
    "        target_t: target time scale, in [HOURS] (default 24 hours)\n",
    "        origin_x: linear spatial scale of origin gridded prcp [km] (default 25)\n",
    "        origin_t: time scale of origin gridded pecip [HOURS] (default 24 hours)\n",
    "        ninterp=1000: number of interpolation in time dimension\n",
    "        plot=False: only True if you wanna see fancy plots\n",
    "    OUTPUT:\n",
    "        beta: ratio between pwet at the grid cell scale ()\n",
    "              to pwet at the target subgrid scale ()\n",
    "\n",
    "    NOTE: TESTED ONLY WITH TRMM - TMPA - 3B42 - 3 hourly * 0.25 deg product!\n",
    "          FOR PRECIPITATION ACCUMULATION TARGETS AT THE DAILY SCALE\n",
    "    ------------------------------------------------------------------------'''\n",
    "    pwets, xscales, tscales = compute_pwet_xr(xdata, thresh,\n",
    "                                    cube1size=3, dt=dt, tmax=48)\n",
    "    resbeta = Taylor_beta(pwets, xscales, tscales, L1=L1,\n",
    "                            target_x=target_x, target_t=target_t,\n",
    "                            origin_x=origin_x, origin_t=origin_t,\n",
    "                            ninterp=ninterp, plot=plot)\n",
    "    return resbeta\n",
    "\n",
    "\n",
    "def compute_pwet_xr(xray, thresh, *,\n",
    "                    cube1size=3, dt=3, tmax=48):\n",
    "    '''-----------------------------------------------------------------------\n",
    "    Compute the fraction of observations above a given threshold for\n",
    "    different integration scales (in time) and averaging scales (space)\n",
    "\n",
    "    INPUT:  xray: xarray with dimensions lat, lon, time\n",
    "                  (must be already loaded in memory,\n",
    "                  and missing values must be converted to np.nan in advance:\n",
    "                  here np.nan are propagated until the end when integrating.\n",
    "                  time step MUST BE between 0.5 and 3 HRS (IMERG / TMPA)\n",
    "                  Dimension in pixel of LAT and LON must be equal (square)\n",
    "                  and sufficiently long record in time to compute reliably.\n",
    "\n",
    "\n",
    "            thresh:  threshold for determining wet fraction\n",
    "            cube1size = 1: lateral size of cube used to decide\n",
    "                       among how many single pixel average at smallest scale\n",
    "            dt = 3: temporal resolution of observations [HOURS]\n",
    "            tmax = 48: maximum time scale of integration [HOURS]\n",
    "\n",
    "\n",
    "    OUTPUT: pwets: numpy array of shape tscales*xscales with the values\n",
    "                   of wet fraction at different time / space scales\n",
    "            xscales: spatial scales (dimensionless, relative to pixel size)\n",
    "            tscales: temporal scales (in HOURS!)\n",
    "\n",
    "    Note: at the smallest scale (xscale=1), values are computed for each\n",
    "          time series within a centered cube of size cube1size,\n",
    "          and then averaged.\n",
    "\n",
    "          At the largest spatial scale, spatial average includes all the\n",
    "          pixels in the array\n",
    "\n",
    "          at intermediate scales, the wet fraction is computed for 4 boxes\n",
    "          starting at the 4 corners of the lattice, and averaged.\n",
    "    -----------------------------------------------------------------------'''\n",
    "\n",
    "    smax = xray.shape[0] # max spatial scale\n",
    "    tscales = np.array([1, 2, 3, 4, 5, 6, 8, 10, 12, 16, 20,\n",
    "                                        24, 36, 48, 96])*dt\n",
    "    tscales = tscales[tscales < tmax + 0.001]\n",
    "    xscales = np.arange(1, smax+1)\n",
    "    ntscales = np.size(tscales)\n",
    "    nsscales = np.size(xscales)\n",
    "    pwets = np.zeros((ntscales, nsscales))\n",
    "\n",
    "    def wetfrac(array, thresh):\n",
    "        return np.size(array[array > thresh])/np.size(array)\n",
    "\n",
    "    for it, st in enumerate(tscales):\n",
    "        datamat = xray.resample(time='{}h'.format(st)).sum(\n",
    "                                    dim='time', skipna = False)\n",
    "\n",
    "        for ix, sx in enumerate(xscales):\n",
    "\n",
    "            if sx == 1: # pixel scale\n",
    "\n",
    "                toskip = smax - cube1size\n",
    "                if toskip % 2 == 0:\n",
    "                    buffer1 = toskip // 2\n",
    "                    buffer2 = toskip // 2\n",
    "                else:\n",
    "                    buffer1 = toskip // 2\n",
    "                    buffer2 = buffer1 + 1\n",
    "\n",
    "                if buffer2 > 0:\n",
    "                    aggt1 = datamat[buffer1:-buffer2, buffer1:-buffer2, :]\n",
    "                else:\n",
    "                    aggt1 = datamat[buffer1:, buffer1:, :]\n",
    "\n",
    "                # print('shape of p1 array = ', aggt1.shape)\n",
    "\n",
    "                p1 = np.zeros((aggt1.shape[0], aggt1.shape[1]))\n",
    "\n",
    "\n",
    "                for x in range(aggt1.shape[0]):\n",
    "                    for y in range(aggt1.shape[1]):\n",
    "                        # print('Hello1')\n",
    "                        p1[x, y] = wetfrac(aggt1[x, y, :].dropna(\n",
    "                                            dim='time', how='any'), thresh)\n",
    "                pwets[it, ix] = np.mean(p1)\n",
    "\n",
    "\n",
    "            elif sx == smax: # largest scale: simple average\n",
    "                pwets[it, ix] = wetfrac( datamat.mean(dim=('lat', 'lon'),\n",
    "                              skipna = False).dropna(dim='time', how='any'),\n",
    "                              thresh)\n",
    "\n",
    "            else: # intermediate scales\n",
    "                c1 = np.zeros(4)\n",
    "                c1[0] = wetfrac(datamat[:sx, :sx, :].mean(dim=('lat', 'lon'),\n",
    "                               skipna=False).dropna(dim='time', how='any'),\n",
    "                                thresh)\n",
    "                c1[1] = wetfrac(datamat[-sx:, :sx, :].mean(dim=('lat', 'lon'),\n",
    "                               skipna=False).dropna(dim='time', how='any'),\n",
    "                                thresh)\n",
    "                c1[2] = wetfrac(datamat[:sx, :sx, :].mean(dim=('lat', 'lon'),\n",
    "                               skipna=False).dropna(dim='time', how='any'),\n",
    "                                thresh)\n",
    "                c1[3] = wetfrac(datamat[-sx:, :sx, :].mean(dim=('lat', 'lon'),\n",
    "                               skipna=False).dropna(dim='time', how='any'),\n",
    "                                thresh)\n",
    "                pwets[it, ix] = np.mean(c1)\n",
    "    return pwets, xscales, tscales\n",
    "\n",
    "\n",
    "def Taylor_beta(pwets, xscales, tscales, *, L1=25,target_x=0.001, target_t=24,\n",
    "                      origin_x=25, origin_t=24, ninterp = 1000, plot=False):\n",
    "    '''------------------------------------------------------------------\n",
    "    Extrapolate the wet fraction of the rainfall fields at small scales\n",
    "    smaller than the resolution of the gridded precipitation product.\n",
    "    INPUT:\n",
    "        pwets: array of wet fractions for different integration scales.\n",
    "               Must have shape tscales*xscales\n",
    "        xscales: array of spatial scales (DIMENSIONLESS, in pixel units!!!)\n",
    "        tscales: array of temporal scales (DIMENSIONAL, in [HOURS])\n",
    "        L1: pixel linear size, in [Km]\n",
    "        target_x: linear scale of target, in [Km] (e.g.,rain gauge measurem.)\n",
    "        target_t: time scale of target, in [HOURS] (default 24 hours)\n",
    "        origin_x: origin linear spatial scale, in [Km] (default 25 Km)\n",
    "        origin_t: origin time scale, in [Hours] (default 24 hours)\n",
    "        ninterp: number of point for interpolation in time (default = 1000)\n",
    "        plot: if plot == True, produces some nice plots (default is false)\n",
    "\n",
    "\n",
    "    OUTPUT:\n",
    "        beta -> ratio between the pwet at scales (origin_x, origin_t)\n",
    "                e.g., known satellite pixel aggregated at the daily scale,\n",
    "                to pwet at a smaller spatial scale (target_x, target_y)\n",
    "\n",
    "    NOTE:\n",
    "        For now: The origin_x scale must be one where the pwet is known,\n",
    "        so one of the values in xscales. Otherwise an additional\n",
    "        interpolation is necessary (furure release)\n",
    "\n",
    "    NOTE: TESTED ONLY WITH TRMM - TMPA - 3B42 - 3 hourly * 0.25 deg product!\n",
    "    ------------------------------------------------------------------'''\n",
    "    xscales_km = xscales*L1\n",
    "    ntscales = np.size(tscales)\n",
    "    nxscales = np.size(xscales)\n",
    "    tscales_int = np.linspace(np.min(tscales), np.max(tscales), ninterp)\n",
    "    pwmat_int = np.zeros((ninterp, nxscales))\n",
    "    for col in range(nxscales):\n",
    "        pwmat_int[:, col] = np.interp(tscales_int, tscales, pwets[:, col])\n",
    "\n",
    "    # dxv, dtv = np.meshgrid(xscales_km, tscales_int )\n",
    "    pw_min = np.min(pwets)\n",
    "    pw_max = np.max(pwets)\n",
    "    mypw = np.linspace(pw_min, pw_max, ninterp)\n",
    "\n",
    "    myU = np.zeros(ninterp)   # initialize linear slope\n",
    "    myX0 = np.zeros(ninterp)  # initialize linear intercept\n",
    "    myindices = np.zeros((ninterp, nxscales), dtype = int)\n",
    "    # tvec =  np.zeros((ninterp, nxscales))\n",
    "\n",
    "    for ii in range(ninterp):\n",
    "        Tvec = np.zeros(nxscales)\n",
    "        for jj in range(nxscales):\n",
    "            myindices[ii,jj] = np.argmin(np.abs(pwmat_int[:, jj] - mypw[ii]))\n",
    "            # tvec[ii,jj] = tscales_int[myindices[ii,jj]]\n",
    "            Tvec[jj] = tscales_int[myindices[ii,jj]]\n",
    "        warnings.simplefilter('ignore', np.RankWarning)\n",
    "        res = np.polyfit(Tvec[:2], xscales_km[:2], 1)\n",
    "        myU[ii] = res[0]\n",
    "        myX0[ii] = res[1]\n",
    "\n",
    "    # first remove timescales where the line goes out of bounds\n",
    "    min_index = myindices.min(axis=1)\n",
    "    max_index = myindices.max(axis=1)\n",
    "    col_to_keep = np.logical_and(min_index > 0, max_index < ninterp-1)\n",
    "    mypw2 = mypw[col_to_keep]\n",
    "    U = myU[col_to_keep]\n",
    "    X0 = myX0[col_to_keep]\n",
    "\n",
    "    # best_index = this is the index of the pwet such that\n",
    "    # the line extrapolated to the gauge spatial scale\n",
    "    # matches the requested time scale\n",
    "    dthat = (target_x - X0) / U # compute time scale that would give me pwet\n",
    "    deltat = np.abs(dthat - target_t)\n",
    "    opt_deltat = np.min(deltat)\n",
    "    max_dt = 0.5 # half hour accuracy should suffice?\n",
    "    # set a TOLL value to check we are not too far away from the real deltat\n",
    "    if opt_deltat > max_dt:\n",
    "        print('Taylor_Beta WARNING: not enough accuracy!')\n",
    "        print('the time resolution of the coarse data might not be enough!')\n",
    "        print('or try to increase the value of interp')\n",
    "    best_index = np.argmin(deltat)  # best prediction of 24 hour interval\n",
    "    pwet_target = mypw2[best_index]\n",
    "\n",
    "    # compute pwet at the original scale:\n",
    "    # this is ok if origin_x is one of the points where pwet was computed\n",
    "    # if not, add an interpolation in the x direction!\n",
    "    pos_xmin = np.argmin(np.abs(origin_x - xscales_km))\n",
    "    pos_tmin = np.argmin(np.abs(origin_t - tscales_int))\n",
    "    pwet_origin = pwmat_int[pos_tmin, pos_xmin]\n",
    "    beta = pwet_origin / pwet_target\n",
    "    # return beta\n",
    "    res = {}\n",
    "    res['beta'] = beta\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(tscales, pwets[:,0], 'ok')\n",
    "        plt.plot(tscales, pwets[:,1], 'ob')\n",
    "        plt.plot(tscales_int, pwmat_int[:,0], '-k', label = 'dx')\n",
    "        plt.plot(tscales_int, pwmat_int[:,1], '-b', label = '2dx')\n",
    "        if nxscales > 2:\n",
    "            plt.plot(tscales, pwets[:,2], 'og')\n",
    "            plt.plot(tscales_int, pwmat_int[:,2], '-g', label = '3dx')\n",
    "        plt.xlabel('Time scale [hours]')\n",
    "        plt.ylabel('wet fraction')\n",
    "        plt.title('Observed wet fraction')\n",
    "        plt.legend()\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        res['fig'] = fig\n",
    "\n",
    "        # PLOT CONTOUR\n",
    "        xxx1 = np.linspace(0, 50, 10)\n",
    "        xxx2 = np.linspace(50, 75, 10)\n",
    "        contour = plt.figure()\n",
    "        # PS3 = plt.contourf(dxv, dtv, Nmatv)\n",
    "        PS3 = plt.contourf(xscales_km, tscales_int, pwmat_int)\n",
    "        cbar = plt.colorbar(PS3)\n",
    "        cbar.set_label(r'$p_r$')\n",
    "        cbar.set_label(r'$p_r$', rotation=270)\n",
    "        for ii in range(nxscales):\n",
    "            for jj in range(ntscales):\n",
    "                plt.plot(xscales_km[ii], tscales[jj], 'sk')\n",
    "        plt.plot(xxx1, (xxx1 - X0[best_index]) / U[best_index], 'b')\n",
    "        plt.plot(xxx2, (xxx2 - X0[best_index]) / U[best_index], '--b')\n",
    "        plt.plot(target_x, target_t, 'sr')\n",
    "        plt.plot(origin_x, origin_t, '^r')\n",
    "        plt.xlabel('spatial scale [km]')\n",
    "        plt.ylabel('temporal scale [hours]')\n",
    "        plt.title('Wet Fraction Extrapolation')\n",
    "        plt.text(0.1, 24.5, 'Gauge', color='red')\n",
    "        plt.text(L1, 24.5, 'Grid cell', color='red')\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        res['contour'] = contour\n",
    "    return res\n",
    "\n",
    "\n",
    "################################ CORRELATION ##################################\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def corr(x, y):\n",
    "    '''correlation between 2 numpy arrays'''\n",
    "    xn = (x - np.mean(x))/np.std(x)\n",
    "    yn = (y - np.mean(y))/np.std(y)\n",
    "    pearson_corr = np.mean(xn*yn)\n",
    "    return pearson_corr\n",
    "\n",
    "\n",
    "def str_exp_fun(x, d0, mu):\n",
    "    '''\n",
    "    Stretched exponential rainfall correlation function\n",
    "    from eg Habib and krajewski (2003)\n",
    "    or Villarini and Krajewski (2007)\n",
    "    with 2 parameters d0, mu (value as x = 0 is 1)\n",
    "    '''\n",
    "    x = np.asarray(x) # transform to numpy array\n",
    "    is_scalar = False if x.ndim > 0 else True # create flag for output\n",
    "    x.shape = (1,)*(1-x.ndim) + x.shape # give it dimension 1 if scalar\n",
    "    myfun = np.exp( -(x/d0)**mu)\n",
    "    myfun = myfun if not is_scalar else myfun[0]\n",
    "    return  myfun\n",
    "\n",
    "\n",
    "def epl_fun(x, epsilon, alpha):\n",
    "    '''\n",
    "    Marco's Exponential kernel + Power law tail\n",
    "    (autocorrelation) function with exponential nucleus and power law decay\n",
    "    for x> epsilon - 2 parameters\n",
    "    see Marani 2003 WRR for more details\n",
    "    '''\n",
    "    x = np.asarray(x) # transform to numpy array\n",
    "    is_scalar = False if x.ndim > 0 else True # create flag for output\n",
    "    x.shape = (1,)*(1-x.ndim) + x.shape # give it dimension 1 if scalar\n",
    "    m = np.size(x)\n",
    "    myfun = np.zeros(m)\n",
    "    for ii in range(m):\n",
    "        if x[ii] < 10e-6:\n",
    "            myfun[ii] = 1\n",
    "        elif x[ii] < epsilon:\n",
    "            myfun[ii] = np.exp(-alpha*x[ii]/epsilon)\n",
    "        else:\n",
    "            # silence it if negative parameters -\n",
    "            # with warnings.simplefilter('ignore', RuntimeWarning):\n",
    "            myfun[ii] = (epsilon/np.exp(1)/x[ii])**alpha\n",
    "            # print('epsilon = ', epsilon)\n",
    "            # if epsilon < 1e-5 or alpha < 1e-5:\n",
    "            #     print('epsilon = ', epsilon)\n",
    "            #     print('alpha =', alpha)\n",
    "            # else:\n",
    "    myfun = myfun if not is_scalar else myfun[0]\n",
    "    return  myfun\n",
    "\n",
    "\n",
    "def myacf_2d(x,y, parhat, acf = 'str'):\n",
    "    '''########################################################################\n",
    "    set of 2D autocorrelation functions\n",
    "    INPUTS::\n",
    "        x,y = variables of the ACF\n",
    "        parhat = set of the two parameters of the ACF\n",
    "        acf: which ACF. possible choices:\n",
    "            acf = 'str': stretched exponential ACF,\n",
    "                WITH PARAMETERS: scale d0 and shape mu\n",
    "            acf = 'mar': Marani 2003 exponential kernel + power law tail\n",
    "                WITH PARAMETERS: transition point epsilon and shape alpha\n",
    "            acf 'exp': 2d exponential function\n",
    "                WITH PARAMETERS: a and b --> scale in x and y axis respectively\n",
    "    OUTPUTS::\n",
    "        value of the ACF at a point\n",
    "    ########################################################################'''\n",
    "    d = np.sqrt(x**2 + y**2)\n",
    "    if acf == 'str': # par d0, mu0\n",
    "        d0 = parhat[0]\n",
    "        mu = parhat[1]\n",
    "        return np.exp( -(d/d0)**mu)\n",
    "    elif acf == 'mar': # par:: epsilon, alpha\n",
    "        # print('it actually is a Power Law')\n",
    "        epsilon = parhat[0]\n",
    "        alpha   = parhat[1]\n",
    "        return epl_fun(d, epsilon , alpha)\n",
    "\n",
    "\n",
    "def grid_corr(xdata, plot=True, thresh=0):\n",
    "    '''------------------------------------------------------------------------\n",
    "    Given an xarray with the data (dimensions: lat, lon, time)\n",
    "    computes the correlation between each couple of grid cells\n",
    "    and fit a correlation model.\n",
    "    Note: negative values for missing data must be set to NaN in advance\n",
    "    ------------------------------------------------------------------------'''\n",
    "    xdata = xdata.dropna(dim='time', how='any')\n",
    "    lats = xdata.lat.values\n",
    "    lons = xdata.lon.values\n",
    "    nlats = np.size(lats)\n",
    "    nlons = np.size(lons)\n",
    "    nelem = nlats*nlons\n",
    "    lats9 = np.repeat(lats, nlons)\n",
    "    lons9 = np.tile(lons, nlats)\n",
    "    ncorr = (nelem)*(nelem - 1)//2\n",
    "    vcorr = np.zeros(ncorr)\n",
    "    vdist = np.zeros(ncorr)\n",
    "    count = 0\n",
    "    for i in range(nelem):\n",
    "        tsi = xdata.loc[dict(lat=lats9[i], lon=lons9[i])].values\n",
    "        tsi = np.maximum(tsi-thresh, 0.0)\n",
    "        for j in range(i+1, nelem):\n",
    "            tsj = xdata.loc[dict(lat=lats9[j], lon=lons9[j])].values\n",
    "            tsj = np.maximum(tsj-thresh, 0.0)\n",
    "            vcorr[count] = corr(tsi, tsj)\n",
    "            vdist[count] = haversine(lats9[i], lats9[j], lons9[i], lons9[j])\n",
    "            count = count + 1\n",
    "    res = {}\n",
    "    res['vdist'] = vdist\n",
    "    res['vcorr'] = vcorr\n",
    "    xx = np.linspace(np.min(vdist), np.max(vdist), 20)\n",
    "    # fit curves to observed correlation\n",
    "    popt, pcov = curve_fit(str_exp_fun, vdist, vcorr, p0=np.array([50, 1]),\n",
    "               bounds=(np.array([0.0, 0.0]), np.array([+np.inf, +np.inf])))\n",
    "    res['d0_s']= popt[0]\n",
    "    res['mu0_s'] = popt[1]\n",
    "    popt1, pcov1 = curve_fit(epl_fun, vdist, vcorr, p0=np.array([50, 1]),\n",
    "               bounds=(np.array([0.0, 0.0]), np.array([+np.inf, +np.inf])))\n",
    "    res['eps_s'] = popt1[0]\n",
    "    res['alp_s'] = popt1[1]\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(vdist, vcorr, 'o', label='empirical')\n",
    "        plt.plot(xx, str_exp_fun(xx, res['d0_s'], res['mu0_s']), 'r',\n",
    "                                             label='Stretched exp.')\n",
    "        plt.plot(xx, epl_fun(xx, res['eps_s'], res['alp_s']), 'g',\n",
    "                                             label='Exp.-power law')\n",
    "        plt.xlabel('distance [Km]')\n",
    "        plt.ylabel('correlation [-]')\n",
    "        plt.ylim([0, 1])\n",
    "        plt.xlim([min(vdist), max(vdist)])\n",
    "        plt.legend()\n",
    "        plt.close()\n",
    "        res['fig'] = fig\n",
    "    return res\n",
    "\n",
    "\n",
    "def nabla_2d(par_acf, myacf, T1, T2, err_min = 1e-2):\n",
    "    '''########################################################################\n",
    "    Compute the variance function as in Vanmarcke's book.\n",
    "    INPUTS::\n",
    "        par_acf = tuple with the parameters of the autocorr function (ACF)\n",
    "        myacf = ACF in 1,2,or 3 dimensions, with parameters in par_acf\n",
    "        T1 = 1st dimension of averaging area\n",
    "        T2 = 2nd dim of the averaging area'''\n",
    "    if (T1 == 0) or (T2 == 0):\n",
    "        print('integration domain is zero')\n",
    "        return 0.0 # integral is zero in this case.\n",
    "    else:\n",
    "        fun_XY = lambda x ,y: (T1 - x ) *(T2 - y ) *myacf(x ,y, par_acf)\n",
    "        myint, myerr = nquad(fun_XY, [[0. ,T1], [0. ,T2]])\n",
    "        # if myint != 0.0:\n",
    "        #     rel_err = myerr /myint\n",
    "        # else:\n",
    "        #     rel_err = 0\n",
    "        # if rel_err > err_min:\n",
    "        #     print('varfun ERROR --> Numeric Integration scheme does not converge')\n",
    "        #     print('int rel error = ', rel_err)\n",
    "        #     sys.exit(\"aargh! there are errors!\") # to stop code execution\n",
    "        return 4.0 * myint\n",
    "\n",
    "\n",
    "def fast_corla_2d(par_acf, myacf, Tx, L, err_min=1e-2):\n",
    "    nab_1 = nabla_2d(par_acf, myacf, L, Tx[0], err_min=err_min)\n",
    "    nab_2 = nabla_2d(par_acf, myacf, L, Tx[1], err_min=err_min)\n",
    "    nab_3 = nabla_2d(par_acf, myacf, L, Tx[2], err_min=err_min)\n",
    "    nab_den = nabla_2d(par_acf, myacf, L, L, err_min=err_min)\n",
    "    if np.abs(nab_den) < 10e-6: # to avoid infinities\n",
    "        # print('correcting - inf value')\n",
    "        nab_den = 10e-6\n",
    "    covla = 2*(nab_1 -2*nab_2 + nab_3)/(4*nab_den)\n",
    "    # print('parhat =', par_acf)\n",
    "    return covla\n",
    "\n",
    "\n",
    "def myfun_sse(xx, yobs, parhat, L, acf = 'mar'):\n",
    "    xx = np.asarray(xx)\n",
    "    myacf = lambda x, y, parhat: myacf_2d(x, y, parhat, acf=acf)\n",
    "    # Ty = np.array([L, 0., L, 0.])\n",
    "    sse = 0\n",
    "    m = np.size(xx)\n",
    "    for ii in range(m):\n",
    "        myx = xx[ii]\n",
    "        Tx = np.array([np.abs(L - myx), myx, L + myx, myx])\n",
    "        # L2 = [L, L]\n",
    "        # res = corla_2d(L2, L2, parhat, myacf, Tx, Ty, err_min=1e-5)\n",
    "        # faster option: - same result (optimized)\n",
    "        res = fast_corla_2d(parhat, myacf, Tx, L, err_min=1e-2)\n",
    "        sse = sse + (res - yobs[ii]) ** 2\n",
    "        # sse = sse + ((res - yobs[ii])/yobs[ii]) ** 2 # norm\n",
    "    # print('sse', sse)\n",
    "    # print('parhat =', parhat)\n",
    "    return sse\n",
    "\n",
    "\n",
    "def bin_ave_corr(vdist, vcorr, toll=0.3, plot=False):\n",
    "    ''' compute block averages to approximate\n",
    "     the empirical correlation function'''\n",
    "    vd = np.sort(vdist)\n",
    "    cd = vcorr[np.argsort(vdist)]\n",
    "    m = np.size(vd)\n",
    "    cluster = np.zeros(m)\n",
    "    count = 0\n",
    "    for i in range(1, m):\n",
    "        if np.abs(vd[i]-vd[i-1]) < toll:\n",
    "            cluster[i] = count\n",
    "        else:\n",
    "            count = count + 1\n",
    "            cluster[i] = count\n",
    "    clust = set(cluster)\n",
    "    nclust = len(clust)\n",
    "    vdist_ave = np.zeros(nclust)\n",
    "    vcorr_ave = np.zeros(nclust)\n",
    "    for ei, elem in enumerate(clust):\n",
    "        di = vd[cluster==elem]\n",
    "        ci = cd[cluster==elem]\n",
    "        vdist_ave[ei] = np.mean(di)\n",
    "        vcorr_ave[ei] = np.mean(ci)\n",
    "    res = {}\n",
    "    res['vdist_ave'] = vdist_ave\n",
    "    res['vcorr_ave'] = vcorr_ave\n",
    "    res['vd'] = vd\n",
    "    res['cd'] = cd\n",
    "    res['cluster']  = cluster\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        plt.scatter(vd, cd, c=cluster, cmap='jet')\n",
    "        plt.scatter(res['vdist_ave'], res['vcorr_ave'], c='k')\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        res['fig'] = fig\n",
    "    return res\n",
    "\n",
    "\n",
    "def down_corr(vdist, vcorr, L1, *, acf='mar',\n",
    "              use_ave=True, opt_method = 'genetic', disp=True, toll=0.005,\n",
    "              plot=False):\n",
    "    '''------------------------------------------------------------------------\n",
    "    Downscale the correlation function obtained from spatial averages\n",
    "    INPUT:\n",
    "        vdist = array of distances [Km]\n",
    "        vcorr = array of correlation values [-]\n",
    "        L1 = linear scale of spatial averaging (e.g., grid cell resolution)\n",
    "        acf = type of acf used (default 'mar'). Can be 'mar' or 'str'\n",
    "        use_ave = True. If true use binned average value of the correlation\n",
    "                  function instead of the  actual values\n",
    "                  (faster and more stable numerically)\n",
    "        method='genetic'. Method used for the optimization.\n",
    "               'genetic' -> for a genetic algorithm (suggested, default)\n",
    "               'lbfgsb' -> for the L-BFGS-B algoithm.\n",
    "                        This is only LOCAL and could get stuck in local minima.\n",
    "        disp= True: display optimization status\n",
    "    ------------------------------------------------------------------------'''\n",
    "    res = {}\n",
    "    parnames = ['eps_d', 'alp_d'] if acf == 'mar' else ['d0_d', 'mu0_d']\n",
    "    if not use_ave:\n",
    "        def myfun(pardown):\n",
    "            return myfun_sse(vdist, vcorr, pardown, L1, acf=acf)\n",
    "    else:\n",
    "        res_ave = bin_ave_corr(vdist, vcorr, toll = 0.3, plot=plot)\n",
    "        dd = res_ave['vdist_ave']\n",
    "        cc = res_ave['vcorr_ave']\n",
    "        def myfun(pardown):\n",
    "            return myfun_sse(dd, cc, pardown, L1, acf=acf)\n",
    "    if opt_method == 'lbfgsb':\n",
    "        x0 = (50, 1)  # initial guess\n",
    "        resmin = minimize(myfun, x0, method=\"L-BFGS-B\",\n",
    "                               bounds=((0, 2000), (0, 10)),\n",
    "                               options={'gtol': 1e-8, 'disp': True})\n",
    "        res[parnames[0]] = resmin.x[0]\n",
    "        res[parnames[1]] = resmin.x[1]\n",
    "        res['success'] = resmin.success\n",
    "        res['fuvval'] = resmin.fun\n",
    "\n",
    "    elif opt_method == 'genetic':\n",
    "        bounds = [(0.0, 200.0),(0.0, 1.00)]\n",
    "        resmin = differential_evolution(myfun, bounds, disp=disp,\n",
    "                                          tol = toll, atol = toll)\n",
    "        res[parnames[0]] = resmin.x[0]\n",
    "        res[parnames[1]] = resmin.x[1]\n",
    "        res['success'] = resmin.success\n",
    "        res['funval'] = resmin.fun\n",
    "    else:\n",
    "        print('down_corr ERROR: please insert a valid optimization method')\n",
    "        print('downscaling not performed')\n",
    "        res[parnames[0]] = np.nan\n",
    "        res[parnames[1]] = np.nan\n",
    "        res['success'] = False\n",
    "        res['funval'] = -9999\n",
    "    if plot:\n",
    "        xx = np.linspace(0.0, 100)\n",
    "        corrL = int_corr(xx, (res[parnames[0]], res[parnames[1]]), acf, L1)\n",
    "        fig = plt.figure()\n",
    "        plt.plot(vdist, vcorr, 'or', label='empirical correlation')\n",
    "        if use_ave:\n",
    "            plt.plot(dd, cc, 'sk', label='binned correlation')\n",
    "        plt.plot(xx, corrL, 'sc', label='integrated correlation')\n",
    "        if acf=='str':\n",
    "            plt.plot(xx, str_exp_fun(xx, res[parnames[0]],\n",
    "                     res[parnames[1]]), 'r', label='Stretched exp.')\n",
    "        else:\n",
    "            plt.plot(xx, epl_fun(xx, res[parnames[0]],\n",
    "                     res[parnames[1]]), 'g', label='Exp.-power law')\n",
    "        plt.xlabel('distance [Km]')\n",
    "        plt.ylabel('correlation [-]')\n",
    "        plt.ylim([0.4, 1])\n",
    "        plt.xlim([0.0, max(100, np.max(vdist))])\n",
    "        plt.legend()\n",
    "        plt.title('Downscaling correlation function')\n",
    "        plt.close()\n",
    "        res['fig'] = fig\n",
    "    return res\n",
    "\n",
    "\n",
    "def int_corr(xx, parhat, acf, L):\n",
    "    '''------------------------------------------------------------------------\n",
    "    compute correlation of Local averages\n",
    "    (inverse than the down_corr function)\n",
    "    ------------------------------------------------------------------------'''\n",
    "    m = np.size(xx)\n",
    "    corrL = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        Tx = np.array([np.abs(L - xx[i]), xx[i], L + xx[i], xx[i]])\n",
    "        myacf = lambda x, y, parhat: myacf_2d(x, y, parhat, acf=acf)\n",
    "        corrL[i] = fast_corla_2d(parhat, myacf, Tx, L, err_min=1e-2)\n",
    "    return corrL\n",
    "\n",
    "\n",
    "def vrf(L, L0, par_acf, acf='mar'):\n",
    "    '''-------------------------------------------------------------\n",
    "    compute the variance reduction factor\n",
    "    between scales L (large) and L0 (small)\n",
    "    defined as Var[L]/Var[L0]\n",
    "    INPUT:\n",
    "        L [km]\n",
    "        L0 [Km]\n",
    "        parhat = tuple with ACF parameters (eg epsilon, alpha)\n",
    "        acf='mar' type of acf (mar or str available)\n",
    "    OUTPUT:\n",
    "        gam, variance reduction factor\n",
    "        ---------------------------------------------------------'''\n",
    "    def myacf(x, y, parhat, acf):\n",
    "        if acf == 'str':  # par d0, mu0\n",
    "            return str_exp_fun(np.sqrt(x ** 2 + y ** 2), parhat[0], parhat[1])\n",
    "        elif acf == 'mar':  # par:: epsilon, alpha\n",
    "            return epl_fun(np.sqrt(x ** 2 + y ** 2), parhat[0], parhat[1])\n",
    "        else:\n",
    "            print('vrf ERROR: insert a valid auto correlation function')\n",
    "    # compute variance reduction factor\n",
    "    fun_XY = lambda x, y: (L - x) * (L - y) * myacf(x, y, par_acf, acf)\n",
    "    fun_XY0 = lambda x, y: (L0 - x) * (L0 - y) * myacf(x, y, par_acf, acf)\n",
    "    # its 2D integral a-la Vanmarcke\n",
    "    int_XY, abserr   = dblquad(fun_XY,  0.0, L,  lambda x: 0.0, lambda x: L)\n",
    "    int_XY0, abserr0 = dblquad(fun_XY0, 0.0, L0, lambda x: 0.0, lambda x: L0)\n",
    "    # gam  = 4/L**4*int_XY # between scale L and a point\n",
    "    gam = (L0 / L) ** 4 * (int_XY / int_XY0)  # between scales L and L0\n",
    "    return gam\n",
    "\n",
    "\n",
    "def down_wei(Ns, Cs, Ws, L, L0, beta, par_acf, acf='mar'):\n",
    "    ''' -----------------------------------------------------------------------\n",
    "    Downscale Weibull parameters from grid cell scale to a subgrid scale:\n",
    "    compute the downscaled weibull parameters from a large scale L\n",
    "    to a smaller scale L0, both in Km\n",
    "\n",
    "    INPUT ::\n",
    "        Ns, Cs, Ws (original parmaters at scale L - may be scalars or arrays)\n",
    "        L [km] large scale\n",
    "        L0 [km] small scale\n",
    "        gams = ratio between the mean number of wet days at scales L and L0\n",
    "        (should be larger than 1)\n",
    "        par_acf: set of paramaters for the acf (tuple)\n",
    "\n",
    "    OPTIONAL ARGUMENTS ::\n",
    "        acf: what acf. default is 'str'. others available:\n",
    "            'str'  ->  for 2p stretched exonential (Krajewski 2003)\n",
    "            'mar'  ->  for 2p power law with exp kernel (Marani 2003)\n",
    "\n",
    "        N_bias:: correction to N if comparing satellite and gauges\n",
    "        default is equal to zero (no correction)\n",
    "\n",
    "    OUTPUT::\n",
    "        Nd, Cd, Wd (downscaled parameters)\n",
    "        gam = variance reduction function\n",
    "        fval = function value at the end of numerical minimization\n",
    "    ---------------------------------------------------------------------'''\n",
    "    Ns = np.asarray(Ns)  # check if scalar input - should be the same for N,C,W\n",
    "    Cs = np.asarray(Cs)\n",
    "    Ws = np.asarray(Ws)\n",
    "    # the three parameter mush have same shape - I only check one here\n",
    "    is_scalar = False if Cs.ndim > 0 else True\n",
    "    Ns.shape = (1,) * (1 - Ns.ndim) + Ns.shape\n",
    "    Cs.shape = (1,) * (1 - Cs.ndim) + Cs.shape\n",
    "    Ws.shape = (1,) * (1 - Ws.ndim) + Ws.shape\n",
    "    m = Cs.shape[0]  # length of parameter arrays = number of blocks=\n",
    "    # spatial auto correlation function - epl or str\n",
    "    # def myacf(x, y, parhat, acf):\n",
    "    #     if acf == 'str':  # par d0, mu0\n",
    "    #         return str_exp_fun(np.sqrt(x ** 2 + y ** 2), parhat[0], parhat[1])\n",
    "    #     elif acf == 'mar':  # par:: epsilon, alpha\n",
    "    #         return epl_fun(np.sqrt(x ** 2 + y ** 2), parhat[0], parhat[1])\n",
    "    #     else:\n",
    "    #         print('down_wei WARNING: insert a valid auto correlation function')\n",
    "    # # compute variance reduction factor\n",
    "    # fun_XY = lambda x, y: (L - x) * (L - y) * myacf(x, y, par_acf, acf)\n",
    "    # fun_XY0 = lambda x, y: (L0 - x) * (L0 - y) * myacf(x, y, par_acf, acf)\n",
    "    # # its 2D integral a-la Vanmarcke\n",
    "    # int_XY, abserr   = dblquad(fun_XY,  0.0, L,  lambda x: 0.0, lambda x: L)\n",
    "    # int_XY0, abserr0 = dblquad(fun_XY0, 0.0, L0, lambda x: 0.0, lambda x: L0)\n",
    "    # # gam  = 4/L**4*int_XY # between scale L and a point\n",
    "    # gam = (L0 / L) ** 4 * (int_XY / int_XY0)  # between scales L and L0\n",
    "\n",
    "    gam = vrf(L, L0, par_acf, acf=acf)\n",
    "    # vrf = gam\n",
    "    # prob wet:: correct satellite N adding the average difference\n",
    "    pws = np.mean(Ns) / 365.25\n",
    "    Wd = np.zeros(m)\n",
    "    Cd = np.zeros(m)\n",
    "    Nd = np.zeros(m)\n",
    "    for ii in range(m):\n",
    "        cs = Cs[ii]\n",
    "        ws = Ws[ii]\n",
    "        rhs = 1 / gam / beta * (2 * ws * gamma(2 / ws) / (\n",
    "                                    gamma(1 / ws)) ** 2 + (gam - 1) * pws)\n",
    "        wpfun = lambda w: 2 * w * gamma(2 / w) / (gamma(1 / w)) ** 2 - rhs\n",
    "\n",
    "        res = fsolve(wpfun, 0.1, full_output=True, xtol=1e-06, maxfev=10000)\n",
    "        Wd[ii] = res[0]\n",
    "        info = res[1]\n",
    "        print(f'TEST: {info}')\n",
    "        fval = info['fvec'][0] # <--------------------------------------------------------------------- ARTURO's CORRECTION\n",
    "        if fval > 1e-5:\n",
    "            print('warning - downscaling function:: '\n",
    "                'there is something wrong solving fsolve!')\n",
    "        Cd[ii] = beta * Wd[ii] * (cs / ws) * gamma(1 / ws) / gamma(1 / Wd[ii])\n",
    "        Nd[ii] = int( np.rint( Ns[ii] / beta))\n",
    "    Nd = Nd if not is_scalar else Nd[0]\n",
    "    Cd = Cd if not is_scalar else Cd[0]\n",
    "    Wd = Wd if not is_scalar else Wd[0]\n",
    "    return Nd, Cd, Wd, gam, fval\n",
    "\n",
    "\n",
    "\n",
    "def downscale(xdata, Tr, *, thresh=1, L0=0.0001, acf='mar', dt=3,\n",
    "            plot=False, tscale=24, save_yearly = True, toll=0.005,\n",
    "            maxmiss=36, clat=None, clon=None,\n",
    "            opt_method='genetic'):\n",
    "    '''------------------------------------------------------------------------\n",
    "    Downscale a precipitation dataset contained in the x-array xdata.\n",
    "    xdata must have 3 dimensions, named (lat, lon, time)\n",
    "    and dimensions 'lat' and 'lon' should have equal size,\n",
    "    and possibly have odd length so as to clearly define a central pixel.\n",
    "    INPUT:\n",
    "        xdata = x-array with obs at time scale dt.\n",
    "        NOTE: rainfall amounts must be ACCUMULATIONS, not rainfall RATES!\n",
    "              and the array must already be loaded in memory, not dask!\n",
    "\n",
    "        Tr: return time for computing an axtreme value quantile. Must be scalar\n",
    "\n",
    "        thresh = threshold for defining wet days (default = 1mm)\n",
    "        L0 = scale in [km] to which downscale rainfall statistics\n",
    "             (default is L0 = 0.0001 km = 0.1 m ~ rain gauge obs. scale)\n",
    "\n",
    "        acf: type of autocorrelation function used. Possibilities:\n",
    "                acf='mar' for the power-law described in Marani 2003 (default)\n",
    "                acf='str' for a stretched exponential acf (Villarini et al)\n",
    "\n",
    "        dt: time scale in [hours] of the precipitation dataset.\n",
    "            Default is dt=3 (for TRMM - TMPA - 3b42 dataset)\n",
    "\n",
    "        plot: if true save plots (default is false).\n",
    "\n",
    "        yearly: if true, compute the yearly Weibull parameters\n",
    "                in addition to the global ones.\n",
    "\n",
    "        toll: tolerence for optimization of downscaled correlation.\n",
    "              (default is 0.005)\n",
    "\n",
    "        maxmiss: if computing yearly parameters, do so only for years with\n",
    "                 no more than maxmiss days of missing observations\n",
    "                 (default is 36, ~10% of obs. at the daily time scale)\n",
    "\n",
    "        tscale: timescale at which we want the downscaled statistics [hours]\n",
    "                default is 24 hours = DAILY time scale.\n",
    "                NOTE: METHOD HAS NOT BEEN TESTED AT DIFFERENT TIME SCALES.\n",
    "\n",
    "        clat, clon: if provided, they determine the central pixel\n",
    "                    in the arraay. If not provided, the 'central'\n",
    "                    pixel is selected by default.\n",
    "\n",
    "        method = optimization method for downscaling the correlation\n",
    "                 (lbfgsb, genetics or none). Default is 'genetics'.\n",
    "                 If none, no correlation downscaling is performed, return nan\n",
    "\n",
    "    OUTPUT: Dictionary with the following quantities:\n",
    "        gamma -> variance reduction function between the two scales\n",
    "        beta  -> wet fraction reduction function between the two scale\n",
    "\n",
    "        Nd Global Downscaled parameters\n",
    "        Cd\n",
    "        Wd\n",
    "        Ns Global grid cell scale parameters\n",
    "        Cs\n",
    "        Ws\n",
    "\n",
    "        NYd Yearly downscaled parameters (only if yearly = True)\n",
    "        CYd\n",
    "        WYd\n",
    "        NYs Yearly grid cell scale parameters (only if yearly = True)\n",
    "        CYs\n",
    "        WYs\n",
    "\n",
    "        eps_s, alp_s, (or d0_s and mu0_s): parameters of the correlation\n",
    "                                           function at the pixel scale\n",
    "\n",
    "        eps_d, alp_d, (or d0_d and mu0_d): parameters of the correlation\n",
    "                                           function downscaled at scale L0\n",
    "\n",
    "        corr_down_success -> True if optimization successfun for correlation.\n",
    "        corr_down_funval -> final value of function minimized to\n",
    "                            downscale the correlation function.\n",
    "        w_down_funval -> final value of function minimized to downscale w.\n",
    "        thresh -> threshold used in the analysis\n",
    "        (only if plot==True):\n",
    "        Taylor_contour -> nice plot with the wet fraction contour\n",
    "        corr_plot -> nice plot with the correlation function\n",
    "    ------------------------------------------------------------------------'''\n",
    "    res = {} # initialize dictionary for storing results\n",
    "    xdata = xdata.where(xdata >= -0.001) # set negative values to np.nan if any\n",
    "    xdaily0 = xdata.resample(time ='{}h'.format(tscale)).sum(dim='time', skipna=False)\n",
    "    xdaily = xdaily0.dropna(dim='time', how='any')\n",
    "    lons = xdata.lon.values\n",
    "    lats = xdata.lat.values\n",
    "    nlon = np.size(lons)\n",
    "    nlat = np.size(lats)\n",
    "    dx = np.abs(lons[1] - lons[0])\n",
    "    if nlon != nlat:\n",
    "        print('downscale warning: box sizes are not equal')\n",
    "    if nlon % 2 == 0:\n",
    "        print('downscale warning: at least one box size has even length')\n",
    "    if (bool(clat) and bool(clon) and clat in lats and clon in lons):\n",
    "        clat = lats[np.argmin(np.abs(clat - lats))]\n",
    "        clon = lons[np.argmin(np.abs(clon - lons))]\n",
    "        # otherwise us the one provided by the\n",
    "    else:\n",
    "        clat = lats[np.argmin(np.abs(np.mean(lats) - lats))]\n",
    "        clon = lons[np.argmin(np.abs(np.mean(lons) - lons))]\n",
    "    L1 = area_lat_long(clat, clon, dx, dx)[0] # in Km\n",
    "    # get the pixel closer to the center as central pixel:\n",
    "    # get the time series for the central pixel\n",
    "    tsc = xdaily.loc[dict(lat = clat, lon = clon)]\n",
    "\n",
    "    # c_excesses = np.maximum(tsc.values-thresh, 0.0)\n",
    "    c_excesses = tsc.values[tsc.values > thresh] - thresh\n",
    "    NCW = wei_fit(c_excesses)\n",
    "    pws = NCW[0]/xdaily.shape[2]\n",
    "    Ns = int(np.floor(pws*365.25))\n",
    "    Cs = NCW[1]\n",
    "    Ws = NCW[2]\n",
    "\n",
    "    # Taylor Hypothesis for downscaling intermittency\n",
    "    print('Downscaling Intermittency')\n",
    "    taylor = downscale_pwet(xdata, thresh=thresh, dt=dt, L1=L1,\n",
    "                   target_x=L0, target_t=tscale,\n",
    "                   origin_x=L1, origin_t=tscale, ninterp=1000, plot=plot)\n",
    "\n",
    "    print('Downscaling the correlation')\n",
    "    parnames = ['eps', 'alp'] if acf == 'mar' else ['d0', 'mu0']\n",
    "\n",
    "    # Correlation downscaling\n",
    "    print('Computing the correlation')\n",
    "    rcorr = grid_corr(xdaily, plot=plot, thresh=thresh)\n",
    "    gam_s = vrf(L1, L0, (rcorr['{}_s'.format(parnames[0])],\n",
    "                         rcorr['{}_s'.format(parnames[1])]), acf=acf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dcorr =  down_corr(rcorr['vdist'], rcorr['vcorr'], L1, acf=acf,\n",
    "                     use_ave=True, opt_method=opt_method, toll=toll,\n",
    "                       plot=plot)\n",
    "\n",
    "    # downscaling the Weibull pdf\n",
    "    print('Downscaling pdf - global Weibull parameters')\n",
    "    par_acf = (dcorr['{}_d'.format(parnames[0])], dcorr['{}_d'.format(parnames[1])])\n",
    "    Nd, Cd, Wd, gam_d, fval_w = down_wei(Ns, Cs, Ws, L1, L0,\n",
    "                               taylor['beta'], par_acf, acf=acf)\n",
    "\n",
    "    print('Downscaling pdf - yearly Weibull parameters')\n",
    "    NCWy, YEARSy = fit_yearly_weibull(tsc, thresh=thresh, maxmiss=maxmiss)\n",
    "    NYd, CYd, WYd, _, _ = down_wei(NCWy[:,0], NCWy[:,1], NCWy[:,2], L1, L0,\n",
    "                           taylor['beta'], par_acf, acf=acf)\n",
    "\n",
    "    if save_yearly:\n",
    "        res['NYs'] = NCWy[:,0]\n",
    "        res['CYs'] = NCWy[:,1]\n",
    "        res['WYs'] = NCWy[:,2]\n",
    "        res['NYd'] = NYd\n",
    "        res['CYd'] = CYd\n",
    "        res['WYd'] = WYd\n",
    "\n",
    "    # estimate some extreme quantiles with MEVD\n",
    "    # Tr = np.array([10, 20, 50, 100]) # pass\n",
    "    Fi = 1-1/Tr\n",
    "    res['Tr'] = Tr\n",
    "    # x0 = 150.0\n",
    "    x0 = 9.0*np.mean(CYd)\n",
    "    res['mev_d'] = mev_quant(Fi, x0, NYd, CYd, WYd, thresh=thresh)[0]\n",
    "    res['mev_s'] = mev_quant(Fi, x0, NCWy[:,0], NCWy[:,1], NCWy[:,2],\n",
    "                                               thresh=thresh)[0]\n",
    "\n",
    "    res['gam_d'] = gam_d\n",
    "    res['gam_s'] = gam_s\n",
    "    res['beta'] = taylor['beta']\n",
    "    res['Nd'] = Nd\n",
    "    res['Cd'] = Cd\n",
    "    res['Wd'] = Wd\n",
    "    res['Ns'] = Ns\n",
    "    res['Cs'] = Cs\n",
    "    res['Ws'] = Ws\n",
    "    res['{}_s'.format(parnames[0])] = rcorr['{}_s'.format(parnames[0])]\n",
    "    res['{}_s'.format(parnames[1])] = rcorr['{}_s'.format(parnames[1])]\n",
    "    res['{}_d'.format(parnames[0])] = dcorr['{}_d'.format(parnames[0])]\n",
    "    res['{}_d'.format(parnames[1])] = dcorr['{}_d'.format(parnames[1])]\n",
    "    res['corr_down_success'] =        dcorr['success']\n",
    "    res['corr_down_funval'] =         dcorr['funval']\n",
    "    res['w_down_funval'] = fval_w[0]\n",
    "    res['thresh'] = thresh\n",
    "    res['clat'] = clat\n",
    "    res['clon'] = clon\n",
    "    if plot:\n",
    "        res['corr_plot'] = dcorr['fig']\n",
    "        res['Taylor_contour'] = taylor['contour']\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "\n",
    "###################### EXTREME VALUE ANALYSIS FUNCTIONS ########################\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def gev_fit_lmom(sample):\n",
    "    ''' Fit GEV distribution to a sample of annual maxima\n",
    "    by means of LMOM technique (Hosking 1990 &co0)\n",
    "    maxima must be numpy column array\n",
    "    return parhat = (csi, psi, mu) i.e., (GEV shape scale location)\n",
    "    rem here csi > 0 --> Heavy tailed distribution'''\n",
    "    sample = np.asarray(sample)\n",
    "    n = np.size(sample)\n",
    "    x = np.sort(sample, axis=0)\n",
    "    b0 = np.sum(x) / n\n",
    "    b1 = 0.0\n",
    "    for j in range(0, n):  # skip first element\n",
    "        jj = j + 1  # real index\n",
    "        b1 = b1 + (jj - 1) / (n - 1) * x[j]\n",
    "    b1 = b1 / n\n",
    "    b2 = 0.0\n",
    "    for j in range(0, n):  # skip first two elements\n",
    "        jj = j + 1  # real\n",
    "        b2 = b2 + (jj - 1) * (jj - 2) / (n - 1) / (n - 2) * x[j]\n",
    "    b2 = b2 / n\n",
    "    # L MOMENTS - linear combinations of PWMs\n",
    "    L1 = b0\n",
    "    L2 = 2 * b1 - b0\n",
    "    L3 = 6 * b2 - 6 * b1 + b0\n",
    "    t3 = L3 / L2  # L skewness\n",
    "    # GEV parameters from L moments ( Hoskins 1990)\n",
    "    # using Hoskins (1985) approximation for computing k:\n",
    "    c = 2 / (3 + t3) - np.log(2) / np.log(3)\n",
    "    k = 7.8590 * c + 2.9554 * c ** 2\n",
    "    csi = -k  # ususal shape\n",
    "    psi = L2 * k / ((1 - 2 ** (-k)) * gamma(1 + k))  # scale\n",
    "    mu = L1 - psi * (1 - gamma(1 + k)) / k  # location\n",
    "    parhat = (csi, psi, mu)\n",
    "    return parhat\n",
    "\n",
    "\n",
    "def gev_quant(Fi, csi, psi, mu):\n",
    "    ''' compute GEV quantile q for given non exceedance probabilities in Fi\n",
    "    with parameters csi, psi, mu (shape, scale, location)\n",
    "    optional: if ci = True also produce the upper and lower confidence\n",
    "    intervals obtained under the hyp of normal distribution.\n",
    "    In this case the covariance matrix of the parameters must be provided\n",
    "    varcov = variance-covariance matrix of parameters.'''\n",
    "    Fi = np.asarray(Fi)\n",
    "    is_scalar = False if Fi.ndim > 0 else True\n",
    "    Fi.shape = (1,) * (1 - Fi.ndim) + Fi.shape\n",
    "    q = mu + psi / csi * ((-np.log(Fi)) ** (-csi) - 1)\n",
    "    q = q if not is_scalar else q[0]\n",
    "    return q\n",
    "\n",
    "\n",
    "def tab_rain_max(df):\n",
    "    '''--------------------------------------------------------------------------\n",
    "    arguments: df, pandas data frame with fields YEAR, PRCP\n",
    "    returns:\n",
    "    XI -> array of annual maxima (ranked in ascending order)\n",
    "    Fi -> Weibull plotting position estimate of their non exceedance probability\n",
    "    TR -> their relative return times\n",
    "    Default using Weibull plotting position for non exceedance probability\n",
    "    -----------------------------------------------------------------------------'''\n",
    "    years_all  = df['YEAR']\n",
    "    years      = np.unique(years_all)\n",
    "    nyears     = np.size(years)\n",
    "    maxima     = np.zeros(nyears)\n",
    "    for jj in range(nyears):\n",
    "        my_year      = df.PRCP[df['YEAR'] == years[jj]]\n",
    "        maxima[jj]   = np.max(my_year)\n",
    "    XI         = np.sort(maxima, axis = 0) # default ascend\n",
    "    Fi         = np.arange(1,nyears+1)/(nyears + 1)\n",
    "    TR         = 1/(1 - Fi)\n",
    "    return XI,Fi,TR\n",
    "\n",
    "\n",
    "def remove_missing_years(df, nmin):\n",
    "    '''\n",
    "    # input has to be a pandas data frame df\n",
    "    # including the variables YEAR, PRCP\n",
    "    # returns the same dataset after removing all years with less of nmin\n",
    "    days of data missing\n",
    "    # (accounts for missing entries, negative values)\n",
    "    # the number of years remaining (nyears2)\n",
    "    # and the original number of years (nyears1)\n",
    "    '''\n",
    "    years_all  = df['YEAR']\n",
    "    years      = pd.Series.unique(years_all)\n",
    "    nyears1    = np.size(years)\n",
    "    for jj in range(nyears1):\n",
    "        dfjj      = df[ df['YEAR'] == years[jj] ]\n",
    "        my_year   = dfjj.PRCP[ dfjj['PRCP'] >= 0 ] # remove -9999 V\n",
    "        my_year2  = my_year[ np.isfinite(my_year) ] # remove  nans - infs V\n",
    "        my_length = len(my_year2)\n",
    "        if my_length < 366-nmin:\n",
    "            df    = df[df.YEAR != years[jj]] # remove this year from the data frame\n",
    "    # then remove NaNs and -9999 from the record\n",
    "    # df.dropna(subset=['PRCP'], inplace = True)\n",
    "    df = df.dropna(subset=['PRCP'])\n",
    "    # df = df.ix[df['PRCP'] >= 0]\n",
    "    df = df[df['PRCP'] >= 0].copy()\n",
    "    # check how many years remain\n",
    "    years_all_2 = df['YEAR']\n",
    "    nyears2 = np.size(pd.Series.unique(years_all_2))\n",
    "    return df, nyears2, nyears1\n",
    "\n",
    "\n",
    "def wei_fit(sample):\n",
    "    ''' fit a 2-parameters Weibull distribution to a sample\n",
    "    by means of Probability Weighted Moments (PWM) matching (Greenwood 1979)\n",
    "    using only observations larger than a value 'threshold' are used for the fit\n",
    "    -- threshold without renormalization -- it assumes the values below are\n",
    "    not present. Default threshold = 0\n",
    "    INPUT:: sample (array with observations)\n",
    "           threshold (default is = 0)\n",
    "    OUTPUT::\n",
    "    returns dimension of the sample (n) (only values above threshold)\n",
    "    Weibull scale (c) and shape (w) parameters '''\n",
    "    sample = np.asarray(sample) # from list to Numpy array\n",
    "    wets   = sample[sample > 0.0]\n",
    "    x      = np.sort(wets) # sort ascend by default\n",
    "    M0hat  = np.mean(x)\n",
    "    M1hat  = 0.0\n",
    "    n      = x.size # sample size\n",
    "    for ii in range(n):\n",
    "        real_ii = ii + 1\n",
    "        M1hat   = M1hat + x[ii]*(n - real_ii)\n",
    "    M1hat = M1hat/(n*(n-1))\n",
    "    c     = M0hat/gamma( np.log(M0hat/M1hat)/np.log(2)) # scale par\n",
    "    w     = np.log(2)/np.log(M0hat/(2*M1hat)) # shape par\n",
    "    return  n, c, w\n",
    "\n",
    "\n",
    "def mev_fit(df, thresh=1):\n",
    "    '''--------------------------------------------------------------------\n",
    "    fit MEV to a dataframe of daily rainfall observations df - with PRCP, YEAR\n",
    "    years with too many missing data must already have been removed\n",
    "    -----------------------------------------------------------------'''\n",
    "    years   = np.unique(df.YEAR)\n",
    "    nyears  = np.size(years)\n",
    "    N = np.zeros(nyears)\n",
    "    C = np.zeros(nyears)\n",
    "    W = np.zeros(nyears)\n",
    "    for iy, year in enumerate(years):\n",
    "        sample = df['PRCP'].values[df['YEAR']==year]\n",
    "        excesses = sample[sample > thresh] - thresh\n",
    "        Ni = np.size(excesses)\n",
    "        if Ni == 0:\n",
    "            N[iy] = 0\n",
    "            C[iy] = 1e-9\n",
    "            W[iy] = 1.0\n",
    "        elif Ni == 1:\n",
    "            N[iy] = 1\n",
    "            W[iy] = 0.7\n",
    "            C[iy] = excesses[0]/gamma(1 + 1/W[iy])\n",
    "        else:\n",
    "            N[iy], C[iy], W[iy] = wei_fit(excesses)\n",
    "    return N,C,W\n",
    "\n",
    "\n",
    "def mev_fun(y, pr, N, C, W):\n",
    "    ''' MEV distribution function, to minimize numerically\n",
    "    for computing quantiles\n",
    "    Updated version, to include accounting for dry years with 0 events'''\n",
    "    nyears = N.size\n",
    "    # mev0f = numzero + np.sum( ( 1-np.exp(-(y/Cn)**Wn ))**Nn) - nyears*pr\n",
    "    mev0f = np.sum( ( 1-np.exp(-(y/C)**W ))**N) - nyears*pr\n",
    "    return mev0f\n",
    "\n",
    "\n",
    "def mev_quant(Fi, x0, N, C, W, thresh=1):\n",
    "    '''--------------------------------------------------------------------\n",
    "    computes the MEV quantile for given non exceedance prob. in Fi\n",
    "    arguments:\n",
    "    Fi: non exceedance probability (either scalar or array of values)\n",
    "    x0: starting guess for numerical solution\n",
    "    N, C, W: Yearly parameters of MEV distribution\n",
    "    potmode: if True, considers the distributions of value above threshold (default is False)\n",
    "    (In practice if potmode=True, the distribution of excesses over threshold is computed\n",
    "    and then from it the cdf is computed for the effective quantile = quant - thresh)\n",
    "    thresh: threshold for defining ordinary events (default is zero)\n",
    "    returns:\n",
    "    quant -> single quantile, or array of quantiles\n",
    "    flags -> flag = 0 if everything is ok, = 1 if convergence problems\n",
    "    when It happens, a different x0 should be used.\n",
    "    ---------------------------------------------------------------------'''\n",
    "    Fi = np.asarray(Fi)\n",
    "    is_scalar = False if Fi.ndim > 0 else True\n",
    "    Fi.shape = (1,)*(1-Fi.ndim) + Fi.shape\n",
    "    m = np.size(Fi)\n",
    "    quant = np.zeros(m)\n",
    "    flags = np.zeros((m), dtype = bool) # flag for the convergence of numerical solver\n",
    "    for ii in range(m):\n",
    "        myfun     = lambda y: mev_fun(y,Fi[ii],N,C,W)\n",
    "        res       = sc.optimize.fsolve(myfun, x0, full_output = True)\n",
    "        quant[ii] = res[0]\n",
    "        info      = res[1]\n",
    "        fval      = info['fvec']\n",
    "        if fval > 1e-5:\n",
    "            print('mevd_quant:: ERROR - fsolve does not work -  change x0')\n",
    "            flags[ii] = 1\n",
    "    quant = quant + thresh\n",
    "    quant  = quant if not is_scalar else quant[0]\n",
    "    flags  = flags if not is_scalar else flags[0]\n",
    "    return quant, flags\n",
    "\n",
    "\n",
    "def fit_yearly_weibull(xdata, thresh=1, maxmiss=36):\n",
    "    nobsmin = 366 - maxmiss\n",
    "    yearsall = xdata.time.dt.year.values\n",
    "    years = np.unique(yearsall)\n",
    "    nyears0 = np.size(years)\n",
    "    NCW = np.zeros((nyears0, 3))\n",
    "    NOBS = np.zeros(nyears0)\n",
    "    YEARS = np.zeros(nyears0)\n",
    "    for i in range(nyears0):\n",
    "        sample = xdata.sel(time=str(years[i]))\n",
    "        NOBS[i] = np.size(sample)\n",
    "        excesses = sample[sample > thresh] - thresh\n",
    "        Ni = np.size(excesses)\n",
    "        if Ni == 0:\n",
    "            # NCW[i, :] = np.array([0, np.nan, np.nan])\n",
    "            NCW[i, :] = np.array([0, 1E-9, 1.0])\n",
    "        elif Ni == 1:\n",
    "            what = 0.7 # Prior belief on shape parameter\n",
    "            chat = excesses[0]/gamma(1 + 1/what)\n",
    "            NCW[i, :] = np.array([1.0, chat, what])\n",
    "        else:\n",
    "            NCW[i,:] = wei_fit(excesses)\n",
    "        YEARS[i] = years[i]\n",
    "    cond = NOBS > nobsmin\n",
    "    NCW = NCW[cond]\n",
    "    YEARS = YEARS[cond]\n",
    "    return NCW, YEARS\n",
    "\n",
    "\n",
    "# main functions for the conus project\n",
    "\n",
    "# TODO: move the main_0 codes to the download folder,\n",
    "# TODO: or add them to the main workflow\n",
    "# TODO: add options for EV of gauges and trmm-domain\n",
    "\n",
    "# TODO: separate QRF results and plots\n",
    "# TODO: check that mev_s_all, years and hdf quantiles are consistent\n",
    "# TODO: fix or remove the multiprocessing code\n",
    "\n",
    "# TODO: for testing: move sample data to its own folder\n",
    "# TODO: improve testing\n",
    "# TODO: speed up EV analysis - multiprocessing\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "# project directories\n",
    "# tmpa_dir = os.path.join('..', 'data', 'tmpa_conus_data')\n",
    "tmpa_dir = os.path.join('..', 'data', 'tmpa_conus_data')\n",
    "outdir_data = os.path.join('..', 'output', 'pixel_stats')\n",
    "outplot = os.path.join('..', 'output', 'outplot')\n",
    "# list_gauges_dir = os.path.join(tmpa_dir, 'gauges_noaa_hpd')\n",
    "list_gauges_dir = os.path.join('..', 'data', 'data_noaa_hpd_gauges')\n",
    "gauges_dir = os.path.join(list_gauges_dir, 'daily_csv')\n",
    "stat_list_file = os.path.join(list_gauges_dir, 'HOURLY_LOC_NYEARS.csv')\n",
    "tmpa_hdf_file = os.path.join(tmpa_dir, 'data_tmpa_3h.hdf5')\n",
    "pickletemp = os.path.join('..','output','pickletemp')\n",
    "elev_dir = os.path.join('..', 'data', 'data_elevation')\n",
    "\n",
    "\n",
    "#### QUANTITIES TO SET FOR ANALYSIS: ####\n",
    "Tr = np.array([10, 20, 50, 100]) # return times for extreme value analysis\n",
    "evd_domain = 'world' # (can be = 'conus' or 'world' for EV analysis only)\n",
    "do_evd_all_gauges = True # do EV for all gauges in the dataset\n",
    "# do_evd_all_tmpa = True   # do EV for all pixels in evd_domain (WORLD or CONUS)\n",
    "do_trmm_evd = True # to fit MEV to each grid cell over CONUS\n",
    "do_trmm = True # to downscale trmm where gauges are available\n",
    "do_gauges = True # to compute gauge stats where there are enough\n",
    "\n",
    "\n",
    "################# added for reading elevation::\n",
    "# Boundaries of the CONUS domain and TRMM grid coordinates\n",
    "solat = 22    # south bound\n",
    "nolat = 50    # north\n",
    "welon = -130  # west\n",
    "ealon = - 60   # east\n",
    "dx = 0.25\n",
    "lats = np.arange(-49.875, 49.876, 0.25) # south to North\n",
    "lons = np.arange(-179.875, 179.876, 0.25) # West to East\n",
    "nlats = np.size(lats)\n",
    "nlons = np.size(lons)\n",
    "#################\n",
    "\n",
    "# kwargs in input for the function analyze_cell\n",
    "pixelkwargs = {\n",
    "        'npix':3, # box size for analysis, in number of grid cells\n",
    "        'npix_gauges':5, # size of the box for selecting gauges, in grid cells\n",
    "        'dx':dx, # size of a grid cell (dx = 0.25 for TMPA)\n",
    "        'minstat_bb':4, # number of gauges required over npix_gauges**2 area\n",
    "        'minstat_pixel':1, # number of gauges required over the grid cell\n",
    "        'min_nyears_pixel':10, # min record length (years) to select a gauge\n",
    "        'min_overlap_corr':2000, # compute correlation if at least 2000 obs.\n",
    "        'thresh':1, # [mm] precipitation magnitude threshold\n",
    "        'L0':0.0001, # rain gauge point scale [km]\n",
    "        'acf':'mar', # autocorrelation function used\n",
    "        'dt':3, # hours - timescale of TMPA dataset\n",
    "        'plot':False, # plot results (set False when running on the Cluster)\n",
    "        'tscale':24, # daily timescale to perform analysis\n",
    "        'save_yearly':True, # save yearly Weibull parameters\n",
    "        'toll':0.005, # for optimization algorithm for correlation downscaling\n",
    "        'maxmiss':36, # max number of missing daily data in each year of record\n",
    "        'opt_method':'genetic', # opt algorithm for corr. downscaling\n",
    "        'do_trmm_evd':do_trmm_evd,   # to fit MEV to each grid cell over CONUS\n",
    "        'do_gauges':  do_gauges,   # to compute gauge stats where there are enough\n",
    "        'do_trmm':    do_trmm,   # to downscale trmm where gauges are available\n",
    "        'do_smoke':   False   # to test the pixel positions\n",
    "        }\n",
    "\n",
    "\n",
    "def tmpa_evd(clon, clat, tmpa_hdf_file, Tr, *,\n",
    "             thresh=1, maxmiss=36):\n",
    "    \"\"\" extreme value analysis for a tmpa grid cell pixel\n",
    "    load the pixel centered in (clon, clat) from the\n",
    "    dask - xarray stored in the hdf file named tmpa_hdf_file,\n",
    "    and compute quantiles for the range of return times in the array Tr\n",
    "    optional: thresh = 1 theshold for MEV\n",
    "              maxmiss = 36 max number of missing data / year\n",
    "    Do not perform analysis if dry years have less than 2 events\"\"\"\n",
    "    res_evd = {}\n",
    "    Fi = 1 - 1 / Tr\n",
    "    xconus = read_gridded_data(tmpa_hdf_file)\n",
    "    # xdata = load_bounding_box(xconus, clon, clat, 1)\n",
    "    # print(xdata.shape)\n",
    "    xconus = xconus.where(xconus >= -0.001)\n",
    "    xpixel = xconus.sel(lat=clat, lon=clon).resample(time='D').sum(\n",
    "        dim='time', skipna=False).dropna(dim='time',\n",
    "                                         how='any').load()\n",
    "    ts = xpixel.values\n",
    "    years = xpixel.time.dt.year.values\n",
    "    df = pd.DataFrame({'PRCP': ts, 'YEAR': years})\n",
    "    df, ny2, ny1 = down.remove_missing_years(df, maxmiss)\n",
    "    Ny, Cy, Wy = down.mev_fit(df, thresh=thresh)\n",
    "    x0 = 9.0 * np.mean(Cy)\n",
    "    mevq = down.mev_quant(Fi, x0, Ny, Cy, Wy, thresh=thresh)[0]\n",
    "    XIemp, Fiemp, TRemp = down.tab_rain_max(df)\n",
    "    csi, psi, mu = down.gev_fit_lmom(XIemp)  # fit to annual maxima\n",
    "    gevq = down.gev_quant(Fi, csi, psi, mu)\n",
    "    res_evd['mev_s_all'] = mevq\n",
    "    res_evd['gev_s_all'] = gevq\n",
    "    return res_evd\n",
    "\n",
    "\n",
    "def gauge_evd(Tr, gauges_dir, stat_list_file, output_dir,\n",
    "              *, nyears_min=10, maxmiss=36, thresh=1):\n",
    "    '''----------------\n",
    "    compute evd statistics for all gauges in the dataset\n",
    "    with enough complete years of data\n",
    "    ----------------'''\n",
    "    sdf = pd.read_csv(stat_list_file, index_col = 0)\n",
    "    nstats = np.shape(sdf)[0]\n",
    "    ntr = np.size(Tr)\n",
    "    Fi = 1-1/Tr\n",
    "    nyearsg = np.zeros(nstats, dtype = int)\n",
    "    mev_g_all = np.zeros((nstats, ntr))*np.nan\n",
    "    gev_g_all = np.zeros((nstats, ntr))*np.nan\n",
    "    for i in range(nstats):\n",
    "        df0 = pd.read_csv( os.path.join(gauges_dir,\n",
    "                 '{}.csv'.format(sdf['ID'].values[i])))\n",
    "        df = df0[(df0['PRCP'] > -0.1) & (~np.isnan(df0['PRCP']))]\n",
    "        df, ny2, ny1 = down.remove_missing_years(df, maxmiss)\n",
    "        XIemp, Fiemp, TRemp = down.tab_rain_max(df)\n",
    "        nyearsg[i] = ny2\n",
    "        if nyearsg[i] >= nyears_min:\n",
    "            Ny, Cy, Wy = down.mev_fit(df, thresh=thresh)\n",
    "            x0 = 6.0*np.mean(Cy)\n",
    "            mev_g_all[i, :] = down.mev_quant(Fi, x0, Ny, Cy, Wy,\n",
    "                                        thresh=thresh)[0]\n",
    "            csi, psi, mu = down.gev_fit_lmom(XIemp)\n",
    "            gev_g_all[i,:] = down.gev_quant(Fi, csi, psi, mu)\n",
    "    # subset dataframe keeping only long enough time series:\n",
    "    sdf['nyearsg'] = nyearsg\n",
    "    for i in range(ntr):\n",
    "        sdf['mev_{}'.format(Tr[i])] = mev_g_all[:,i]\n",
    "        sdf['gev_{}'.format(Tr[i])] = gev_g_all[:,i]\n",
    "    sdf = sdf[sdf['nyearsg'] >= nyears_min]\n",
    "    ngauges = np.shape(sdf)[0]\n",
    "    sdf.to_csv(os.path.join(output_dir, 'dfres_gauges_{}.csv'.format(ngauges)))\n",
    "    return sdf\n",
    "\n",
    "\n",
    "def gauge_stats(clon, clat, df, Tr, gauges_dir, *, npix=5, dx=0.25,\n",
    "                minstat_bb=4,\n",
    "                minstat_pixel=1, thresh=1,\n",
    "                min_nyears_pixel=10, min_overlap_corr=2000,\n",
    "                maxmiss=36):\n",
    "    '''------------------------------------------------------------------------\n",
    "    gauge_stats:\n",
    "    Computes the statistics for longest-record gauge in the pixel (clon, clat)\n",
    "    if it is at least min_nyears_pixel years long,\n",
    "    and compute correlation between gauges in a npix*npix bounding box\n",
    "    if there are at least minstat_bb gauges with min common record of\n",
    "    min_overlap correlation.\n",
    "    Returns dictionary with corrlation and local gauge Weibull C, W, N\n",
    "    and if there were enough gauges in the pixel / bounding box\n",
    "    INPUT:\n",
    "    clon = longitude central pixel point\n",
    "    clat - latitude central pixel point\n",
    "    df = data frame with list of stations, extracted by NOAA HPD, daily scale\n",
    "    ------------------------------------------------------------------------'''\n",
    "\n",
    "    # default values of output variables if not enough stations at the ground:\n",
    "    enough_gauges_bb = False\n",
    "    enough_gauges_pixel = False\n",
    "    alpha = np.nan\n",
    "    epsilon = np.nan\n",
    "    d0 = np.nan\n",
    "    mu0 = np.nan\n",
    "    pwet = np.nan\n",
    "    C = np.nan\n",
    "    W = np.nan\n",
    "    N = np.nan\n",
    "    # Cy = np.zeros(min_nyears_pixel)*np.nan\n",
    "    # Wy = np.zeros(min_nyears_pixel)*np.nan\n",
    "    # Ny = np.zeros(min_nyears_pixel)*np.nan\n",
    "    gam_g = np.nan\n",
    "    nyearsg = np.nan\n",
    "    mev_g = np.zeros(np.size(Tr))*np.nan\n",
    "    # read stations within the box\n",
    "    wb = clon - npix/2*dx\n",
    "    eb = clon + npix/2*dx\n",
    "    nb = clat + npix/2*dx\n",
    "    sb = clat - npix/2*dx\n",
    "    wbpix = clon - 1/2*dx # pixel\n",
    "    ebpix = clon + 1/2*dx\n",
    "    nbpix = clat + 1/2*dx\n",
    "    sbpix = clat - 1/2*dx\n",
    "    # stations within the central pixel and the bounding box\n",
    "    mydf = df[ (df['LAT'] < nb) & (df['LAT'] > sb) &\n",
    "               (df['LON'] > wb) & (df['LON'] < eb) ]\n",
    "    mydfc = df[ (df['LAT'] < nbpix) & (df['LAT'] > sbpix)\n",
    "                & (df['LON'] > wbpix) & (df['LON'] < ebpix) ]\n",
    "    nstations_bb = np.shape(mydf)[0] # numebr of stats in bounding box\n",
    "    nstations_pixel = np.shape(mydfc)[0] # number of stats in central pixel\n",
    "    # compute empirical correlation\n",
    "    if nstations_bb >= minstat_bb:\n",
    "        vdist = []\n",
    "        vcorr = []\n",
    "        for iii in range(nstations_bb):\n",
    "            dfi0 = pd.read_csv( os.path.join(gauges_dir,\n",
    "                 '{}.csv'.format(mydf['ID'].values[iii])))\n",
    "            dfi = dfi0[(dfi0['PRCP'] > -0.1) & (~np.isnan(dfi0['PRCP']))]\n",
    "            dates_ii = dfi['DATE'].values\n",
    "            for jjj in range(iii + 1, nstations_bb):\n",
    "                dfj0 = pd.read_csv( os.path.join(gauges_dir,\n",
    "                     '{}.csv'.format(mydf['ID'].values[jjj])))\n",
    "                dfj = dfj0[(dfj0['PRCP'] > -0.1) & (~np.isnan(dfj0['PRCP']))]\n",
    "                dates_jj = dfj['DATE'].values\n",
    "                commondates = np.intersect1d(dates_ii, dates_jj)\n",
    "                sample_ii = dfi['PRCP'].values[dfi['DATE'].isin(commondates)]\n",
    "                sample_jj = dfj['PRCP'].values[dfj['DATE'].isin(commondates)]\n",
    "                if np.size(sample_ii) > min_overlap_corr:\n",
    "                    excesses_ii = np.maximum(sample_ii - thresh, 0.0)\n",
    "                    excesses_jj = np.maximum(sample_jj - thresh, 0.0)\n",
    "                    vcorr.append(np.corrcoef(excesses_ii, excesses_jj)[0,1])\n",
    "                    vdist.append(down.haversine(mydf['LAT'].values[iii],\n",
    "                                                mydf['LAT'].values[jjj],\n",
    "                                                mydf['LON'].values[iii],\n",
    "                                                mydf['LON'].values[jjj]\n",
    "                                                ))\n",
    "        # fit acf function\n",
    "        if len(vdist) >= minstat_bb:\n",
    "            try:\n",
    "                popt0, pcov0 = curve_fit(down.epl_fun,\n",
    "                             np.array(vdist), np.array(vcorr),\n",
    "                             p0 = np.array([50.0, 1.0]),\n",
    "                             bounds = ((0.0, 0.0), (+np.inf, +np.inf)))\n",
    "                epsilon = popt0[0]\n",
    "                alpha = popt0[1]\n",
    "                enough_gauges_bb = True\n",
    "                L = down.area_lat_long(clat, clon, dx, dx)[0]\n",
    "                L0 = 0.0001\n",
    "                gam_g = down.vrf(L, L0, (epsilon, alpha), acf='mar')\n",
    "                popt1, pcov1 = curve_fit(down.str_exp_fun,\n",
    "                             np.array(vdist), np.array(vcorr),\n",
    "                             p0 = np.array([50.0, 1.0]),\n",
    "                             bounds = ((0.0, 0.0), (+np.inf, +np.inf)))\n",
    "                d0 = popt1[0]\n",
    "                mu0 = popt1[1]\n",
    "            except:\n",
    "                print('gauge_stats WARNING: \\n'\n",
    "                      'pass - not possible to compute correlation reliably')\n",
    "    # fit Weibull to the longest station in the central pixel\n",
    "    if nstations_pixel >= minstat_pixel:\n",
    "        vec_nyears = mydfc['NYEARS'].values\n",
    "        if np.max(vec_nyears) >= min_nyears_pixel: # at least 10 years of data\n",
    "            # enough_gauges_pixel = True\n",
    "            long_index = np.argmax(mydfc['NYEARS'].values)\n",
    "            dfl0 = pd.read_csv( os.path.join(gauges_dir,\n",
    "                         '{}.csv'.format(mydfc['ID'].values[long_index])))\n",
    "            dfl = dfl0[ (dfl0['PRCP'] > -0.1) & (~np.isnan(dfl0['PRCP']) )]\n",
    "            sample = dfl['PRCP'].values\n",
    "            excesses = sample[sample > thresh] - thresh\n",
    "            NCWg = down.wei_fit(excesses)\n",
    "            pwet = NCWg[0]/np.size(sample)\n",
    "            C = NCWg[1]\n",
    "            W = NCWg[2]\n",
    "            N = int(np.rint(pwet*365.25))\n",
    "            # fit MEV\n",
    "            # TODOS: add option to save yearly parameters here if needed\n",
    "            dfl, ny2, ny1 = down.remove_missing_years(dfl, maxmiss)\n",
    "            if ny2 >= min_nyears_pixel:\n",
    "                enough_gauges_pixel = True\n",
    "                Ny, Cy, Wy = down.mev_fit(dfl, thresh=thresh)\n",
    "                # nyearsg = np.size(Ny)\n",
    "                Fi = 1-1/Tr\n",
    "                x0 = 6.0*np.mean(Cy)\n",
    "                mev_g = down.mev_quant(Fi, x0, Ny, Cy, Wy, thresh=thresh)[0]\n",
    "\n",
    "    # save results in dictionary:\n",
    "    res_gauges = {'Cg':C,\n",
    "                  'Wg':W,\n",
    "                  'Ng':N,\n",
    "                  'pwg':pwet,\n",
    "                  'enough_gauges_pixel':enough_gauges_pixel,\n",
    "                  'enough_gauges_bb':enough_gauges_bb,\n",
    "                  'ngauges_bb':nstations_bb,\n",
    "                  'ngauges_pixel':nstations_pixel,\n",
    "                  'nyears_gauge':nyearsg,\n",
    "                  'alp_g':alpha,\n",
    "                  'eps_g':epsilon,\n",
    "                  'd0_g':d0,\n",
    "                  'mu0_g':mu0,\n",
    "                  'Tr':Tr,\n",
    "                  'gam_g':gam_g,\n",
    "                  'mev_g':mev_g\n",
    "                 }\n",
    "    return res_gauges\n",
    "\n",
    "\n",
    "def read_gridded_data(tmpa_hdffile):\n",
    "    # f = h5py.File(os.path.join(datadir, 'data_tmpa_3h.hdf5'), \"r\")\n",
    "    f = h5py.File(tmpa_hdffile, \"r\")\n",
    "    # print(list(f.keys()))\n",
    "    tmpalat = f['lat'][:]\n",
    "    tmpalon = f['lon'][:]\n",
    "    dates_int = f['dates'][:]\n",
    "    hours_int = f['hours'][:]\n",
    "    dset = f['prcp']\n",
    "    # print('dataset shape = {}'.format(dset.shape))\n",
    "    x = da.from_array(dset, chunks=(6, 6, 300))\n",
    "    dates = [datetime.strptime(str(integd)+str(inthour), '%Y%m%d%H')\n",
    "             for integd, inthour in zip(dates_int, hours_int)] # UTC time\n",
    "    xconus = xr.DataArray(x,\n",
    "                      coords={'lon':tmpalon, 'lat':tmpalat, 'time':dates},\n",
    "                      dims=('lon', 'lat', 'time'))\n",
    "    return xconus\n",
    "\n",
    "\n",
    "def load_bounding_box(xconus, clon, clat, npix, dropna = False):\n",
    "    ''' load data within the bounding box in memory\n",
    "    from an out-of-memory xarray + dask array\n",
    "    DOES NOT REMOVE MISSING DATA, BUT SET THEM TO NANS'''\n",
    "    xconus = xconus.where(xconus >= -0.001)\n",
    "    lons = xconus.lon.values\n",
    "    dx = np.abs(lons[1] - lons[0])\n",
    "    buffer = 0.50*npix*dx\n",
    "    eps = 1e-4 # to make sure to include boundaires -> add an eps buffer\n",
    "    solat = clat - buffer + eps\n",
    "    nolat = clat + buffer + eps\n",
    "    ealon = clon + buffer + eps\n",
    "    welon = clon - buffer + eps\n",
    "    bcond = np.logical_and(\n",
    "                np.logical_and( xconus.lat > solat, xconus.lat < nolat),\n",
    "                np.logical_and( xconus.lon > welon, xconus.lon < ealon))\n",
    "    # Load in memory the bounding box of interest\n",
    "    if dropna:\n",
    "        xdata = xconus.where(bcond, drop = True).load()\n",
    "    else:\n",
    "        xdata = xconus.where(bcond, drop = True\n",
    "                             ).dropna(dim='time', how='any').load()\n",
    "\n",
    "    return xdata\n",
    "\n",
    "\n",
    "def analyze_cell(i, j, clon, clat, Tr, stat_list_file, tmpa_hdf_file,\n",
    "                gauges_dir, *,\n",
    "                npix=3, npix_gauges=5, dx=0.25,\n",
    "                minstat_bb=4, minstat_pixel=1,\n",
    "                min_nyears_pixel=10, min_overlap_corr=2000,\n",
    "                thresh=1,\n",
    "                L0=0.0001,\n",
    "                acf='mar', dt=3, plot=False, tscale=24,\n",
    "                save_yearly = True, toll=0.005, maxmiss=36,\n",
    "                opt_method='genetic',\n",
    "                do_smoke = True,\n",
    "                do_trmm = True,\n",
    "                do_gauges = True,\n",
    "                do_trmm_evd = True):\n",
    "    '''------------------------------------------------------------------------\n",
    "    analyze gauge data and gridded qpes for a bouding box of size npix\n",
    "    centered in clat, clon (indexes i, j respectively)\n",
    "    sdf = list of station coordinates and names\n",
    "    ------------------------------------------------------------------------'''\n",
    "    # compute some basic statistics, as NCW\n",
    "    res_smoke = {}\n",
    "    if do_smoke:\n",
    "        res_smoke['clon'] = clon\n",
    "        res_smoke['clat'] = clat\n",
    "\n",
    "    res_evd = {}\n",
    "    if do_trmm_evd:\n",
    "        res_evd = tmpa_evd(clon, clat, tmpa_hdf_file, Tr,\n",
    "                           thresh=thresh, maxmiss=maxmiss)\n",
    "\n",
    "    res_gauges = {}\n",
    "    res_tmpa = {}\n",
    "    if do_gauges:\n",
    "        sdf = pd.read_csv(stat_list_file, index_col=0)\n",
    "        res_gauges = gauge_stats(clon, clat, sdf, Tr, gauges_dir,\n",
    "                    npix=npix_gauges, dx=dx,  thresh=thresh,\n",
    "                    minstat_bb=minstat_bb, minstat_pixel=minstat_pixel,\n",
    "                    min_nyears_pixel=min_nyears_pixel,\n",
    "                    min_overlap_corr=min_overlap_corr, maxmiss=maxmiss)\n",
    "\n",
    "        if res_gauges['enough_gauges_bb'] and res_gauges['enough_gauges_pixel']:\n",
    "            res_gauges['complete_pixel'] = True\n",
    "        else:\n",
    "            res_gauges['complete_pixel'] = False\n",
    "\n",
    "        # by default analyze trmm only where enough gauges\n",
    "            ## CANCELLED FOR CONUS ANALYSIS _ LONG!\n",
    "        # if res_gauges['complete_pixel']:\n",
    "        if do_trmm:\n",
    "\n",
    "            xconus = read_gridded_data(tmpa_hdf_file)\n",
    "            xdata = load_bounding_box(xconus, clon, clat, npix,\n",
    "                                      dropna=False)\n",
    "\n",
    "            res_tmpa = down.downscale(xdata, Tr, thresh=thresh, L0=L0,\n",
    "                                      acf=acf, dt=dt,\n",
    "            plot=plot, tscale=tscale, save_yearly=save_yearly, toll=toll,\n",
    "            maxmiss=maxmiss, clat=clat, clon=clon, opt_method=opt_method)\n",
    "\n",
    "    res = {'i':i, 'j':j, **res_gauges, **res_tmpa, **res_evd, **res_smoke}\n",
    "    return res\n",
    "\n",
    "\n",
    "def load_results_df(csvname='dfres.csv'):\n",
    "    ''' load results from main analysis and add elevation data'''\n",
    "    dfres = pd.read_csv(os.path.join(outdir_data, csvname), index_col=0)\n",
    "    dfres['esa_d'] = dfres['eps_d']/dfres['alp_d']\n",
    "    dfres['esa_s'] = dfres['eps_s']/dfres['alp_s']\n",
    "    dfres['esa_g'] = dfres['eps_g']/dfres['alp_g']\n",
    "    dfres['etaC'] = (dfres['Cd']-dfres['Cg'])/dfres['Cg']\n",
    "    dfres['etaW'] = (dfres['Wd']-dfres['Wg'])/dfres['Wg']\n",
    "    dfres['etaN'] = (dfres['Nd']-dfres['Ng'])/dfres['Ng']\n",
    "\n",
    "    # load elevation and its stdv and add it to the dataset\n",
    "    with h5py.File(os.path.join(tmpa_dir, \"elev.hdf5\"), \"r\") as fr:\n",
    "        # print(list(fr.keys()))\n",
    "        mean_el_conus = fr['mean_el'][:]\n",
    "        stdv_el_conus = fr['stdv_el'][:]\n",
    "        # npix_stdv_conus = fr['npix_stdv']\n",
    "        elev_lat = fr['lat'][:]\n",
    "        elev_lon = fr['lon'][:]\n",
    "\n",
    "    nelem = dfres.shape[0]\n",
    "    dfres['melev'] = np.zeros(nelem)*np.nan\n",
    "    dfres['selev'] = np.zeros(nelem)*np.nan\n",
    "    for i in range(nelem):\n",
    "        if dfres['complete_pixel'].loc[i]:\n",
    "            clon = dfres['clon'].loc[i]\n",
    "            clat = dfres['clat'].loc[i]\n",
    "            ii = np.argmin(np.abs(clon-elev_lon))\n",
    "            jj = np.argmin(np.abs(clat-elev_lat))\n",
    "            dfres.at[i, 'melev'] = mean_el_conus[ii,jj]\n",
    "            dfres.at[i, 'selev'] = stdv_el_conus[ii,jj]\n",
    "\n",
    "    # cond1 = np.logical_and(dfres['complete_pixel'].values == True, True)\n",
    "    # dfresc = dfres[cond1]\n",
    "    # cond2 = np.logical_and(dfresc['esa_d'].values > 25.0,\n",
    "    #                        dfresc['corr_down_funval'].values < 1)\n",
    "    # dfresc = dfresc[cond2]\n",
    "\n",
    "    cond1 = dfres['complete_pixel'].values == True\n",
    "    dfresc = dfres[cond1]\n",
    "    num_tot_pixels = np.shape(dfresc)[0]\n",
    "    print('total number of complete pixel loaded = {}'.format(num_tot_pixels))\n",
    "    cond21 = dfresc['esa_d'].values > 25.0\n",
    "    cond_large = dfresc['esa_d'].values > 200.0\n",
    "    num_esad_above_200 = np.size(cond_large[cond_large==True])\n",
    "    # cond22 = dfresc['corr_down_funval'].values < 1\n",
    "\n",
    "    num_esad_below_25 = np.size(cond21[cond21==False])\n",
    "    # num_downfun_below_1 = np.size(cond22[cond22==False])\n",
    "    print(' number of complete pixels where e/a down < 25: '\n",
    "          '{}'.format(num_esad_below_25))\n",
    "\n",
    "\n",
    "    print(' number of complete pixels where e/a down > 200: '\n",
    "          '{}'.format(num_esad_above_200))\n",
    "    # print(' number of complete pixels where down fun > 1: '\n",
    "    #       '{}'.format(num_downfun_below_1))\n",
    "    # cond2 = np.logical_and(cond21, cond22)\n",
    "    dfresc = dfresc[cond21]\n",
    "    dfresc.reset_index(inplace=True)\n",
    "\n",
    "    return dfresc\n",
    "\n",
    "\n",
    "def load_results_netcdf(ncname='ncres.nc', elevname='elev.hdf5'):\n",
    "    ''' load results from netcdf file in x-array\n",
    "        and add elevation '''\n",
    "    # load hdf file with mean elevation and its stdv and add it to the dataset\n",
    "    with h5py.File(os.path.join(tmpa_dir, elevname), \"r\") as fr:\n",
    "        mean_el_conus = fr['mean_el'][:]\n",
    "        stdv_el_conus = fr['stdv_el'][:]\n",
    "        elev_lat = fr['lat'][:]\n",
    "        elev_lon = fr['lon'][:]\n",
    "    melev = xr.DataArray(mean_el_conus,\n",
    "                         coords=[elev_lon, elev_lat], dims=['lon', 'lat'])\n",
    "    selev = xr.DataArray(stdv_el_conus,\n",
    "                         coords=[elev_lon, elev_lat], dims=['lon', 'lat'])\n",
    "    ncres = xr.open_dataset(os.path.join(outdir_data, ncname))\n",
    "    ncres['melev'] = melev\n",
    "    ncres['selev'] = selev\n",
    "    ncres['esa_d'] = (ncres['eps_d'])/ncres['alp_d']\n",
    "    ncres['esa_s'] = (ncres['eps_s'])/ncres['alp_s']\n",
    "    ncres['esa_g'] = (ncres['eps_g'])/ncres['alp_g']\n",
    "    ncres['etaC'] = (ncres['Cd'] - ncres['Cg'])/ncres['Cg']\n",
    "    ncres['etaW'] = (ncres['Wd'] - ncres['Wg'])/ncres['Wg']\n",
    "    ncres['etaN'] = (ncres['Nd'] - ncres['Ng'])/ncres['Ng']\n",
    "    ncres['etaGAM'] = (ncres['gam_d'] - ncres['gam_g'])/ncres['gam_g']\n",
    "    ncres['etaESA'] = (ncres['esa_d'] - ncres['esa_g'])/ncres['esa_g']\n",
    "\n",
    "    # set values with epsilon/alpha < 25 to NaN\n",
    "\n",
    "\n",
    "    return ncres"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
