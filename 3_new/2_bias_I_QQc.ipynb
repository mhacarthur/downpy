{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32c6ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "playsound is relying on another python subprocess. Please use `pip install pygobject` if you want playsound to run more efficiently.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import shapely.vectorized as sv\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from playsound import playsound\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from function import DOWN_raw\n",
    "from function import ART_statistic as ART_sta\n",
    "from function import ART_downscale as ART_down\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c7e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_quantile_quantile(product, time_reso, seed, frac=0.7):\n",
    "    # ==================================================================================================\n",
    "    # VERIFY COMPUTER\n",
    "    COMPUTERNAME = os.environ['COMPUTERNAME']\n",
    "    # print(f'Computer     : {COMPUTERNAME}')\n",
    "\n",
    "    if COMPUTERNAME == 'BR_DELL':\n",
    "        dir_font = os.path.join('/','run')\n",
    "    else:\n",
    "        dir_font = os.path.join('/')\n",
    "\n",
    "    veneto_dir = os.path.join(dir_font,'media','arturo','T9','Data','shapes','Europa','Italy')\n",
    "\n",
    "    # ==================================================================================================\n",
    "    # LOAD ITALY SHAPEFILE\n",
    "    if os.path.exists(veneto_dir):\n",
    "        ITALY = gpd.read_file(os.path.join(veneto_dir,'Italy_clear.geojson'))\n",
    "    else:\n",
    "        raise SystemExit(f\"File not found: {veneto_dir}\")\n",
    "\n",
    "    obs_base = os.path.join(dir_font,'media','arturo','T9','Data','Italy','Rain_Gauges_QC')\n",
    "    weibul_base = os.path.join(dir_font,'media','arturo','T9','Data','Italy','Rain_Gauges_QC','Weibull','1dy')\n",
    "\n",
    "    sat_base = os.path.join('/','media','arturo','T9','Data','Italy','Satellite','5_DOWN')\n",
    "    # ENS_base = os.path.join('/','media','arturo','T9','Data','Italy','Satellite','5_ENSEMBLE')\n",
    "\n",
    "    # ==================================================================================================\n",
    "    # READ OBS METADATA\n",
    "    METADATA = pd.read_csv(os.path.join(obs_base, 'data', 'METADATA', 'METADATA_FTS_QCv4_Case1_wAIRHO_v3_1dy.csv'))\n",
    "    METADATA[\"Lat\"] = np.round(METADATA[\"Lat\"], 6)\n",
    "    METADATA[\"Lon\"] = np.round(METADATA[\"Lon\"], 6)\n",
    "\n",
    "    ISO_names = np.unique(METADATA.ISO.values)\n",
    "\n",
    "    # ==================================================================================================\n",
    "    # CREATE TRAIN AND TEST DATASETS\n",
    "    Q_train_list = []\n",
    "    Q_val_list = []\n",
    "\n",
    "    for iso in METADATA['ISO'].unique():\n",
    "        \n",
    "        META_iso = METADATA[METADATA['ISO'] == iso]\n",
    "\n",
    "        # Si una regi√≥n tiene muy pocas estaciones, evita errores\n",
    "        if len(META_iso) < 2:\n",
    "            Q_train_list.append(META_iso)\n",
    "            continue\n",
    "\n",
    "        META_80 = META_iso.sample(frac=frac, random_state=seed)\n",
    "        META_20 = META_iso.drop(META_80.index)\n",
    "\n",
    "        Q_train_list.append(META_80)\n",
    "        Q_val_list.append(META_20)\n",
    "\n",
    "    Q_train = pd.concat(Q_train_list, ignore_index=True)\n",
    "    Q_val = pd.concat(Q_val_list, ignore_index=True)\n",
    "\n",
    "    # print(f'Stations     : {len(METADATA)}')\n",
    "    # print(f'Train Dataset: {len(Q_train)}')\n",
    "    # print(f'Valid Dataset: {len(Q_val)}')\n",
    "\n",
    "    N_obs, C_obs, W_obs = [], [], []\n",
    "    for nn in range(len(Q_train)):\n",
    "        file_ = os.path.join(weibul_base, Q_train.ISO[nn], Q_train.File[nn])\n",
    "        statistic = pd.read_csv(file_)\n",
    "        statistic = statistic[(statistic['Year']>=2002)&(statistic['Year']<=2023)].reset_index(drop=True)\n",
    "        N_obs.extend(statistic.N.values)\n",
    "        C_obs.extend(statistic.C.values)\n",
    "        W_obs.extend(statistic.W.values)\n",
    "\n",
    "    # ==================================================================================================\n",
    "    # LOAD RSR PRODUCT\n",
    "    dir_in = os.path.join(sat_base, f'ITALY_DOWN_{product}_{time_reso}_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson.nc')\n",
    "    print(f'Reading    : {dir_in.split('/')[-1]}')\n",
    "    DATA = xr.open_dataset(dir_in)\n",
    "\n",
    "    if product == 'CHIRPS':\n",
    "        DATA['NYs'] = DATA.NYs.where(DATA.NYs != 0)\n",
    "        DATA['CYs'] = DATA.CYs.where(DATA.CYs != 0)\n",
    "        DATA['WYs'] = DATA.WYs.where(DATA.WYs != 0)\n",
    "        \n",
    "        DATA['NYd'] = DATA.NYd.where(DATA.NYd != 0)\n",
    "        DATA['CYd'] = DATA.CYd.where(DATA.CYd != 0)\n",
    "        DATA['WYd'] = DATA.WYd.where(DATA.WYd != 0)\n",
    "\n",
    "    lats = DATA.lat\n",
    "    lons = DATA.lon\n",
    "    lon2d, lat2d = np.meshgrid(lons, lats)\n",
    "    Sat_year = DATA.year.values\n",
    "\n",
    "    italy_geom = ITALY.union_all()\n",
    "    mask_italy = sv.contains(italy_geom, lon2d, lat2d)\n",
    "\n",
    "    NYs = DATA.NYs.where(mask_italy)\n",
    "    CYs = DATA.CYs.where(mask_italy)\n",
    "    WYs = DATA.WYs.where(mask_italy)\n",
    "\n",
    "    NYd = DATA.NYd.where(mask_italy)\n",
    "    CYd = DATA.CYd.where(mask_italy)\n",
    "    WYd = DATA.WYd.where(mask_italy)\n",
    "\n",
    "    ntime, nlat, nlon = NYd.shape\n",
    "\n",
    "    # ==================================================================================================\n",
    "    # REMOVE NAN VALUES\n",
    "    mask = ~np.isnan(NYs.values)\n",
    "    NYs_valid = NYs.values[mask]\n",
    "\n",
    "    mask = ~np.isnan(CYs.values)\n",
    "    CYs_valid = CYs.values[mask]\n",
    "\n",
    "    mask = ~np.isnan(WYs.values)\n",
    "    WYs_valid = WYs.values[mask]\n",
    "\n",
    "    mask = ~np.isnan(NYd.values)\n",
    "    NYd_valid = NYd.values[mask]\n",
    "\n",
    "    mask = ~np.isnan(CYd.values)\n",
    "    CYd_valid = CYd.values[mask]\n",
    "\n",
    "    mask = ~np.isnan(WYd.values)\n",
    "    WYd_valid = WYd.values[mask]\n",
    "\n",
    "    # ==================================================================================================\n",
    "    # APPLY QQ BIAS CORRECTION\n",
    "    NYs_full = ART_sta.ISIMIP_QM(N_obs, NYs_valid)\n",
    "    CYs_full = ART_sta.ISIMIP_QM(C_obs, CYs_valid)\n",
    "    WYs_full = ART_sta.ISIMIP_QM(W_obs, WYs_valid)\n",
    "\n",
    "    NYd_full = ART_sta.ISIMIP_QM(N_obs, NYd_valid)\n",
    "    CYd_full = ART_sta.ISIMIP_QM(C_obs, CYd_valid)\n",
    "    WYd_full = ART_sta.ISIMIP_QM(W_obs, WYd_valid)\n",
    "\n",
    "    # ==================================================================================================\n",
    "    # RESHAPE THE RESULTS\n",
    "    NYs_corrected = np.full(NYs.values.shape, np.nan)\n",
    "    NYs_corrected[mask] = NYs_full\n",
    "\n",
    "    CYs_corrected = np.full(CYs.values.shape, np.nan)\n",
    "    CYs_corrected[mask] = CYs_full\n",
    "\n",
    "    WYs_corrected = np.full(WYs.values.shape, np.nan)\n",
    "    WYs_corrected[mask] = WYs_full\n",
    "\n",
    "    NYd_corrected = np.full(NYd.values.shape, np.nan)\n",
    "    NYd_corrected[mask] = NYd_full\n",
    "\n",
    "    CYd_corrected = np.full(CYd.values.shape, np.nan)\n",
    "    CYd_corrected[mask] = CYd_full\n",
    "\n",
    "    WYd_corrected = np.full(WYd.values.shape, np.nan)\n",
    "    WYd_corrected[mask] = WYd_full\n",
    "\n",
    "    Tr = [5,  10,  20,  50, 100, 200]\n",
    "    Fi = 1 - 1/np.array(Tr)\n",
    "\n",
    "    # ==================================================================================================\n",
    "    # COMPUTE THE EXTREME QUANTILES\n",
    "    Mevs_corrected = ART_down.pre_quantiles_array(\n",
    "                        NYs_corrected, \n",
    "                        CYs_corrected, \n",
    "                        WYs_corrected, \n",
    "                        Tr, \n",
    "                        lats, lons,\n",
    "                        1)\n",
    "\n",
    "    Mevd_corrected = ART_down.pre_quantiles_array(\n",
    "                            NYd_corrected, \n",
    "                            CYd_corrected, \n",
    "                            WYd_corrected, \n",
    "                            Tr, \n",
    "                            lats, lons,\n",
    "                            1)\n",
    "\n",
    "    # ==================================================================================================\n",
    "    # CREATE THE NETCDF\n",
    "    DOWN_corrected = xr.Dataset(\n",
    "    data_vars={\n",
    "        \"NYs\": ((\"year\",\"lat\",\"lon\"), NYs_corrected),\n",
    "        \"CYs\": ((\"year\",\"lat\",\"lon\"), CYs_corrected),\n",
    "        \"WYs\": ((\"year\",\"lat\",\"lon\"), WYs_corrected),\n",
    "        \"NYd\": ((\"year\",\"lat\",\"lon\"), NYd_corrected),\n",
    "        \"CYd\": ((\"year\",\"lat\",\"lon\"), CYd_corrected),\n",
    "        \"WYd\": ((\"year\",\"lat\",\"lon\"), WYd_corrected),\n",
    "        \"Mev_d\": ((\"Tr\",\"lat\",\"lon\"), Mevd_corrected),\n",
    "        \"Mev_s\": ((\"Tr\",\"lat\",\"lon\"), Mevs_corrected),\n",
    "        },\n",
    "    coords={\n",
    "        'year': Sat_year, \n",
    "        'lat': lats, \n",
    "        'lon': lons\n",
    "        },\n",
    "        attrs=dict(description=f\"{product} Weibull parameters and MEV corrected applying ISIMIP_QM method using 80% of stations in Italy\",))\n",
    "\n",
    "    DOWN_corrected.NYs.attrs[\"units\"] = \"# days\"\n",
    "    DOWN_corrected.NYs.attrs[\"long_name\"] = \"Corrected Raw Number of Wet Days\"\n",
    "    DOWN_corrected.NYs.attrs[\"origname\"] = \"Raw Wet Days\"\n",
    "\n",
    "    DOWN_corrected.CYs.attrs[\"units\"] = \"nondimensional\"\n",
    "    DOWN_corrected.CYs.attrs[\"long_name\"] = \"Corrected Raw Scale Parameter\"\n",
    "    DOWN_corrected.CYs.attrs[\"origname\"] = \"Raw Scale\"\n",
    "\n",
    "    DOWN_corrected.WYs.attrs[\"units\"] = \"nondimensional\"\n",
    "    DOWN_corrected.WYs.attrs[\"long_name\"] = \"Corrected Raw Shape Parameter\"\n",
    "    DOWN_corrected.WYs.attrs[\"origname\"] = \"Raw Shape\"\n",
    "\n",
    "    DOWN_corrected.Mev_s.attrs[\"units\"] = \"mm/day\"\n",
    "    DOWN_corrected.Mev_s.attrs[\"long_name\"] = \"Corrected Raw Extreme Quantiles\"\n",
    "    DOWN_corrected.Mev_s.attrs[\"origname\"] = \"Raw Ext-Quant\"\n",
    "\n",
    "    DOWN_corrected.NYd.attrs[\"units\"] = \"# days\"\n",
    "    DOWN_corrected.NYd.attrs[\"long_name\"] = \"Corrected Downscaled Number of Wet Days\"\n",
    "    DOWN_corrected.NYd.attrs[\"origname\"] = \"Down Wet Days\"\n",
    "\n",
    "    DOWN_corrected.CYd.attrs[\"units\"] = \"nondimensional\"\n",
    "    DOWN_corrected.CYd.attrs[\"long_name\"] = \"Corrected Downscaled Scale Parameter\"\n",
    "    DOWN_corrected.CYd.attrs[\"origname\"] = \"Down Scale\"\n",
    "\n",
    "    DOWN_corrected.WYd.attrs[\"units\"] = \"nondimensional\"\n",
    "    DOWN_corrected.WYd.attrs[\"long_name\"] = \"Corrected Downscaled Shape Parameter\"\n",
    "    DOWN_corrected.WYd.attrs[\"origname\"] = \"Down Shape\"\n",
    "\n",
    "    DOWN_corrected.Mev_d.attrs[\"units\"] = \"mm/day\"\n",
    "    DOWN_corrected.Mev_d.attrs[\"long_name\"] = \"Corrected Downscaled Extreme Quantiles\"\n",
    "    DOWN_corrected.Mev_d.attrs[\"origname\"] = \"Down Ext-Quant\"\n",
    "\n",
    "    DOWN_corrected.lat.attrs[\"units\"] = \"degrees_north\"\n",
    "    DOWN_corrected.lat.attrs[\"long_name\"] = \"Latitude\"\n",
    "\n",
    "    DOWN_corrected.lon.attrs[\"units\"] = \"degrees_east\"\n",
    "    DOWN_corrected.lon.attrs[\"long_name\"] = \"Longitude\"\n",
    "\n",
    "    # ==================================================================================================\n",
    "    # EXPORT AS NETCDF\n",
    "    dir_base = os.path.join('/','media','arturo','T9','Data','Italy','Satellite')\n",
    "    PRE_out = os.path.join(os.path.join(dir_base, '6_DOWN_BCorrected', dir_in.split('/')[-1].replace('_pearson',f'_pearson_QQc')))\n",
    "    print(f'Exportin as: {PRE_out.split('/')[-1]}')\n",
    "    DOWN_corrected.to_netcdf(PRE_out)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b19366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading    : ITALY_DOWN_IMERG_1dy_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson.nc\n",
      "Exportin as: ITALY_DOWN_IMERG_1dy_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson_QQc.nc\n",
      "\n",
      "Reading    : ITALY_DOWN_CMORPH_3h_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson.nc\n",
      "Exportin as: ITALY_DOWN_CMORPH_3h_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson_QQc.nc\n",
      "\n",
      "Reading    : ITALY_DOWN_ERA5_3h_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson.nc\n",
      "Exportin as: ITALY_DOWN_ERA5_3h_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson_QQc.nc\n",
      "\n",
      "Reading    : ITALY_DOWN_MSWEP_3h_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson.nc\n",
      "Exportin as: ITALY_DOWN_MSWEP_3h_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson_QQc.nc\n",
      "\n",
      "Reading    : ITALY_DOWN_CHIRPS_1dy_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson.nc\n",
      "Exportin as: ITALY_DOWN_CHIRPS_1dy_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson_QQc.nc\n",
      "\n",
      "Reading    : ITALY_DOWN_GSMaP_3h_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson.nc\n",
      "Exportin as: ITALY_DOWN_GSMaP_3h_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson_QQc.nc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed, frac = 23, 0.7\n",
    "\n",
    "product, time_reso = 'IMERG', '1dy'\n",
    "correction_quantile_quantile(product, time_reso, seed, frac=0.7)\n",
    "\n",
    "product, time_reso = 'CMORPH', '3h'\n",
    "correction_quantile_quantile(product, time_reso, seed, frac=0.7)\n",
    "\n",
    "product, time_reso = 'ERA5', '3h'\n",
    "correction_quantile_quantile(product, time_reso, seed, frac=0.7)\n",
    "\n",
    "product, time_reso = 'MSWEP', '3h'\n",
    "correction_quantile_quantile(product, time_reso, seed, frac=0.7)\n",
    "\n",
    "product, time_reso = 'CHIRPS', '1dy'\n",
    "correction_quantile_quantile(product, time_reso, seed, frac=0.7)\n",
    "\n",
    "product, time_reso = 'GSMaP', '3h'\n",
    "correction_quantile_quantile(product, time_reso, seed, frac=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c0651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "playsound(\"../sound/HOMER_DOH.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0010ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AXE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
