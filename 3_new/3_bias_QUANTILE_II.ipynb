{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "138fb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import scipy.stats as stats\n",
    "\n",
    "from matplotlib import patches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "import shapely.geometry as sg\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from function import DOWN_raw\n",
    "from function import ART_preprocessing as ART_pre\n",
    "\n",
    "from playsound import playsound\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa358d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product, time_reso = 'IMERG', '1dy'\n",
    "product, time_reso = 'CMORPH', '3h'\n",
    "# product, time_reso = 'ERA5', '3h'\n",
    "# product, time_reso = 'MSWEP', '3h'\n",
    "# product, time_reso = 'CHIRPS', '1dy'\n",
    "# product, time_reso = 'GSMaP', '3h'\n",
    "# product, time_reso = 'ENSEMBLE_median', '1dy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "073e69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(product, dir_base, val_max=1.1, corrected=False, corr_method=None):\n",
    "    # The list bellow is the rain gauges with suspect data\n",
    "    list_remove = [\n",
    "            'IT-820_1424_FTS_1440_QCv4.csv', 'IT-250_602781_FTS_1440_QCv4.csv', \n",
    "            'IT-250_602779_FTS_1440_QCv4.csv', 'IT-780_2370_FTS_1440_QCv4.csv', \n",
    "            'IT-750_450_FTS_1440_QCv4.csv', 'IT-520_TOS11000099_FTS_1440_QCv4.csv',\n",
    "            'IT-520_TOS11000080_FTS_1440_QCv4.csv', 'IT-520_TOS11000072_FTS_1440_QCv4.csv',\n",
    "            'IT-520_TOS11000060_FTS_1440_QCv4.csv', 'IT-520_TOS11000025_FTS_1440_QCv4.csv',\n",
    "            'IT-520_TOS09001200_FTS_1440_QCv4.csv', 'IT-520_TOS02000237_FTS_1440_QCv4.csv',\n",
    "            'IT-230_1200_FTS_1440_QCv4.csv'\n",
    "            ]\n",
    "\n",
    "    if corrected == True:\n",
    "        if corr_method == 'QQc':\n",
    "            print(f\"Loading {product} corrected statistics...\")\n",
    "            hdf5_file = os.path.join(dir_base,'statistics','QQc',f'statistics_obs_{product}_corrected_{corr_method}.h5')\n",
    "        else:\n",
    "            raise ValueError(\"corr_method must be specified between 'QQc' or 'LRC' when corrected=True\")\n",
    "    else:\n",
    "        hdf5_file = os.path.join(dir_base,'statistics',f'statistics_obs_{product}.h5')\n",
    "    data = pd.HDFStore(hdf5_file, mode='r')\n",
    "\n",
    "    keys = data.keys()\n",
    "    keys_QUANTILES = [k for k in keys if k.endswith(\"/QUANTILES\")]\n",
    "    keys_INFO = [k for k in keys if k.endswith('/INFO')]\n",
    "\n",
    "    stations = []\n",
    "    lats, lons, elevs = [], [], []\n",
    "    Nobs, Cobs, Wobs = [], [], []\n",
    "    OBS, MEVd = [], []\n",
    "    for nn in range(len(keys_INFO)):\n",
    "        station = keys_INFO[nn].split('/')[2]\n",
    "        \n",
    "        if station in list_remove:\n",
    "            continue\n",
    "        else:\n",
    "            lat = data[keys_INFO[nn]]['lat_obs'].values[0]\n",
    "            lon = data[keys_INFO[nn]]['lon_obs'].values[0]\n",
    "            elev = data[keys_INFO[nn]]['elev_obs'].values[0]\n",
    "            Obs_ = data[keys_QUANTILES[nn]].OBS.values[3]\n",
    "            Down_ = data[keys_QUANTILES[nn]].SAT_down.values[3] \n",
    "\n",
    "            stations.append(station)\n",
    "            lats.append(lat)\n",
    "            lons.append(lon)\n",
    "            elevs.append(elev)\n",
    "            OBS.append(Obs_)\n",
    "            MEVd.append(Down_)\n",
    "\n",
    "    DF_DATA = pd.DataFrame({'STATION':stations, 'LON':lons, 'LAT':lats, 'ELEV':elevs, 'OBS':OBS, 'MEVd':MEVd})\n",
    "\n",
    "    return DF_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc741a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 7\n",
      "Export as: /media/arturo/T9/Data/Italy/statistics/QUANTILE/statistics_obs_CMORPH_corrected_QQc_LLc_0007.h5\n",
      "\n",
      "Seed: 19\n",
      "Export as: /media/arturo/T9/Data/Italy/statistics/QUANTILE/statistics_obs_CMORPH_corrected_QQc_LLc_0019.h5\n",
      "\n",
      "Seed: 31\n",
      "Export as: /media/arturo/T9/Data/Italy/statistics/QUANTILE/statistics_obs_CMORPH_corrected_QQc_LLc_0031.h5\n",
      "\n",
      "Seed: 53\n",
      "Export as: /media/arturo/T9/Data/Italy/statistics/QUANTILE/statistics_obs_CMORPH_corrected_QQc_LLc_0053.h5\n",
      "\n",
      "Seed: 89\n",
      "Export as: /media/arturo/T9/Data/Italy/statistics/QUANTILE/statistics_obs_CMORPH_corrected_QQc_LLc_0089.h5\n",
      "\n",
      "Seed: 127\n"
     ]
    }
   ],
   "source": [
    "frac = 0.7\n",
    "seeds_list = [7, 19, 31, 53, 89, 127, 211, 307, 401, 509, 613, 727, 839, 947, 1051]\n",
    "\n",
    "for seed in seeds_list:\n",
    "    print(f'Seed: {seed}')\n",
    "    \n",
    "    dir_ = os.path.join('/','media','arturo','T9','Data','Italy')\n",
    "    DF_INPUT = get_parameters('ENSEMBLE_ALL_MEDIAN',dir_)\n",
    "\n",
    "    obs_base = os.path.join('/','media','arturo','T9','Data','Italy','Rain_Gauges_QC')\n",
    "    METADATA = pd.read_csv(os.path.join(obs_base, 'data', 'METADATA', 'METADATA_FTS_QCv4_Case1_wAIRHO_v3_1dy.csv'))\n",
    "    METADATA_CLEAR = METADATA[METADATA['File'].isin(DF_INPUT['STATION'])].reset_index(drop=True)\n",
    "\n",
    "    Q_train_list = []\n",
    "    Q_val_list = []\n",
    "\n",
    "    for iso in METADATA_CLEAR['ISO'].unique():\n",
    "\n",
    "        META_iso = METADATA_CLEAR[METADATA_CLEAR['ISO'] == iso]\n",
    "\n",
    "        # Si una regi贸n tiene muy pocas estaciones, evita errores\n",
    "        if len(META_iso) < 2:\n",
    "            Q_train_list.append(META_iso)\n",
    "            continue\n",
    "\n",
    "        META_80 = META_iso.sample(frac=frac, random_state=seed)\n",
    "        META_20 = META_iso.drop(META_80.index)\n",
    "\n",
    "        Q_train_list.append(META_80)\n",
    "        Q_val_list.append(META_20)\n",
    "\n",
    "    Q_train = pd.concat(Q_train_list, ignore_index=True)\n",
    "    Q_val = pd.concat(Q_val_list, ignore_index=True)\n",
    "    list_train = Q_train.File.values\n",
    "\n",
    "    dir_base = os.path.join('/','media','arturo','T9','Data','Italy','Satellite','6_DOWN_BCorrected','QUANTILE')\n",
    "    dir_input = os.path.join(os.path.join(dir_base, f'ITALY_DOWN_{product}_{time_reso}_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson_QQc_LLc_{str(seed).zfill(4)}.nc'))\n",
    "    DATA = xr.open_dataset(dir_input)\n",
    "\n",
    "\n",
    "    Tr_index = 3\n",
    "    # print(f'Tr: {Tr[Tr_index]} years')\n",
    "    Tr = [5,  10,  20,  50, 100, 200]\n",
    "    Fi = 1 - 1/np.array(Tr)\n",
    "    ISO_names = np.unique(METADATA.ISO.values)\n",
    "\n",
    "    INFO_region = {}\n",
    "    WEIBULL_region = {}\n",
    "    QUANTILES_region = {}\n",
    "\n",
    "    for rr in range(len(ISO_names)):\n",
    "        region_ISO = ISO_names[rr]\n",
    "\n",
    "        INFO_dict = {}\n",
    "        QUANTILES_dict = {}\n",
    "\n",
    "        # print(f'{rr+1}: {region_ISO}')\n",
    "\n",
    "        METADATA_clear = Q_val[Q_val['ISO']==region_ISO].reset_index(inplace=False) # only validation METADATA\n",
    "\n",
    "        for nn in range(len(METADATA_clear)):#len(METADATA_clear)\n",
    "            filename = f'{METADATA_clear['File'].values[nn]}'\n",
    "            lat_obs = METADATA_clear['Lat'][nn]\n",
    "            lon_obs = METADATA_clear['Lon'][nn]\n",
    "            elev_obs = METADATA_clear['DEM_Elevation'][nn]\n",
    "\n",
    "            OBS_pd = pd.read_csv(os.path.join(obs_base, 'Weibull', '1dy', region_ISO, filename))\n",
    "            OBS_pd = OBS_pd[(OBS_pd['Year']>=2002)&(OBS_pd['Year']<=2023)].reset_index(drop=True)\n",
    "            \n",
    "            if len(OBS_pd) == 0:\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "                OBS_N = OBS_pd['N'].values\n",
    "                OBS_C = OBS_pd['C'].values\n",
    "                OBS_W = OBS_pd['W'].values\n",
    "                OBS_Y = OBS_pd['Year'].values\n",
    "\n",
    "                mask = ~np.isnan(OBS_N)\n",
    "\n",
    "                OBS_N = OBS_N[mask]\n",
    "                OBS_C = OBS_C[mask]\n",
    "                OBS_W = OBS_W[mask]\n",
    "                OBS_Y = OBS_Y[mask]\n",
    "\n",
    "                if len(OBS_Y) >= 8: # greather than 8 years\n",
    "\n",
    "                    x0 = np.nanmean(OBS_C)\n",
    "                    OBS_Q, flag = DOWN_raw.mev_quant_update(Fi, x0, OBS_N, OBS_C, OBS_W, thresh=1)\n",
    "                    OBS_Q2 = np.where(flag, OBS_Q, np.nan)\n",
    "\n",
    "                    PREC_SAT = DATA.sel(lat=lat_obs, lon=lon_obs, method='nearest')\n",
    "                    lat_ref = float(PREC_SAT.lat.values)\n",
    "                    lon_ref = float(PREC_SAT.lon.values)\n",
    "\n",
    "                    INFO = pd.DataFrame({'lat_obs':[lat_obs], 'lon_obs':[lon_obs], 'elev_obs':[elev_obs], 'lat_ref':[lat_ref], 'lon_ref':[lon_ref]})\n",
    "                    \n",
    "                    Mevd_tmp = PREC_SAT.MEVd_Down.data\n",
    "                    Mevd_LLc_tmp = PREC_SAT.MEVd_LLc.data\n",
    "                    Mevd_QQc_tmp = PREC_SAT.MEVd_QQc.data\n",
    "                    \n",
    "                    QUANTILES = pd.DataFrame({'Tr':[50], 'OBS':OBS_Q2[3], 'DOWN':Mevd_tmp, 'LLc':Mevd_LLc_tmp, 'QQc':Mevd_QQc_tmp})\n",
    "                    \n",
    "                    INFO_dict[filename] = INFO\n",
    "                    QUANTILES_dict[filename] = QUANTILES\n",
    "        \n",
    "        INFO_region[region_ISO] = INFO_dict\n",
    "        QUANTILES_region[region_ISO] = QUANTILES_dict\n",
    "\n",
    "    dir_out = os.path.join('/','media','arturo','T9','Data','Italy','statistics','QUANTILE')\n",
    "    hdf5_file = os.path.join(dir_out, f'statistics_obs_{product}_corrected_QQc_LLc_{str(seed).zfill(4)}.h5')\n",
    "\n",
    "    print(f'Export as: {hdf5_file}')\n",
    "\n",
    "    with pd.HDFStore(hdf5_file, mode='w') as store:\n",
    "\n",
    "        for region_ISO in INFO_region.keys():\n",
    "            stations = INFO_region[region_ISO].keys()  # las estaciones de la regi贸n\n",
    "\n",
    "            for station in stations:\n",
    "\n",
    "                info_df = INFO_region[region_ISO][station]\n",
    "                quantiles_df = QUANTILES_region[region_ISO][station]\n",
    "\n",
    "                store[f\"/{region_ISO}/{station}/INFO\"] = info_df\n",
    "                store[f\"/{region_ISO}/{station}/QUANTILES\"] = quantiles_df\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "playsound(\"../sound/HOMER_DOH.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8e91b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1643fc",
   "metadata": {},
   "source": [
    "## For Individual seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.7\n",
    "seeds_list = [7, 19, 31, 53, 89, 127, 211, 307, 401, 509, 613, 727, 839, 947, 1051]\n",
    "# seed = seeds_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(product, dir_base, val_max=1.1, corrected=False, corr_method=None):\n",
    "    # The list bellow is the rain gauges with suspect data\n",
    "    list_remove = [\n",
    "            'IT-820_1424_FTS_1440_QCv4.csv', 'IT-250_602781_FTS_1440_QCv4.csv', \n",
    "            'IT-250_602779_FTS_1440_QCv4.csv', 'IT-780_2370_FTS_1440_QCv4.csv', \n",
    "            'IT-750_450_FTS_1440_QCv4.csv', 'IT-520_TOS11000099_FTS_1440_QCv4.csv',\n",
    "            'IT-520_TOS11000080_FTS_1440_QCv4.csv', 'IT-520_TOS11000072_FTS_1440_QCv4.csv',\n",
    "            'IT-520_TOS11000060_FTS_1440_QCv4.csv', 'IT-520_TOS11000025_FTS_1440_QCv4.csv',\n",
    "            'IT-520_TOS09001200_FTS_1440_QCv4.csv', 'IT-520_TOS02000237_FTS_1440_QCv4.csv',\n",
    "            'IT-230_1200_FTS_1440_QCv4.csv'\n",
    "            ]\n",
    "\n",
    "    if corrected == True:\n",
    "        if corr_method == 'QQc':\n",
    "            print(f\"Loading {product} corrected statistics...\")\n",
    "            hdf5_file = os.path.join(dir_base,'statistics','QQc',f'statistics_obs_{product}_corrected_{corr_method}.h5')\n",
    "        else:\n",
    "            raise ValueError(\"corr_method must be specified between 'QQc' or 'LRC' when corrected=True\")\n",
    "    else:\n",
    "        hdf5_file = os.path.join(dir_base,'statistics',f'statistics_obs_{product}.h5')\n",
    "    data = pd.HDFStore(hdf5_file, mode='r')\n",
    "\n",
    "    keys = data.keys()\n",
    "    keys_QUANTILES = [k for k in keys if k.endswith(\"/QUANTILES\")]\n",
    "    keys_INFO = [k for k in keys if k.endswith('/INFO')]\n",
    "\n",
    "    stations = []\n",
    "    lats, lons, elevs = [], [], []\n",
    "    Nobs, Cobs, Wobs = [], [], []\n",
    "    OBS, MEVd = [], []\n",
    "    for nn in range(len(keys_INFO)):\n",
    "        station = keys_INFO[nn].split('/')[2]\n",
    "        \n",
    "        if station in list_remove:\n",
    "            continue\n",
    "        else:\n",
    "            lat = data[keys_INFO[nn]]['lat_obs'].values[0]\n",
    "            lon = data[keys_INFO[nn]]['lon_obs'].values[0]\n",
    "            elev = data[keys_INFO[nn]]['elev_obs'].values[0]\n",
    "            Obs_ = data[keys_QUANTILES[nn]].OBS.values[3]\n",
    "            Down_ = data[keys_QUANTILES[nn]].SAT_down.values[3] \n",
    "\n",
    "            stations.append(station)\n",
    "            lats.append(lat)\n",
    "            lons.append(lon)\n",
    "            elevs.append(elev)\n",
    "            OBS.append(Obs_)\n",
    "            MEVd.append(Down_)\n",
    "\n",
    "    DF_DATA = pd.DataFrame({'STATION':stations, 'LON':lons, 'LAT':lats, 'ELEV':elevs, 'OBS':OBS, 'MEVd':MEVd})\n",
    "\n",
    "    return DF_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = os.path.join('/','media','arturo','T9','Data','Italy')\n",
    "DF_INPUT = get_parameters('ENSEMBLE_ALL_MEDIAN',dir_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86340c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_base = os.path.join('/','media','arturo','T9','Data','Italy','Rain_Gauges_QC')\n",
    "METADATA = pd.read_csv(os.path.join(obs_base, 'data', 'METADATA', 'METADATA_FTS_QCv4_Case1_wAIRHO_v3_1dy.csv'))\n",
    "METADATA_CLEAR = METADATA[METADATA['File'].isin(DF_INPUT['STATION'])].reset_index(drop=True)\n",
    "\n",
    "Q_train_list = []\n",
    "Q_val_list = []\n",
    "\n",
    "for iso in METADATA_CLEAR['ISO'].unique():\n",
    "\n",
    "    META_iso = METADATA_CLEAR[METADATA_CLEAR['ISO'] == iso]\n",
    "\n",
    "    # Si una regi贸n tiene muy pocas estaciones, evita errores\n",
    "    if len(META_iso) < 2:\n",
    "        Q_train_list.append(META_iso)\n",
    "        continue\n",
    "\n",
    "    META_80 = META_iso.sample(frac=frac, random_state=seed)\n",
    "    META_20 = META_iso.drop(META_80.index)\n",
    "\n",
    "    Q_train_list.append(META_80)\n",
    "    Q_val_list.append(META_20)\n",
    "\n",
    "Q_train = pd.concat(Q_train_list, ignore_index=True)\n",
    "Q_val = pd.concat(Q_val_list, ignore_index=True)\n",
    "list_train = Q_train.File.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_base = os.path.join('/','media','arturo','T9','Data','Italy','Satellite','6_DOWN_BCorrected','QUANTILE')\n",
    "dir_input = os.path.join(os.path.join(dir_base, f'ITALY_DOWN_{product}_{time_reso}_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson_QQc_LLc_{str(seed).zfill(4)}.nc'))\n",
    "DATA = xr.open_dataset(dir_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b936964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Tr_index = 3\n",
    "# print(f'Tr: {Tr[Tr_index]} years')\n",
    "Tr = [5,  10,  20,  50, 100, 200]\n",
    "Fi = 1 - 1/np.array(Tr)\n",
    "ISO_names = np.unique(METADATA.ISO.values)\n",
    "\n",
    "INFO_region = {}\n",
    "WEIBULL_region = {}\n",
    "QUANTILES_region = {}\n",
    "\n",
    "for rr in range(len(ISO_names)):\n",
    "    region_ISO = ISO_names[rr]\n",
    "\n",
    "    INFO_dict = {}\n",
    "    QUANTILES_dict = {}\n",
    "\n",
    "    # print(f'{rr+1}: {region_ISO}')\n",
    "\n",
    "    METADATA_clear = Q_val[Q_val['ISO']==region_ISO].reset_index(inplace=False) # only validation METADATA\n",
    "\n",
    "    for nn in range(len(METADATA_clear)):#len(METADATA_clear)\n",
    "        filename = f'{METADATA_clear['File'].values[nn]}'\n",
    "        lat_obs = METADATA_clear['Lat'][nn]\n",
    "        lon_obs = METADATA_clear['Lon'][nn]\n",
    "        elev_obs = METADATA_clear['DEM_Elevation'][nn]\n",
    "\n",
    "        OBS_pd = pd.read_csv(os.path.join(obs_base, 'Weibull', '1dy', region_ISO, filename))\n",
    "        OBS_pd = OBS_pd[(OBS_pd['Year']>=2002)&(OBS_pd['Year']<=2023)].reset_index(drop=True)\n",
    "        \n",
    "        if len(OBS_pd) == 0:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            OBS_N = OBS_pd['N'].values\n",
    "            OBS_C = OBS_pd['C'].values\n",
    "            OBS_W = OBS_pd['W'].values\n",
    "            OBS_Y = OBS_pd['Year'].values\n",
    "\n",
    "            mask = ~np.isnan(OBS_N)\n",
    "\n",
    "            OBS_N = OBS_N[mask]\n",
    "            OBS_C = OBS_C[mask]\n",
    "            OBS_W = OBS_W[mask]\n",
    "            OBS_Y = OBS_Y[mask]\n",
    "\n",
    "            if len(OBS_Y) >= 8: # greather than 8 years\n",
    "\n",
    "                x0 = np.nanmean(OBS_C)\n",
    "                OBS_Q, flag = DOWN_raw.mev_quant_update(Fi, x0, OBS_N, OBS_C, OBS_W, thresh=1)\n",
    "                OBS_Q2 = np.where(flag, OBS_Q, np.nan)\n",
    "\n",
    "                PREC_SAT = DATA.sel(lat=lat_obs, lon=lon_obs, method='nearest')\n",
    "                lat_ref = float(PREC_SAT.lat.values)\n",
    "                lon_ref = float(PREC_SAT.lon.values)\n",
    "\n",
    "                INFO = pd.DataFrame({'lat_obs':[lat_obs], 'lon_obs':[lon_obs], 'elev_obs':[elev_obs], 'lat_ref':[lat_ref], 'lon_ref':[lon_ref]})\n",
    "                \n",
    "                Mevd_tmp = PREC_SAT.MEVd_Down.data\n",
    "                Mevd_LLc_tmp = PREC_SAT.MEVd_LLc.data\n",
    "                Mevd_QQc_tmp = PREC_SAT.MEVd_QQc.data\n",
    "                \n",
    "                QUANTILES = pd.DataFrame({'Tr':[50], 'OBS':OBS_Q2[3], 'DOWN':Mevd_tmp, 'LLc':Mevd_LLc_tmp, 'QQc':Mevd_QQc_tmp})\n",
    "                \n",
    "                INFO_dict[filename] = INFO\n",
    "                QUANTILES_dict[filename] = QUANTILES\n",
    "    \n",
    "    INFO_region[region_ISO] = INFO_dict\n",
    "    QUANTILES_region[region_ISO] = QUANTILES_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8f906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export as: /media/arturo/T9/Data/Italy/statistics/QUANTILE/statistics_obs_IMERG_corrected_QQc_LLc_0008.h5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dir_out = os.path.join('/','media','arturo','T9','Data','Italy','statistics','QUANTILE')\n",
    "hdf5_file = os.path.join(dir_out, f'statistics_obs_{product}_corrected_QQc_LLc_{str(seed).zfill(4)}.h5')\n",
    "\n",
    "print(f'Export as: {hdf5_file}')\n",
    "\n",
    "with pd.HDFStore(hdf5_file, mode='w') as store:\n",
    "\n",
    "    for region_ISO in INFO_region.keys():\n",
    "        stations = INFO_region[region_ISO].keys()  # las estaciones de la regi贸n\n",
    "\n",
    "        for station in stations:\n",
    "\n",
    "            info_df = INFO_region[region_ISO][station]\n",
    "            quantiles_df = QUANTILES_region[region_ISO][station]\n",
    "\n",
    "            store[f\"/{region_ISO}/{station}/INFO\"] = info_df\n",
    "            store[f\"/{region_ISO}/{station}/QUANTILES\"] = quantiles_df\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334f7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AXE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
