{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "364d32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import scipy.stats as stats\n",
    "\n",
    "from matplotlib import patches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "import shapely.geometry as sg\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from function import DOWN_raw\n",
    "from function import ART_preprocessing as ART_pre\n",
    "\n",
    "from playsound import playsound\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb6011",
   "metadata": {},
   "source": [
    "## Export the RE and important info from Corrected RSR/Ensemble data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSEMBLE_SAT = 'ALL'\n",
    "product, time_reso = 'ENSEMBLE_mean', '1dy'\n",
    "# product, time_reso = 'ENSEMBLE_median', '1dy'\n",
    "\n",
    "# product, time_reso = 'IMERG', '1dy'\n",
    "# product, time_reso = 'CMORPH', '3h'\n",
    "# product, time_reso = 'ERA5', '3h'\n",
    "# product, time_reso = 'MSWEP', '3h'\n",
    "# product, time_reso = 'CHIRPS', '1dy'\n",
    "# product, time_reso = 'GSMaP', '3h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41fc8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction, nameout, label = 'quantile-quantile', 'QQc', 'ISIMIP_QM'\n",
    "correction, nameout, label = 'linear-regression', 'LRc', 'Linear Regression'\n",
    "\n",
    "dir_out = 'TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11dd18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.7\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee1787a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_min, lon_max, lat_min, lat_max, area, toll = 6.5, 19, 36.5, 48, 'ITALY', 0.002\n",
    "\n",
    "Tr = [5,  10,  20,  50, 100, 200]\n",
    "Fi = 1 - 1/np.array(Tr)\n",
    "\n",
    "veneto_dir = os.path.join('/','media','arturo','T9','Data','shapes','Europa','Italy')\n",
    "\n",
    "if os.path.exists(veneto_dir):\n",
    "    REGIONS = gpd.read_file(os.path.join(veneto_dir,'Italy_regions.geojson'))\n",
    "else:\n",
    "    raise SystemExit(f\"File not found: {veneto_dir}\")\n",
    "\n",
    "obs_base = os.path.join('/','media','arturo','T9','Data','Italy','Rain_Gauges_QC')\n",
    "bias_base = os.path.join('/','media','arturo','T9','Data','Italy','Satellite','6_DOWN_BCorrected',dir_out)\n",
    "\n",
    "METADATA = pd.read_csv(os.path.join(obs_base, 'data', 'METADATA', 'METADATA_FTS_QCv4_Case1_wAIRHO_v3_1dy.csv'))\n",
    "METADATA[\"Lat\"] = np.round(METADATA[\"Lat\"], 6)\n",
    "METADATA[\"Lon\"] = np.round(METADATA[\"Lon\"], 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1aafc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: ITALY_ENSEMBLE_ALL_1dy_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson_mean_LRc.nc\n",
      "Export as: /media/arturo/T9/Data/Italy/statistics/TEST/statistics_obs_ENSEMBLE_ALL_mean_corrected_LRc_0007.h5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ISO_list = METADATA.ISO.unique()\n",
    "\n",
    "Q_train_list = []\n",
    "Q_val_list = []\n",
    "\n",
    "for iso in METADATA['ISO'].unique():\n",
    "    \n",
    "    META_iso = METADATA[METADATA['ISO'] == iso]\n",
    "\n",
    "    # Si una región tiene muy pocas estaciones, evita errores\n",
    "    if len(META_iso) < 2:\n",
    "        Q_train_list.append(META_iso)\n",
    "        continue\n",
    "\n",
    "    META_80 = META_iso.sample(frac=frac, random_state=seed)\n",
    "    META_20 = META_iso.drop(META_80.index)\n",
    "\n",
    "    Q_val_list.append(META_20)\n",
    "\n",
    "Q_val = pd.concat(Q_val_list, ignore_index=True)\n",
    "\n",
    "# print(f'Load {product}')\n",
    "if product == 'ENSEMBLE_mean':\n",
    "    dir_in = os.path.join(bias_base,f'ITALY_ENSEMBLE_{ENSEMBLE_SAT}_{time_reso}_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson_mean_{nameout}.nc')\n",
    "elif product == 'ENSEMBLE_median':\n",
    "    dir_in = os.path.join(bias_base,f'ITALY_ENSEMBLE_{ENSEMBLE_SAT}_{time_reso}_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson_median_{nameout}.nc')\n",
    "else:\n",
    "    dir_in = os.path.join(bias_base, f'ITALY_DOWN_{product}_{time_reso}_2002_2023_npix_2_thr_1_acf_mar_genetic_pearson_{nameout}.nc')\n",
    "\n",
    "print(f'Reading: {dir_in.split('/')[-1]}')\n",
    "data = xr.open_dataset(dir_in)\n",
    "\n",
    "Tr_index = 3\n",
    "# print(f'Tr: {Tr[Tr_index]} years')\n",
    "\n",
    "ISO_names = np.unique(METADATA.ISO.values)\n",
    "\n",
    "INFO_region = {}\n",
    "WEIBULL_region = {}\n",
    "QUANTILES_region = {}\n",
    "\n",
    "for rr in range(len(ISO_names)):\n",
    "    region_ISO = ISO_names[rr]\n",
    "\n",
    "    INFO_dict = {}\n",
    "    WEIBULL_dict = {}\n",
    "    QUANTILES_dict = {}\n",
    "\n",
    "    # print(f'{rr+1}: {region_ISO}')\n",
    "\n",
    "    METADATA_clear = Q_val[Q_val['ISO']==region_ISO].reset_index(inplace=False)\n",
    "\n",
    "    for nn in range(len(METADATA_clear)):#len(METADATA_clear)\n",
    "        filename = f'{METADATA_clear['File'].values[nn]}'\n",
    "        lat_obs = METADATA_clear['Lat'][nn]\n",
    "        lon_obs = METADATA_clear['Lon'][nn]\n",
    "        elev_obs = METADATA_clear['DEM_Elevation'][nn]\n",
    "\n",
    "        OBS_pd = pd.read_csv(os.path.join(obs_base, 'Weibull', '1dy', region_ISO, filename))\n",
    "        OBS_pd = OBS_pd[(OBS_pd['Year']>=2002)&(OBS_pd['Year']<=2023)].reset_index(drop=True)\n",
    "        \n",
    "        if len(OBS_pd) == 0:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            OBS_N = OBS_pd['N'].values\n",
    "            OBS_C = OBS_pd['C'].values\n",
    "            OBS_W = OBS_pd['W'].values\n",
    "            OBS_Y = OBS_pd['Year'].values\n",
    "\n",
    "            mask = ~np.isnan(OBS_N)\n",
    "\n",
    "            OBS_N = OBS_N[mask]\n",
    "            OBS_C = OBS_C[mask]\n",
    "            OBS_W = OBS_W[mask]\n",
    "            OBS_Y = OBS_Y[mask]\n",
    "\n",
    "            if len(OBS_Y) >= 8: # greather than 8 years\n",
    "\n",
    "                x0 = np.nanmean(OBS_C)\n",
    "                OBS_Q, flag = DOWN_raw.mev_quant_update(Fi, x0, OBS_N, OBS_C, OBS_W, thresh=1)\n",
    "                OBS_Q2 = np.where(flag, OBS_Q, np.nan)\n",
    "\n",
    "                PREC_SAT = data.sel(lat=lat_obs, lon=lon_obs, method='nearest')\n",
    "                lat_ref = float(PREC_SAT.lat.values)\n",
    "                lon_ref = float(PREC_SAT.lon.values)\n",
    "\n",
    "                INFO = pd.DataFrame({'lat_obs':[lat_obs], 'lon_obs':[lon_obs], 'elev_obs':[elev_obs], 'lat_ref':[lat_ref], 'lon_ref':[lon_ref]})\n",
    "\n",
    "                # Sat_raw_pd = pd.DataFrame({'Year':PREC_SAT.year.values, 'N':PREC_SAT.NYs.values, 'C':PREC_SAT.CYs.values, 'W':PREC_SAT.WYs.values})\n",
    "                # Sat_raw_pd = Sat_raw_pd.set_index('Year').loc[OBS_pd['Year']].reset_index()\n",
    "                # x0 = np.nanmean(Sat_raw_pd.C.values)\n",
    "                # SAT_raw_Q, flag = DOWN_raw.mev_quant_update(Fi, x0, Sat_raw_pd.N.values, Sat_raw_pd.C.values, Sat_raw_pd.W.values, thresh=1)\n",
    "                # SAT_raw_Q2 = np.where(flag, SAT_raw_Q, np.nan)\n",
    "\n",
    "                Sat_down_pd = pd.DataFrame({'Year':PREC_SAT.year.values, 'N':PREC_SAT.NYd.values, 'C':PREC_SAT.CYd.values, 'W':PREC_SAT.WYd.values})\n",
    "                Sat_down_pd = Sat_down_pd.set_index('Year').loc[OBS_pd['Year']].reset_index()\n",
    "                x0 = np.nanmean(Sat_down_pd.C.values)\n",
    "                SAT_down_Q, flag = DOWN_raw.mev_quant_update(Fi, x0, Sat_down_pd.N.values, Sat_down_pd.C.values, Sat_down_pd.W.values, thresh=1)\n",
    "                SAT_down_Q2 = np.where(flag, SAT_down_Q, np.nan)\n",
    "\n",
    "                WEIBULL = pd.DataFrame({'Year':OBS_pd.Year, \n",
    "                            'N_obs':OBS_pd.N, 'C_obs':OBS_pd.C, 'W_obs':OBS_pd.W,\n",
    "                            # 'N_raw':Sat_raw_pd.N, 'C_raw':Sat_raw_pd.C, 'W_raw':Sat_raw_pd.W,\n",
    "                            'N_down':Sat_down_pd.N, 'C_down':Sat_down_pd.C, 'W_down':Sat_down_pd.W})\n",
    "\n",
    "                # re_raw = (SAT_raw_Q2 - OBS_Q2)/OBS_Q2\n",
    "                re_down = (SAT_down_Q2 - OBS_Q2)/OBS_Q2\n",
    "\n",
    "                QUANTILES = pd.DataFrame({'Tr':Tr, 'OBS':OBS_Q2, 'SAT_down':SAT_down_Q2, 'RE_down':re_down})\n",
    "\n",
    "                INFO_dict[filename] = INFO\n",
    "                WEIBULL_dict[filename] = WEIBULL\n",
    "                QUANTILES_dict[filename] = QUANTILES\n",
    "\n",
    "    INFO_region[region_ISO] = INFO_dict\n",
    "    WEIBULL_region[region_ISO] = WEIBULL_dict\n",
    "    QUANTILES_region[region_ISO] = QUANTILES_dict\n",
    "\n",
    "if \"ENSEMBLE\" in product:\n",
    "    # print(\"ENSEMBLE Product\")\n",
    "    product_ = product.replace('ENSEMBLE_',f'ENSEMBLE_{ENSEMBLE_SAT}_')\n",
    "    hdf5_file = f\"/media/arturo/T9/Data/Italy/statistics/{dir_out}/statistics_obs_{product_}_corrected_{nameout}_{str(seed).zfill(4)}.h5\"\n",
    "else:\n",
    "    # print(\"No-ENSEMBLE Product\")\n",
    "    if product == 'GSMaP_NoCorrection':\n",
    "        hdf5_file = f\"/media/arturo/T9/Data/Italy/statistics/{dir_out}/statistics_obs_GSMaP_NoCorrection_corrected_{nameout}_{str(seed).zfill(4)}.h5\"\n",
    "    else:\n",
    "        hdf5_file = f\"/media/arturo/T9/Data/Italy/statistics/{dir_out}/statistics_obs_{product}_corrected_{nameout}_{str(seed).zfill(4)}.h5\"\n",
    "\n",
    "print(f'Export as: {hdf5_file}')\n",
    "\n",
    "with pd.HDFStore(hdf5_file, mode='w') as store:\n",
    "\n",
    "    for region_ISO in INFO_region.keys():\n",
    "        stations = INFO_region[region_ISO].keys()  # las estaciones de la región\n",
    "\n",
    "        for station in stations:\n",
    "\n",
    "            info_df = INFO_region[region_ISO][station]\n",
    "            weibull_df = WEIBULL_region[region_ISO][station]\n",
    "            quantiles_df = QUANTILES_region[region_ISO][station]\n",
    "\n",
    "            store[f\"/{region_ISO}/{station}/INFO\"] = info_df\n",
    "            store[f\"/{region_ISO}/{station}/WEIBULL\"] = weibull_df\n",
    "            store[f\"/{region_ISO}/{station}/QUANTILES\"] = quantiles_df\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83673ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AXE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
