{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit, minimize, fsolve\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "from scipy.special import gamma\n",
    "from scipy.integrate import dblquad, nquad\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "def matplotlib_update_settings():\n",
    "    # http://wiki.scipy.org/Cookbook/Matplotlib/LaTeX_Examples\n",
    "    # this is a latex constant, don't change it.\n",
    "    pts_per_inch = 72.27\n",
    "    # write \"\\the\\textwidth\" (or \"\\showthe\\columnwidth\" for a 2 collumn text)\n",
    "    text_width_in_pts = 300.0\n",
    "    # inside a figure environment in latex, the result will be on the\n",
    "    # dvi/pdf next to the figure. See url above.\n",
    "    text_width_in_inches = text_width_in_pts / pts_per_inch\n",
    "    # make rectangles with a nice proportion\n",
    "    golden_ratio = 0.618\n",
    "    # figure.png or figure.eps will be intentionally larger, because it is prettier\n",
    "    inverse_latex_scale = 2\n",
    "    # when compiling latex code, use\n",
    "    # \\includegraphics[scale=(1/inverse_latex_scale)]{figure}\n",
    "    # we want the figure to occupy 2/3 (for example) of the text width\n",
    "    fig_proportion = (3.0 / 3.0)\n",
    "    csize = inverse_latex_scale * fig_proportion * text_width_in_inches\n",
    "    # always 1.0 on the first argument\n",
    "    fig_size = (1.0 * csize, 0.8 * csize)\n",
    "    # find out the fontsize of your latex text, and put it here\n",
    "    text_size = inverse_latex_scale * 12\n",
    "    tick_size = inverse_latex_scale * 8\n",
    "\n",
    "    # learn how to configure:\n",
    "    # http://matplotlib.sourceforge.net/users/customizing.html\n",
    "    params = {\n",
    "        'axes.labelsize': text_size,\n",
    "        'legend.fontsize': tick_size,\n",
    "        'legend.handlelength': 2.5,\n",
    "        'legend.borderaxespad': 0,\n",
    "        'xtick.labelsize': tick_size,\n",
    "        'ytick.labelsize': tick_size,\n",
    "        'font.size': text_size,\n",
    "        'text.usetex': True,\n",
    "        'figure.figsize': fig_size,\n",
    "        # include here any neede package for latex\n",
    "        'text.latex.preamble': [r'\\usepackage{amsmath}',do\n",
    "                                ],\n",
    "    }\n",
    "    plt.rcParams.update(params)\n",
    "    return\n",
    "\n",
    "\n",
    "def haversine(lat1, lat2, lon1, lon2,\n",
    "                convert_to_rad=True):\n",
    "    '''compute haversine distance btw 2 points.\n",
    "    by default provide angles in degrees.\n",
    "    Return distance in km'''\n",
    "\n",
    "    def torad(theta):\n",
    "        # convert angle to radiants\n",
    "        return theta*np.pi/180.0\n",
    "\n",
    "    if convert_to_rad:\n",
    "        lat1 = torad(lat1)\n",
    "        lat2 = torad(lat2)\n",
    "        lon1 = torad(lon1)\n",
    "        lon2 = torad(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    R = 6371.0 # km\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    dist = 2*R*np.arctan2( np.sqrt(a), np.sqrt(1-a))\n",
    "    return dist\n",
    "\n",
    "\n",
    "def area_lat_long(lat_c, lon_c, dlat, dlon):\n",
    "    \"\"\"----------------------------------------------------------\n",
    "    INPUTS: latitude, longitude of pixel center\n",
    "    and delta lat, delta long of pixel size.\n",
    "    Everything in degrees\n",
    "    RETURNS::\n",
    "    my_edge: characteristic pixel size ( L = (L1*L2)**0.5 ) [km]\n",
    "    my_area: pixel area A = L1*L2 [km**2]\n",
    "    hor_size: horizontal pixel size, L1 [km]\n",
    "    vert_size: vertical pixel size, L2 [km]\n",
    "    --------------------------------------------------------------\"\"\"\n",
    "    lat1 = lat_c - dlat/2\n",
    "    lat2 = lat_c + dlat/2\n",
    "    lon1 = lon_c - dlon/2\n",
    "    lon2 = lon_c + dlon/2\n",
    "    hor_size = haversine(lat1, lat1, lon1, lon2)\n",
    "    vert_size = haversine(lat1, lat2, lon1, lon1)\n",
    "    my_area = hor_size*vert_size\n",
    "    my_edge = np.sqrt(my_area)\n",
    "    return my_edge, my_area, hor_size, vert_size\n",
    "\n",
    "def downscale_pwet(xdata, *, thresh=1, dt=3, L1=10,\n",
    "                    target_x, target_t=24,\n",
    "                    origin_x, origin_t=24, ninterp=1000, plot=False):\n",
    "    '''------------------------------------------------------------------------\n",
    "    Use a Taylor hypothesis to trade space for time and give an estimate\n",
    "    of the wet fraction above a certain threshold at a spatial scale smaller\n",
    "    that the grid cell size.\n",
    "\n",
    "    INPUT:\n",
    "        xdata: X-ARRAY with the precipitation ACCUMULATIONS at scale dt\n",
    "                must already be loaded in memory\n",
    "                negative values must be already removed and set to np.nan\n",
    "                has dimensions (lat, lon, time)\n",
    "                time step MUST BE between 0.5 and 3 HRS (IMERG / TMPA)\n",
    "                must be square (same dimension in x and y)\n",
    "               ***SEE FUNCTION CALLED BELOW FOR MORE INFORMATION***\n",
    "\n",
    "        thresh: threshold for computing wet fraction (default 1 prcp unit)\n",
    "        dt: time scale of the precipitation product [HOURS] (default 3 hours)\n",
    "        L1: linear spatial scale of a grid cell [km] (default is 25 km)\n",
    "        target_x: subgrid spatial scale we want pwet at [km](default 0.0001 km)\n",
    "        target_t: target time scale, in [HOURS] (default 24 hours)\n",
    "        origin_x: linear spatial scale of origin gridded prcp [km] (default 25)\n",
    "        origin_t: time scale of origin gridded pecip [HOURS] (default 24 hours)\n",
    "        ninterp=1000: number of interpolation in time dimension\n",
    "        plot=False: only True if you wanna see fancy plots\n",
    "    OUTPUT:\n",
    "        beta: ratio between pwet at the grid cell scale ()\n",
    "                to pwet at the target subgrid scale ()\n",
    "\n",
    "    NOTE: TESTED ONLY WITH TRMM - TMPA - 3B42 - 3 hourly * 0.25 deg product!\n",
    "            FOR PRECIPITATION ACCUMULATION TARGETS AT THE DAILY SCALE\n",
    "    ------------------------------------------------------------------------'''\n",
    "    pwets, xscales, tscales = compute_pwet_xr(xdata, thresh,\n",
    "                                    cube1size=3, dt=dt, tmax=48)\n",
    "    resbeta = Taylor_beta(pwets, xscales, tscales, L1=L1,\n",
    "                            target_x=target_x, target_t=target_t,\n",
    "                            origin_x=origin_x, origin_t=origin_t,\n",
    "                            ninterp=ninterp, plot=plot)\n",
    "    return resbeta\n",
    "\n",
    "\n",
    "def compute_pwet_xr(xray, thresh, *,\n",
    "                    cube1size=3, dt=3, tmax=48):\n",
    "    '''-----------------------------------------------------------------------\n",
    "    Compute the fraction of observations above a given threshold for\n",
    "    different integration scales (in time) and averaging scales (space)\n",
    "\n",
    "    INPUT:  xray: xarray with dimensions lat, lon, time\n",
    "                    (must be already loaded in memory,\n",
    "                    and missing values must be converted to np.nan in advance:\n",
    "                    here np.nan are propagated until the end when integrating.\n",
    "                    time step MUST BE between 0.5 and 3 HRS (IMERG / TMPA)\n",
    "                    Dimension in pixel of LAT and LON must be equal (square)\n",
    "                    and sufficiently long record in time to compute reliably.\n",
    "\n",
    "\n",
    "            thresh:  threshold for determining wet fraction\n",
    "            cube1size = 1: lateral size of cube used to decide\n",
    "                        among how many single pixel average at smallest scale\n",
    "            dt = 3: temporal resolution of observations [HOURS]\n",
    "            tmax = 48: maximum time scale of integration [HOURS]\n",
    "\n",
    "\n",
    "    OUTPUT: pwets: numpy array of shape tscales*xscales with the values\n",
    "                    of wet fraction at different time / space scales\n",
    "            xscales: spatial scales (dimensionless, relative to pixel size)\n",
    "            tscales: temporal scales (in HOURS!)\n",
    "\n",
    "    Note: at the smallest scale (xscale=1), values are computed for each\n",
    "            time series within a centered cube of size cube1size,\n",
    "            and then averaged.\n",
    "\n",
    "            At the largest spatial scale, spatial average includes all the\n",
    "            pixels in the array\n",
    "\n",
    "            at intermediate scales, the wet fraction is computed for 4 boxes\n",
    "            starting at the 4 corners of the lattice, and averaged.\n",
    "    -----------------------------------------------------------------------'''\n",
    "\n",
    "    smax = xray.shape[0] # max spatial scale\n",
    "    tscales = np.array([1, 2, 3, 4, 5, 6, 8, 10, 12, 16, 20, 24, 36, 48, 96])*dt\n",
    "    tscales = tscales[tscales < tmax + 0.001]\n",
    "    xscales = np.arange(1, smax+1)\n",
    "    ntscales = np.size(tscales)\n",
    "    nsscales = np.size(xscales)\n",
    "    pwets = np.zeros((ntscales, nsscales))\n",
    "\n",
    "    def wetfrac(array, thresh):\n",
    "        return np.size(array[array > thresh])/np.size(array)\n",
    "\n",
    "    for it, st in enumerate(tscales):\n",
    "        datamat = xray.resample(time='{}h'.format(st)).sum(\n",
    "                                    dim='time', skipna = False)\n",
    "\n",
    "        for ix, sx in enumerate(xscales):\n",
    "\n",
    "            if sx == 1: # pixel scale\n",
    "\n",
    "                toskip = smax - cube1size\n",
    "                if toskip % 2 == 0:\n",
    "                    buffer1 = toskip // 2\n",
    "                    buffer2 = toskip // 2\n",
    "                else:\n",
    "                    buffer1 = toskip // 2\n",
    "                    buffer2 = buffer1 + 1\n",
    "\n",
    "                if buffer2 > 0:\n",
    "                    aggt1 = datamat[buffer1:-buffer2, buffer1:-buffer2, :]\n",
    "                else:\n",
    "                    aggt1 = datamat[buffer1:, buffer1:, :]\n",
    "\n",
    "                # print('shape of p1 array = ', aggt1.shape)\n",
    "\n",
    "                p1 = np.zeros((aggt1.shape[0], aggt1.shape[1]))\n",
    "\n",
    "\n",
    "                for x in range(aggt1.shape[0]):\n",
    "                    for y in range(aggt1.shape[1]):\n",
    "                        # print('Hello1')\n",
    "                        p1[x, y] = wetfrac(aggt1[x, y, :].dropna(\n",
    "                                            dim='time', how='any'), thresh)\n",
    "                pwets[it, ix] = np.mean(p1)\n",
    "\n",
    "\n",
    "            elif sx == smax: # largest scale: simple average\n",
    "                pwets[it, ix] = wetfrac( datamat.mean(dim=('lat', 'lon'),\n",
    "                                skipna = False).dropna(dim='time', how='any'),\n",
    "                                thresh)\n",
    "\n",
    "            else: # intermediate scales\n",
    "                c1 = np.zeros(4)\n",
    "                c1[0] = wetfrac(datamat[:sx, :sx, :].mean(dim=('lat', 'lon'),\n",
    "                                skipna=False).dropna(dim='time', how='any'),\n",
    "                                thresh)\n",
    "                c1[1] = wetfrac(datamat[-sx:, :sx, :].mean(dim=('lat', 'lon'),\n",
    "                                skipna=False).dropna(dim='time', how='any'),\n",
    "                                thresh)\n",
    "                c1[2] = wetfrac(datamat[:sx, :sx, :].mean(dim=('lat', 'lon'),\n",
    "                                skipna=False).dropna(dim='time', how='any'),\n",
    "                                thresh)\n",
    "                c1[3] = wetfrac(datamat[-sx:, :sx, :].mean(dim=('lat', 'lon'),\n",
    "                                skipna=False).dropna(dim='time', how='any'),\n",
    "                                thresh)\n",
    "                pwets[it, ix] = np.mean(c1)\n",
    "    return pwets, xscales, tscales\n",
    "\n",
    "\n",
    "def Taylor_beta(pwets, xscales, tscales, *, L1=10, target_x=0.001, target_t=24,\n",
    "                    origin_x=10, origin_t=3, ninterp = 1000, plot=False):\n",
    "    '''------------------------------------------------------------------\n",
    "    Extrapolate the wet fraction of the rainfall fields at small scales\n",
    "    smaller than the resolution of the gridded precipitation product.\n",
    "    INPUT:\n",
    "        pwets: array of wet fractions for different integration scales.\n",
    "                Must have shape tscales*xscales\n",
    "        xscales: array of spatial scales (DIMENSIONLESS, in pixel units!!!)\n",
    "        tscales: array of temporal scales (DIMENSIONAL, in [HOURS])\n",
    "        L1: pixel linear size, in [Km]\n",
    "        target_x: linear scale of target, in [Km] (e.g.,rain gauge measurem.)\n",
    "        target_t: time scale of target, in [HOURS] (default 24 hours)\n",
    "        origin_x: origin linear spatial scale, in [Km] (default 25 Km)\n",
    "        origin_t: origin time scale, in [Hours] (default 24 hours)\n",
    "        ninterp: number of point for interpolation in time (default = 1000)\n",
    "        plot: if plot == True, produces some nice plots (default is false)\n",
    "\n",
    "\n",
    "    OUTPUT:\n",
    "        beta -> ratio between the pwet at scales (origin_x, origin_t)\n",
    "                e.g., known satellite pixel aggregated at the daily scale,\n",
    "                to pwet at a smaller spatial scale (target_x, target_y)\n",
    "\n",
    "    NOTE:\n",
    "        For now: The origin_x scale must be one where the pwet is known,\n",
    "        so one of the values in xscales. Otherwise an additional\n",
    "        interpolation is necessary (furure release)\n",
    "\n",
    "    NOTE: TESTED ONLY WITH TRMM - TMPA - 3B42 - 3 hourly * 0.25 deg product!\n",
    "    ------------------------------------------------------------------'''\n",
    "    xscales_km = xscales*L1\n",
    "    ntscales = np.size(tscales)\n",
    "    nxscales = np.size(xscales)\n",
    "    tscales_int = np.linspace(np.min(tscales), np.max(tscales), ninterp)\n",
    "    pwmat_int = np.zeros((ninterp, nxscales))\n",
    "    for col in range(nxscales):\n",
    "        pwmat_int[:, col] = np.interp(tscales_int, tscales, pwets[:, col])\n",
    "\n",
    "    # dxv, dtv = np.meshgrid(xscales_km, tscales_int )\n",
    "    pw_min = np.min(pwets)\n",
    "    pw_max = np.max(pwets)\n",
    "    mypw = np.linspace(pw_min, pw_max, ninterp)\n",
    "\n",
    "    myU = np.zeros(ninterp)   # initialize linear slope\n",
    "    myX0 = np.zeros(ninterp)  # initialize linear intercept\n",
    "    myindices = np.zeros((ninterp, nxscales), dtype = int)\n",
    "    # tvec =  np.zeros((ninterp, nxscales))\n",
    "\n",
    "    for ii in range(ninterp):\n",
    "        Tvec = np.zeros(nxscales)\n",
    "        for jj in range(nxscales):\n",
    "            myindices[ii,jj] = np.argmin(np.abs(pwmat_int[:, jj] - mypw[ii]))\n",
    "            # tvec[ii,jj] = tscales_int[myindices[ii,jj]]\n",
    "            Tvec[jj] = tscales_int[myindices[ii,jj]]\n",
    "        # warnings.simplefilter('ignore', np.RankWarning)\n",
    "        res = np.polyfit(Tvec[:2], xscales_km[:2], 1)\n",
    "        myU[ii] = res[0]\n",
    "        myX0[ii] = res[1]\n",
    "\n",
    "    # first remove timescales where the line goes out of bounds\n",
    "    min_index = myindices.min(axis=1)\n",
    "    max_index = myindices.max(axis=1)\n",
    "    col_to_keep = np.logical_and(min_index > 0, max_index < ninterp-1)\n",
    "    mypw2 = mypw[col_to_keep]\n",
    "    U = myU[col_to_keep]\n",
    "    X0 = myX0[col_to_keep]\n",
    "\n",
    "    # best_index = this is the index of the pwet such that\n",
    "    # the line extrapolated to the gauge spatial scale\n",
    "    # matches the requested time scale\n",
    "    dthat = (target_x - X0) / U # compute time scale that would give me pwet\n",
    "    deltat = np.abs(dthat - target_t)\n",
    "    opt_deltat = np.min(deltat)\n",
    "    max_dt = 0.5 # half hour accuracy should suffice?\n",
    "    # set a TOLL value to check we are not too far away from the real deltat\n",
    "    if opt_deltat > max_dt:\n",
    "        print('Taylor_Beta WARNING: not enough accuracy!')\n",
    "        print('the time resolution of the coarse data might not be enough!')\n",
    "        print('or try to increase the value of interp')\n",
    "    best_index = np.argmin(deltat)  # best prediction of 24 hour interval\n",
    "    pwet_target = mypw2[best_index]\n",
    "\n",
    "    # compute pwet at the original scale:\n",
    "    # this is ok if origin_x is one of the points where pwet was computed\n",
    "    # if not, add an interpolation in the x direction!\n",
    "    pos_xmin = np.argmin(np.abs(origin_x - xscales_km))\n",
    "    pos_tmin = np.argmin(np.abs(origin_t - tscales_int))\n",
    "    pwet_origin = pwmat_int[pos_tmin, pos_xmin]\n",
    "    beta = pwet_origin / pwet_target\n",
    "    # return beta\n",
    "    res = {}\n",
    "    res['beta'] = beta\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(tscales, pwets[:,0], 'ok')\n",
    "        plt.plot(tscales, pwets[:,1], 'ob')\n",
    "        plt.plot(tscales_int, pwmat_int[:,0], '-k', label = 'dx')\n",
    "        plt.plot(tscales_int, pwmat_int[:,1], '-b', label = '2dx')\n",
    "        if nxscales > 2:\n",
    "            plt.plot(tscales, pwets[:,2], 'og')\n",
    "            plt.plot(tscales_int, pwmat_int[:,2], '-g', label = '3dx')\n",
    "        plt.xlabel('Time scale [hours]')\n",
    "        plt.ylabel('wet fraction')\n",
    "        plt.title('Observed wet fraction')\n",
    "        plt.legend()\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        res['fig'] = fig\n",
    "\n",
    "        # PLOT CONTOUR\n",
    "        xxx1 = np.linspace(0, 50, 10)\n",
    "        xxx2 = np.linspace(50, 75, 10)\n",
    "        contour = plt.figure()\n",
    "        # PS3 = plt.contourf(dxv, dtv, Nmatv)\n",
    "        PS3 = plt.contourf(xscales_km, tscales_int, pwmat_int)\n",
    "        cbar = plt.colorbar(PS3)\n",
    "        cbar.set_label(r'$p_r$')\n",
    "        cbar.set_label(r'$p_r$', rotation=270)\n",
    "        for ii in range(nxscales):\n",
    "            for jj in range(ntscales):\n",
    "                plt.plot(xscales_km[ii], tscales[jj], 'sk')\n",
    "        plt.plot(xxx1, (xxx1 - X0[best_index]) / U[best_index], 'b')\n",
    "        plt.plot(xxx2, (xxx2 - X0[best_index]) / U[best_index], '--b')\n",
    "        plt.plot(target_x, target_t, 'sr')\n",
    "        plt.plot(origin_x, origin_t, '^r')\n",
    "        plt.xlabel('spatial scale [km]')\n",
    "        plt.ylabel('temporal scale [hours]')\n",
    "        plt.title('Wet Fraction Extrapolation')\n",
    "        plt.text(0.1, 24.5, 'Gauge', color='red')\n",
    "        plt.text(L1, 24.5, 'Grid cell', color='red')\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        res['contour'] = contour\n",
    "    return res\n",
    "\n",
    "\n",
    "################################ CORRELATION ##################################\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def corr(x, y):\n",
    "    '''correlation between 2 numpy arrays'''\n",
    "    xn = (x - np.mean(x))/np.std(x)\n",
    "    yn = (y - np.mean(y))/np.std(y)\n",
    "    pearson_corr = np.mean(xn*yn)\n",
    "    return pearson_corr\n",
    "\n",
    "\n",
    "def str_exp_fun(x, d0, mu):\n",
    "    '''\n",
    "    Stretched exponential rainfall correlation function\n",
    "    from eg Habib and krajewski (2003)\n",
    "    or Villarini and Krajewski (2007)\n",
    "    with 2 parameters d0, mu (value as x = 0 is 1)\n",
    "    '''\n",
    "    x = np.asarray(x) # transform to numpy array\n",
    "    is_scalar = False if x.ndim > 0 else True # create flag for output\n",
    "    x.shape = (1,)*(1-x.ndim) + x.shape # give it dimension 1 if scalar\n",
    "    myfun = np.exp( -(x/d0)**mu)\n",
    "    myfun = myfun if not is_scalar else myfun[0]\n",
    "    return  myfun\n",
    "\n",
    "\n",
    "def epl_fun(x, epsilon, alpha):\n",
    "    '''\n",
    "    Marco's Exponential kernel + Power law tail\n",
    "    (autocorrelation) function with exponential nucleus and power law decay\n",
    "    for x> epsilon - 2 parameters\n",
    "    see Marani 2003 WRR for more details\n",
    "    '''\n",
    "    x = np.asarray(x) # transform to numpy array\n",
    "    is_scalar = False if x.ndim > 0 else True # create flag for output\n",
    "    x.shape = (1,)*(1-x.ndim) + x.shape # give it dimension 1 if scalar\n",
    "    m = np.size(x)\n",
    "    myfun = np.zeros(m)\n",
    "    for ii in range(m):\n",
    "        if x[ii] < 10e-6:\n",
    "            myfun[ii] = 1\n",
    "        elif x[ii] < epsilon:\n",
    "            myfun[ii] = np.exp(-alpha*x[ii]/epsilon)\n",
    "        else:\n",
    "            # silence it if negative parameters -\n",
    "            # with warnings.simplefilter('ignore', RuntimeWarning):\n",
    "            myfun[ii] = (epsilon/np.exp(1)/x[ii])**alpha\n",
    "            # print('epsilon = ', epsilon)\n",
    "            # if epsilon < 1e-5 or alpha < 1e-5:\n",
    "            #     print('epsilon = ', epsilon)\n",
    "            #     print('alpha =', alpha)\n",
    "            # else:\n",
    "    myfun = myfun if not is_scalar else myfun[0]\n",
    "    return  myfun\n",
    "\n",
    "\n",
    "def myacf_2d(x,y, parhat, acf = 'str'):\n",
    "    '''########################################################################\n",
    "    set of 2D autocorrelation functions\n",
    "    INPUTS::\n",
    "        x,y = variables of the ACF\n",
    "        parhat = set of the two parameters of the ACF\n",
    "        acf: which ACF. possible choices:\n",
    "            acf = 'str': stretched exponential ACF,\n",
    "                WITH PARAMETERS: scale d0 and shape mu\n",
    "            acf = 'mar': Marani 2003 exponential kernel + power law tail\n",
    "                WITH PARAMETERS: transition point epsilon and shape alpha\n",
    "            acf 'exp': 2d exponential function\n",
    "                WITH PARAMETERS: a and b --> scale in x and y axis respectively\n",
    "    OUTPUTS::\n",
    "        value of the ACF at a point\n",
    "    ########################################################################'''\n",
    "    d = np.sqrt(x**2 + y**2)\n",
    "    if acf == 'str': # par d0, mu0\n",
    "        d0 = parhat[0]\n",
    "        mu = parhat[1]\n",
    "        return np.exp( -(d/d0)**mu)\n",
    "    elif acf == 'mar': # par:: epsilon, alpha\n",
    "        # print('it actually is a Power Law')\n",
    "        epsilon = parhat[0]\n",
    "        alpha   = parhat[1]\n",
    "        return epl_fun(d, epsilon , alpha)\n",
    "\n",
    "\n",
    "def grid_corr(xdata, plot=True, thresh=0):\n",
    "    '''------------------------------------------------------------------------\n",
    "    Given an xarray with the data (dimensions: lat, lon, time)\n",
    "    computes the correlation between each couple of grid cells\n",
    "    and fit a correlation model.\n",
    "    Note: negative values for missing data must be set to NaN in advance\n",
    "    ------------------------------------------------------------------------'''\n",
    "    xdata = xdata.dropna(dim='time', how='any')\n",
    "    lats = xdata.lat.values\n",
    "    lons = xdata.lon.values\n",
    "    nlats = np.size(lats)\n",
    "    nlons = np.size(lons)\n",
    "    nelem = nlats*nlons\n",
    "    lats9 = np.repeat(lats, nlons)\n",
    "    lons9 = np.tile(lons, nlats)\n",
    "    ncorr = (nelem)*(nelem - 1)//2\n",
    "    vcorr = np.zeros(ncorr)\n",
    "    vdist = np.zeros(ncorr)\n",
    "    count = 0\n",
    "    for i in range(nelem):\n",
    "        tsi = xdata.loc[dict(lat=lats9[i], lon=lons9[i])].values\n",
    "        tsi = np.maximum(tsi-thresh, 0.0)\n",
    "        for j in range(i+1, nelem):\n",
    "            tsj = xdata.loc[dict(lat=lats9[j], lon=lons9[j])].values\n",
    "            tsj = np.maximum(tsj-thresh, 0.0)\n",
    "            vcorr[count] = corr(tsi, tsj)\n",
    "            vdist[count] = haversine(lats9[i], lats9[j], lons9[i], lons9[j])\n",
    "            count = count + 1\n",
    "    res = {}\n",
    "    res['vdist'] = vdist\n",
    "    res['vcorr'] = vcorr\n",
    "    xx = np.linspace(np.min(vdist), np.max(vdist), 20)\n",
    "    # fit curves to observed correlation\n",
    "    popt, pcov = curve_fit(str_exp_fun, vdist, vcorr, p0=np.array([50, 1]),\n",
    "                bounds=(np.array([0.0, 0.0]), np.array([+np.inf, +np.inf])))\n",
    "    res['d0_s']= popt[0]\n",
    "    res['mu0_s'] = popt[1]\n",
    "    popt1, pcov1 = curve_fit(epl_fun, vdist, vcorr, p0=np.array([50, 1]),\n",
    "                bounds=(np.array([0.0, 0.0]), np.array([+np.inf, +np.inf])))\n",
    "    res['eps_s'] = popt1[0]\n",
    "    res['alp_s'] = popt1[1]\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(vdist, vcorr, 'o', label='empirical')\n",
    "        plt.plot(xx, str_exp_fun(xx, res['d0_s'], res['mu0_s']), 'r',\n",
    "                                            label='Stretched exp.')\n",
    "        plt.plot(xx, epl_fun(xx, res['eps_s'], res['alp_s']), 'g',\n",
    "                                            label='Exp.-power law')\n",
    "        plt.xlabel('distance [Km]')\n",
    "        plt.ylabel('correlation [-]')\n",
    "        plt.ylim([0, 1])\n",
    "        plt.xlim([min(vdist), max(vdist)])\n",
    "        plt.legend()\n",
    "        plt.close()\n",
    "        res['fig'] = fig\n",
    "    return res\n",
    "\n",
    "\n",
    "def nabla_2d(par_acf, myacf, T1, T2, err_min = 1e-2):\n",
    "    '''########################################################################\n",
    "    Compute the variance function as in Vanmarcke's book.\n",
    "    INPUTS::\n",
    "        par_acf = tuple with the parameters of the autocorr function (ACF)\n",
    "        myacf = ACF in 1,2,or 3 dimensions, with parameters in par_acf\n",
    "        T1 = 1st dimension of averaging area\n",
    "        T2 = 2nd dim of the averaging area'''\n",
    "    if (T1 == 0) or (T2 == 0):\n",
    "        print('integration domain is zero')\n",
    "        return 0.0 # integral is zero in this case.\n",
    "    else:\n",
    "        fun_XY = lambda x ,y: (T1 - x ) *(T2 - y ) *myacf(x ,y, par_acf)\n",
    "        myint, myerr = nquad(fun_XY, [[0. ,T1], [0. ,T2]])\n",
    "        # if myint != 0.0:\n",
    "        #     rel_err = myerr /myint\n",
    "        # else:\n",
    "        #     rel_err = 0\n",
    "        # if rel_err > err_min:\n",
    "        #     print('varfun ERROR --> Numeric Integration scheme does not converge')\n",
    "        #     print('int rel error = ', rel_err)\n",
    "        #     sys.exit(\"aargh! there are errors!\") # to stop code execution\n",
    "        return 4.0 * myint\n",
    "\n",
    "\n",
    "def fast_corla_2d(par_acf, myacf, Tx, L, err_min=1e-2):\n",
    "    nab_1 = nabla_2d(par_acf, myacf, L, Tx[0], err_min=err_min)\n",
    "    nab_2 = nabla_2d(par_acf, myacf, L, Tx[1], err_min=err_min)\n",
    "    nab_3 = nabla_2d(par_acf, myacf, L, Tx[2], err_min=err_min)\n",
    "    nab_den = nabla_2d(par_acf, myacf, L, L, err_min=err_min)\n",
    "    if np.abs(nab_den) < 10e-6: # to avoid infinities\n",
    "        # print('correcting - inf value')\n",
    "        nab_den = 10e-6\n",
    "    covla = 2*(nab_1 -2*nab_2 + nab_3)/(4*nab_den)\n",
    "    # print('parhat =', par_acf)\n",
    "    return covla\n",
    "\n",
    "\n",
    "def myfun_sse(xx, yobs, parhat, L, acf = 'mar'):\n",
    "    xx = np.asarray(xx)\n",
    "    myacf = lambda x, y, parhat: myacf_2d(x, y, parhat, acf=acf)\n",
    "    # Ty = np.array([L, 0., L, 0.])\n",
    "    sse = 0\n",
    "    m = np.size(xx)\n",
    "    for ii in range(m):\n",
    "        myx = xx[ii]\n",
    "        Tx = np.array([np.abs(L - myx), myx, L + myx, myx])\n",
    "        # L2 = [L, L]\n",
    "        # res = corla_2d(L2, L2, parhat, myacf, Tx, Ty, err_min=1e-5)\n",
    "        # faster option: - same result (optimized)\n",
    "        res = fast_corla_2d(parhat, myacf, Tx, L, err_min=1e-2)\n",
    "        sse = sse + (res - yobs[ii]) ** 2\n",
    "        # sse = sse + ((res - yobs[ii])/yobs[ii]) ** 2 # norm\n",
    "    # print('sse', sse)\n",
    "    # print('parhat =', parhat)\n",
    "    return sse\n",
    "\n",
    "\n",
    "def bin_ave_corr(vdist, vcorr, toll=0.3, plot=False):\n",
    "    ''' compute block averages to approximate\n",
    "    the empirical correlation function'''\n",
    "    vd = np.sort(vdist)\n",
    "    cd = vcorr[np.argsort(vdist)]\n",
    "    m = np.size(vd)\n",
    "    cluster = np.zeros(m)\n",
    "    count = 0\n",
    "    for i in range(1, m):\n",
    "        if np.abs(vd[i]-vd[i-1]) < toll:\n",
    "            cluster[i] = count\n",
    "        else:\n",
    "            count = count + 1\n",
    "            cluster[i] = count\n",
    "    clust = set(cluster)\n",
    "    nclust = len(clust)\n",
    "    vdist_ave = np.zeros(nclust)\n",
    "    vcorr_ave = np.zeros(nclust)\n",
    "    for ei, elem in enumerate(clust):\n",
    "        di = vd[cluster==elem]\n",
    "        ci = cd[cluster==elem]\n",
    "        vdist_ave[ei] = np.mean(di)\n",
    "        vcorr_ave[ei] = np.mean(ci)\n",
    "    res = {}\n",
    "    res['vdist_ave'] = vdist_ave\n",
    "    res['vcorr_ave'] = vcorr_ave\n",
    "    res['vd'] = vd\n",
    "    res['cd'] = cd\n",
    "    res['cluster']  = cluster\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        plt.scatter(vd, cd, c=cluster, cmap='jet')\n",
    "        plt.scatter(res['vdist_ave'], res['vcorr_ave'], c='k')\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        res['fig'] = fig\n",
    "    return res\n",
    "\n",
    "\n",
    "def down_corr(vdist, vcorr, L1, *, acf='mar',\n",
    "                use_ave=True, opt_method = 'genetic', disp=True, toll=0.005,\n",
    "                plot=False):\n",
    "    '''------------------------------------------------------------------------\n",
    "    Downscale the correlation function obtained from spatial averages\n",
    "    INPUT:\n",
    "        vdist = array of distances [Km]\n",
    "        vcorr = array of correlation values [-]\n",
    "        L1 = linear scale of spatial averaging (e.g., grid cell resolution)\n",
    "        acf = type of acf used (default 'mar'). Can be 'mar' or 'str'\n",
    "        use_ave = True. If true use binned average value of the correlation\n",
    "                    function instead of the  actual values\n",
    "                    (faster and more stable numerically)\n",
    "        method='genetic'. Method used for the optimization.\n",
    "                'genetic' -> for a genetic algorithm (suggested, default)\n",
    "                'lbfgsb' -> for the L-BFGS-B algoithm.\n",
    "                        This is only LOCAL and could get stuck in local minima.\n",
    "        disp= True: display optimization status\n",
    "    ------------------------------------------------------------------------'''\n",
    "    res = {}\n",
    "    parnames = ['eps_d', 'alp_d'] if acf == 'mar' else ['d0_d', 'mu0_d']\n",
    "    if not use_ave:\n",
    "        def myfun(pardown):\n",
    "            return myfun_sse(vdist, vcorr, pardown, L1, acf=acf)\n",
    "    else:\n",
    "        res_ave = bin_ave_corr(vdist, vcorr, toll = 0.3, plot=plot)\n",
    "        dd = res_ave['vdist_ave']\n",
    "        cc = res_ave['vcorr_ave']\n",
    "        def myfun(pardown):\n",
    "            return myfun_sse(dd, cc, pardown, L1, acf=acf)\n",
    "    if opt_method == 'lbfgsb':\n",
    "        x0 = (50, 1)  # initial guess\n",
    "        resmin = minimize(myfun, x0, method=\"L-BFGS-B\",\n",
    "                                bounds=((0, 2000), (0, 10)),\n",
    "                                options={'gtol': 1e-8, 'disp': True})\n",
    "        res[parnames[0]] = resmin.x[0]\n",
    "        res[parnames[1]] = resmin.x[1]\n",
    "        res['success'] = resmin.success\n",
    "        res['fuvval'] = resmin.fun\n",
    "\n",
    "    elif opt_method == 'genetic':\n",
    "        bounds = [(0.0, 200.0),(0.0, 1.00)]\n",
    "        resmin = differential_evolution(myfun, bounds, disp=disp,\n",
    "                                            tol = toll, atol = toll)\n",
    "        res[parnames[0]] = resmin.x[0]\n",
    "        res[parnames[1]] = resmin.x[1]\n",
    "        res['success'] = resmin.success\n",
    "        res['funval'] = resmin.fun\n",
    "    else:\n",
    "        print('down_corr ERROR: please insert a valid optimization method')\n",
    "        print('downscaling not performed')\n",
    "        res[parnames[0]] = np.nan\n",
    "        res[parnames[1]] = np.nan\n",
    "        res['success'] = False\n",
    "        res['funval'] = -9999\n",
    "    if plot:\n",
    "        xx = np.linspace(0.0, 100)\n",
    "        corrL = int_corr(xx, (res[parnames[0]], res[parnames[1]]), acf, L1)\n",
    "        fig = plt.figure()\n",
    "        plt.plot(vdist, vcorr, 'or', label='empirical correlation')\n",
    "        if use_ave:\n",
    "            plt.plot(dd, cc, 'sk', label='binned correlation')\n",
    "        plt.plot(xx, corrL, 'sc', label='integrated correlation')\n",
    "        if acf=='str':\n",
    "            plt.plot(xx, str_exp_fun(xx, res[parnames[0]],\n",
    "                        res[parnames[1]]), 'r', label='Stretched exp.')\n",
    "        else:\n",
    "            plt.plot(xx, epl_fun(xx, res[parnames[0]],\n",
    "                        res[parnames[1]]), 'g', label='Exp.-power law')\n",
    "        plt.xlabel('distance [Km]')\n",
    "        plt.ylabel('correlation [-]')\n",
    "        plt.ylim([0.4, 1])\n",
    "        plt.xlim([0.0, max(100, np.max(vdist))])\n",
    "        plt.legend()\n",
    "        plt.title('Downscaling correlation function')\n",
    "        plt.close()\n",
    "        res['fig'] = fig\n",
    "    return res\n",
    "\n",
    "\n",
    "def int_corr(xx, parhat, acf, L):\n",
    "    '''------------------------------------------------------------------------\n",
    "    compute correlation of Local averages\n",
    "    (inverse than the down_corr function)\n",
    "    ------------------------------------------------------------------------'''\n",
    "    m = np.size(xx)\n",
    "    corrL = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        Tx = np.array([np.abs(L - xx[i]), xx[i], L + xx[i], xx[i]])\n",
    "        myacf = lambda x, y, parhat: myacf_2d(x, y, parhat, acf=acf)\n",
    "        corrL[i] = fast_corla_2d(parhat, myacf, Tx, L, err_min=1e-2)\n",
    "    return corrL\n",
    "\n",
    "\n",
    "def vrf(L, L0, par_acf, acf='mar'):\n",
    "    '''-------------------------------------------------------------\n",
    "    compute the variance reduction factor\n",
    "    between scales L (large) and L0 (small)\n",
    "    defined as Var[L]/Var[L0]\n",
    "    INPUT:\n",
    "        L [km]\n",
    "        L0 [Km]\n",
    "        parhat = tuple with ACF parameters (eg epsilon, alpha)\n",
    "        acf='mar' type of acf (mar or str available)\n",
    "    OUTPUT:\n",
    "        gam, variance reduction factor\n",
    "        ---------------------------------------------------------'''\n",
    "    def myacf(x, y, parhat, acf):\n",
    "        if acf == 'str':  # par d0, mu0\n",
    "            return str_exp_fun(np.sqrt(x ** 2 + y ** 2), parhat[0], parhat[1])\n",
    "        elif acf == 'mar':  # par:: epsilon, alpha\n",
    "            return epl_fun(np.sqrt(x ** 2 + y ** 2), parhat[0], parhat[1])\n",
    "        else:\n",
    "            print('vrf ERROR: insert a valid auto correlation function')\n",
    "    # compute variance reduction factor\n",
    "    fun_XY = lambda x, y: (L - x) * (L - y) * myacf(x, y, par_acf, acf)\n",
    "    fun_XY0 = lambda x, y: (L0 - x) * (L0 - y) * myacf(x, y, par_acf, acf)\n",
    "    # its 2D integral a-la Vanmarcke\n",
    "    int_XY, abserr   = dblquad(fun_XY,  0.0, L,  lambda x: 0.0, lambda x: L)\n",
    "    int_XY0, abserr0 = dblquad(fun_XY0, 0.0, L0, lambda x: 0.0, lambda x: L0)\n",
    "    \n",
    "    gam  = 4/L**4*int_XY # between scale L and a point\n",
    "    # gam = (L0 / L) ** 4 * (int_XY / int_XY0)  # between scales L and L0\n",
    "    \n",
    "    return gam\n",
    "\n",
    "\n",
    "def down_wei(Ns, Cs, Ws, L, L0, beta, par_acf, acf='mar'):\n",
    "    ''' -----------------------------------------------------------------------\n",
    "    Downscale Weibull parameters from grid cell scale to a subgrid scale:\n",
    "    compute the downscaled weibull parameters from a large scale L\n",
    "    to a smaller scale L0, both in Km\n",
    "\n",
    "    INPUT ::\n",
    "        Ns, Cs, Ws (original parmaters at scale L - may be scalars or arrays)\n",
    "        L [km] large scale\n",
    "        L0 [km] small scale\n",
    "        gams = ratio between the mean number of wet days at scales L and L0\n",
    "        (should be larger than 1)\n",
    "        par_acf: set of paramaters for the acf (tuple)\n",
    "\n",
    "    OPTIONAL ARGUMENTS ::\n",
    "        acf: what acf. default is 'str'. others available:\n",
    "            'str'  ->  for 2p stretched exonential (Krajewski 2003)\n",
    "            'mar'  ->  for 2p power law with exp kernel (Marani 2003)\n",
    "\n",
    "        N_bias:: correction to N if comparing satellite and gauges\n",
    "        default is equal to zero (no correction)\n",
    "\n",
    "    OUTPUT::\n",
    "        Nd, Cd, Wd (downscaled parameters)\n",
    "        gam = variance reduction function\n",
    "        fval = function value at the end of numerical minimization\n",
    "    ---------------------------------------------------------------------'''\n",
    "    Ns = np.asarray(Ns)  # check if scalar input - should be the same for N,C,W\n",
    "    Cs = np.asarray(Cs)\n",
    "    Ws = np.asarray(Ws)\n",
    "    # the three parameter mush have same shape - I only check one here\n",
    "    is_scalar = False if Cs.ndim > 0 else True\n",
    "    Ns.shape = (1,) * (1 - Ns.ndim) + Ns.shape\n",
    "    Cs.shape = (1,) * (1 - Cs.ndim) + Cs.shape\n",
    "    Ws.shape = (1,) * (1 - Ws.ndim) + Ws.shape\n",
    "    m = Cs.shape[0]  # length of parameter arrays = number of blocks=\n",
    "    # spatial auto correlation function - epl or str\n",
    "    # def myacf(x, y, parhat, acf):\n",
    "    #     if acf == 'str':  # par d0, mu0\n",
    "    #         return str_exp_fun(np.sqrt(x ** 2 + y ** 2), parhat[0], parhat[1])\n",
    "    #     elif acf == 'mar':  # par:: epsilon, alpha\n",
    "    #         return epl_fun(np.sqrt(x ** 2 + y ** 2), parhat[0], parhat[1])\n",
    "    #     else:\n",
    "    #         print('down_wei WARNING: insert a valid auto correlation function')\n",
    "    # # compute variance reduction factor\n",
    "    # fun_XY = lambda x, y: (L - x) * (L - y) * myacf(x, y, par_acf, acf)\n",
    "    # fun_XY0 = lambda x, y: (L0 - x) * (L0 - y) * myacf(x, y, par_acf, acf)\n",
    "    # # its 2D integral a-la Vanmarcke\n",
    "    # int_XY, abserr   = dblquad(fun_XY,  0.0, L,  lambda x: 0.0, lambda x: L)\n",
    "    # int_XY0, abserr0 = dblquad(fun_XY0, 0.0, L0, lambda x: 0.0, lambda x: L0)\n",
    "    # # gam  = 4/L**4*int_XY # between scale L and a point\n",
    "    # gam = (L0 / L) ** 4 * (int_XY / int_XY0)  # between scales L and L0\n",
    "    gam = vrf(L, L0, par_acf, acf=acf)\n",
    "    # vrf = gam\n",
    "    # prob wet:: correct satellite N adding the average difference\n",
    "    pws = np.mean(Ns) / 365.25\n",
    "    Wd = np.zeros(m)\n",
    "    Cd = np.zeros(m)\n",
    "    Nd = np.zeros(m)\n",
    "    for ii in range(m):\n",
    "        cs = Cs[ii]\n",
    "        ws = Ws[ii]\n",
    "        rhs = 1 / gam / beta * (2 * ws * gamma(2 / ws) / (\n",
    "                                    gamma(1 / ws)) ** 2 + (gam - 1) * pws)\n",
    "        wpfun = lambda w: 2 * w * gamma(2 / w) / (gamma(1 / w)) ** 2 - rhs\n",
    "\n",
    "        res = fsolve(wpfun, 0.1, full_output=True,\n",
    "                        xtol=1e-06, maxfev=10000)\n",
    "        Wd[ii] = res[0]\n",
    "        info = res[1]\n",
    "        fval = info['fvec']\n",
    "        if fval > 1e-5:\n",
    "            print('warning - downscaling function:: '\n",
    "                    'there is something wrong solving fsolve!')\n",
    "        Cd[ii] = beta * Wd[ii] * (cs / ws) * gamma(1 / ws) / gamma(1 / Wd[ii])\n",
    "        Nd[ii] = int( np.rint( Ns[ii] / beta)) #Nd[ii] = np.int( np.rint( Ns[ii] / beta))\n",
    "\n",
    "    # print(Nd)\n",
    "    # print(Cd)\n",
    "    # print(Wd)\n",
    "\n",
    "    Nd = Nd if not is_scalar else Nd[0]\n",
    "    Cd = Cd if not is_scalar else Cd[0]\n",
    "    Wd = Wd if not is_scalar else Wd[0]\n",
    "    return Nd, Cd, Wd, gam, fval\n",
    "\n",
    "\n",
    "\n",
    "def downscale(xdata, Tr, *, thresh=1, L0, acf='mar', dt=3,\n",
    "                plot=False, tscale=24, save_yearly = True, toll=0.005,\n",
    "                maxmiss=40, clat=None, clon=None,\n",
    "                opt_method='genetic'):\n",
    "    '''------------------------------------------------------------------------\n",
    "    Downscale a precipitation dataset contained in the x-array xdata.\n",
    "    xdata must have 3 dimensions, named (lat, lon, time)\n",
    "    and dimensions 'lat' and 'lon' should have equal size,\n",
    "    and possibly have odd length so as to clearly define a central pixel.\n",
    "    INPUT:\n",
    "        xdata = x-array with obs at time scale dt.\n",
    "        NOTE: rainfall amounts must be ACCUMULATIONS, not rainfall RATES!\n",
    "                and the array must already be loaded in memory, not dask!\n",
    "\n",
    "        Tr: return time for computing an axtreme value quantile. Must be scalar\n",
    "\n",
    "        thresh = threshold for defining wet days (default = 1mm)\n",
    "        L0 = scale in [km] to which downscale rainfall statistics\n",
    "                (default is L0 = 0.0001 km = 0.1 m ~ rain gauge obs. scale)\n",
    "\n",
    "        acf: type of autocorrelation function used. Possibilities:\n",
    "                acf='mar' for the power-law described in Marani 2003 (default)\n",
    "                acf='str' for a stretched exponential acf (Villarini et al)\n",
    "\n",
    "        dt: time scale in [hours] of the precipitation dataset.\n",
    "            Default is dt=3 (for TRMM - TMPA - 3b42 dataset)\n",
    "\n",
    "        plot: if true save plots (default is false).\n",
    "\n",
    "        yearly: if true, compute the yearly Weibull parameters\n",
    "                in addition to the global ones.\n",
    "\n",
    "        toll: tolerence for optimization of downscaled correlation.\n",
    "                (default is 0.005)\n",
    "\n",
    "        maxmiss: if computing yearly parameters, do so only for years with\n",
    "                    no more than maxmiss days of missing observations\n",
    "                    (default is 36, ~10% of obs. at the daily time scale)\n",
    "\n",
    "        tscale: timescale at which we want the downscaled statistics [hours]\n",
    "                default is 24 hours = DAILY time scale.\n",
    "                NOTE: METHOD HAS NOT BEEN TESTED AT DIFFERENT TIME SCALES.\n",
    "\n",
    "        clat, clon: if provided, they determine the central pixel\n",
    "                    in the arraay. If not provided, the 'central'\n",
    "                    pixel is selected by default.\n",
    "\n",
    "        method = optimization method for downscaling the correlation\n",
    "                    (lbfgsb, genetics or none). Default is 'genetics'.\n",
    "                    If none, no correlation downscaling is performed, return nan\n",
    "\n",
    "    OUTPUT: Dictionary with the following quantities:\n",
    "        gamma -> variance reduction function between the two scales\n",
    "        beta  -> wet fraction reduction function between the two scale\n",
    "\n",
    "        Nd Global Downscaled parameters\n",
    "        Cd\n",
    "        Wd\n",
    "        Ns Global grid cell scale parameters\n",
    "        Cs\n",
    "        Ws\n",
    "\n",
    "        NYd Yearly downscaled parameters (only if yearly = True)\n",
    "        CYd\n",
    "        WYd\n",
    "        NYs Yearly grid cell scale parameters (only if yearly = True)\n",
    "        CYs\n",
    "        WYs\n",
    "\n",
    "        eps_s, alp_s, (or d0_s and mu0_s): parameters of the correlation\n",
    "                                            function at the pixel scale\n",
    "\n",
    "        eps_d, alp_d, (or d0_d and mu0_d): parameters of the correlation\n",
    "                                            function downscaled at scale L0\n",
    "\n",
    "        corr_down_success -> True if optimization successfun for correlation.\n",
    "        corr_down_funval -> final value of function minimized to\n",
    "                            downscale the correlation function.\n",
    "        w_down_funval -> final value of function minimized to downscale w.\n",
    "        thresh -> threshold used in the analysis\n",
    "        (only if plot==True):\n",
    "        Taylor_contour -> nice plot with the wet fraction contour\n",
    "        corr_plot -> nice plot with the correlation function\n",
    "    ------------------------------------------------------------------------'''\n",
    "    res = {} # initialize dictionary for storing results\n",
    "    xdata = xdata.where(xdata >= -0.001) # set negative values to np.nan if any\n",
    "    xdaily0 = xdata.resample(time ='{}h'.format(tscale)).sum(dim='time', skipna=False)\n",
    "    xdaily = xdaily0.dropna(dim='time', how='any')\n",
    "    lons = xdata.lon.values\n",
    "    lats = xdata.lat.values\n",
    "    nlon = np.size(lons)\n",
    "    nlat = np.size(lats)\n",
    "    dx = np.abs(lons[1] - lons[0])\n",
    "    if nlon != nlat:\n",
    "        print('downscale warning: box sizes are not equal')\n",
    "    if nlon % 2 == 0:\n",
    "        print('downscale warning: at least one box size has even length')\n",
    "    if (bool(clat) and bool(clon) and clat in lats and clon in lons):\n",
    "        clat = lats[np.argmin(np.abs(clat - lats))]\n",
    "        clon = lons[np.argmin(np.abs(clon - lons))]\n",
    "        # otherwise us the one provided by the\n",
    "    else:\n",
    "        clat = lats[np.argmin(np.abs(np.mean(lats) - lats))]\n",
    "        clon = lons[np.argmin(np.abs(np.mean(lons) - lons))]\n",
    "    L1 = area_lat_long(clat, clon, dx, dx)[0] # in Km\n",
    "    # get the pixel closer to the center as central pixel:\n",
    "    # get the time series for the central pixel\n",
    "    tsc = xdaily.loc[dict(lat = clat, lon = clon)]\n",
    "\n",
    "    # c_excesses = np.maximum(tsc.values-thresh, 0.0)\n",
    "    c_excesses = tsc.values[tsc.values > thresh] - thresh\n",
    "    \n",
    "    NCW = wei_fit(c_excesses)\n",
    "    pws = NCW[0]/xdaily.shape[2]\n",
    "    # pws = NCW[0]/xdaily['PRE'].shape[2]\n",
    "    Ns = int(np.floor(pws*365.25)) #Ns = np.int(np.floor(pws*365.25))\n",
    "    Cs = NCW[1]\n",
    "    Ws = NCW[2]\n",
    "\n",
    "    # Taylor Hypothesis for downscaling intermittency\n",
    "    print('Downscaling Intermittency')\n",
    "    taylor = downscale_pwet(xdata, thresh=thresh, dt=dt, L1=L1,\n",
    "                    target_x=L0, target_t=tscale,\n",
    "                    origin_x=L1, origin_t=tscale, ninterp=1000, plot=plot)\n",
    "\n",
    "    print('Downscaling the correlation')\n",
    "    parnames = ['eps', 'alp'] if acf == 'mar' else ['d0', 'mu0']\n",
    "\n",
    "    # Correlation downscaling\n",
    "    print('Computing the correlation')\n",
    "    rcorr = grid_corr(xdaily, plot=plot, thresh=thresh)\n",
    "    gam_s = vrf(L1, L0, (rcorr['{}_s'.format(parnames[0])],\n",
    "                            rcorr['{}_s'.format(parnames[1])]), acf=acf)\n",
    "\n",
    "    dcorr =  down_corr(rcorr['vdist'], rcorr['vcorr'], L1, acf=acf,\n",
    "                        use_ave=True, opt_method=opt_method, toll=toll,\n",
    "                        plot=plot)\n",
    "\n",
    "    # downscaling the Weibull pdf\n",
    "    print('Downscaling pdf - global Weibull parameters')\n",
    "    par_acf = (dcorr['{}_d'.format(parnames[0])], dcorr['{}_d'.format(parnames[1])])\n",
    "\n",
    "    Nd, Cd, Wd, gam_d, fval_w = down_wei(Ns, Cs, Ws, L1, L0, taylor['beta'], par_acf, acf=acf)\n",
    "\n",
    "    print('Downscaling pdf - yearly Weibull parameters')\n",
    "    NCWy, YEARSy = fit_yearly_weibull(tsc, thresh=thresh, maxmiss=maxmiss)\n",
    "    NYd, CYd, WYd, _, _ = down_wei(NCWy[:,0], NCWy[:,1], NCWy[:,2], L1, L0, taylor['beta'], par_acf, acf=acf)\n",
    "\n",
    "    if save_yearly:\n",
    "        res['NYs'] = NCWy[:,0] # yearly Weibull parameters\n",
    "        res['CYs'] = NCWy[:,1] # yearly Weibull parameters\n",
    "        res['WYs'] = NCWy[:,2] # yearly Weibull parameters\n",
    "        res['NYd'] = NYd # Nd, Cd, Wd (downscaled parameters)\n",
    "        res['CYd'] = CYd # Nd, Cd, Wd (downscaled parameters)\n",
    "        res['WYd'] = WYd # Nd, Cd, Wd (downscaled parameters)\n",
    "\n",
    "    # estimate some extreme quantiles with MEVD\n",
    "    # Tr: return time for computing an axtreme value quantile. Must be scalar\n",
    "    # Tr = np.array([10, 20, 50, 100]) # pass\n",
    "    Fi = 1-1/Tr\n",
    "    res['Tr'] = Tr\n",
    "    # x0 = 150.0\n",
    "    x0 = 9.0*np.mean(CYd)\n",
    "    res['mev_d'] = mev_quant(Fi, x0, NYd, CYd, WYd, thresh=thresh)[0] # Computes the MEV quantile for given non exceedance probability\n",
    "    res['mev_s'] = mev_quant(Fi, x0, NCWy[:,0], NCWy[:,1], NCWy[:,2],thresh=thresh)[0] # Computes the MEV quantile for given non exceedance probability\n",
    "\n",
    "    res['YEARS'] = YEARSy # Add for Arturo\n",
    "\n",
    "    res['gam_d'] = gam_d # variance reduction function downscale\n",
    "    res['gam_s'] = gam_s # variance reduction factor\n",
    "    res['beta'] = taylor['beta'] # ratio between pwet at the grid cell scale () to pwet at the target subgrid scale ()\n",
    "    res['Nd'] = Nd # Nd, Cd, Wd (downscaled parameters)\n",
    "    res['Cd'] = Cd # Nd, Cd, Wd (downscaled parameters)\n",
    "    res['Wd'] = Wd # Nd, Cd, Wd (downscaled parameters)\n",
    "    res['Ns'] = Ns # global Weibull parameters\n",
    "    res['Cs'] = Cs # global Weibull parameters\n",
    "    res['Ws'] = Ws # global Weibull parameters\n",
    "    res['{}_s'.format(parnames[0])] = rcorr['{}_s'.format(parnames[0])]\n",
    "    res['{}_s'.format(parnames[1])] = rcorr['{}_s'.format(parnames[1])]\n",
    "    res['{}_d'.format(parnames[0])] = dcorr['{}_d'.format(parnames[0])]\n",
    "    res['{}_d'.format(parnames[1])] = dcorr['{}_d'.format(parnames[1])]\n",
    "    res['corr_down_success'] =        dcorr['success']\n",
    "    res['corr_down_funval'] =         dcorr['funval']\n",
    "    res['w_down_funval'] = fval_w[0] # function value at the end of numerical minimization\n",
    "    res['thresh'] = thresh # threshold used in the analysis\n",
    "    res['clat'] = clat\n",
    "    res['clon'] = clon\n",
    "    if plot:\n",
    "        res['corr_plot'] = dcorr['fig']\n",
    "        res['Taylor_contour'] = taylor['contour']\n",
    "    return res\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "\n",
    "###################### EXTREME VALUE ANALYSIS FUNCTIONS ########################\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def gev_fit_lmom(sample):\n",
    "    ''' Fit GEV distribution to a sample of annual maxima\n",
    "    by means of LMOM technique (Hosking 1990 &co0)\n",
    "    maxima must be numpy column array\n",
    "    return parhat = (csi, psi, mu) i.e., (GEV shape scale location)\n",
    "    rem here csi > 0 --> Heavy tailed distribution'''\n",
    "    sample = np.asarray(sample)\n",
    "    n = np.size(sample)\n",
    "    x = np.sort(sample, axis=0)\n",
    "    b0 = np.sum(x) / n\n",
    "    b1 = 0.0\n",
    "    for j in range(0, n):  # skip first element\n",
    "        jj = j + 1  # real index\n",
    "        b1 = b1 + (jj - 1) / (n - 1) * x[j]\n",
    "    b1 = b1 / n\n",
    "    b2 = 0.0\n",
    "    for j in range(0, n):  # skip first two elements\n",
    "        jj = j + 1  # real\n",
    "        b2 = b2 + (jj - 1) * (jj - 2) / (n - 1) / (n - 2) * x[j]\n",
    "    b2 = b2 / n\n",
    "    # L MOMENTS - linear combinations of PWMs\n",
    "    L1 = b0\n",
    "    L2 = 2 * b1 - b0\n",
    "    L3 = 6 * b2 - 6 * b1 + b0\n",
    "    t3 = L3 / L2  # L skewness\n",
    "    # GEV parameters from L moments ( Hoskins 1990)\n",
    "    # using Hoskins (1985) approximation for computing k:\n",
    "    c = 2 / (3 + t3) - np.log(2) / np.log(3)\n",
    "    k = 7.8590 * c + 2.9554 * c ** 2\n",
    "    csi = -k  # ususal shape\n",
    "    psi = L2 * k / ((1 - 2 ** (-k)) * gamma(1 + k))  # scale\n",
    "    mu = L1 - psi * (1 - gamma(1 + k)) / k  # location\n",
    "    parhat = (csi, psi, mu)\n",
    "    return parhat\n",
    "\n",
    "\n",
    "def gev_quant(Fi, csi, psi, mu):\n",
    "    ''' compute GEV quantile q for given non exceedance probabilities in Fi\n",
    "    with parameters csi, psi, mu (shape, scale, location)\n",
    "    optional: if ci = True also produce the upper and lower confidence\n",
    "    intervals obtained under the hyp of normal distribution.\n",
    "    In this case the covariance matrix of the parameters must be provided\n",
    "    varcov = variance-covariance matrix of parameters.'''\n",
    "    Fi = np.asarray(Fi)\n",
    "    is_scalar = False if Fi.ndim > 0 else True\n",
    "    Fi.shape = (1,) * (1 - Fi.ndim) + Fi.shape\n",
    "    q = mu + psi / csi * ((-np.log(Fi)) ** (-csi) - 1)\n",
    "    q = q if not is_scalar else q[0]\n",
    "    return q\n",
    "\n",
    "\n",
    "def tab_rain_max(df):\n",
    "    '''--------------------------------------------------------------------------\n",
    "    arguments: df, pandas data frame with fields YEAR, PRCP\n",
    "    returns:\n",
    "    XI -> array of annual maxima (ranked in ascending order)\n",
    "    Fi -> Weibull plotting position estimate of their non exceedance probability\n",
    "    TR -> their relative return times\n",
    "    Default using Weibull plotting position for non exceedance probability\n",
    "    -----------------------------------------------------------------------------'''\n",
    "    years_all  = df['YEAR']\n",
    "    years      = np.unique(years_all)\n",
    "    nyears     = np.size(years)\n",
    "    maxima     = np.zeros(nyears)\n",
    "    for jj in range(nyears):\n",
    "        my_year      = df.PRCP[df['YEAR'] == years[jj]]\n",
    "        maxima[jj]   = np.max(my_year)\n",
    "    XI         = np.sort(maxima, axis = 0) # default ascend\n",
    "    Fi         = np.arange(1,nyears+1)/(nyears + 1)\n",
    "    TR         = 1/(1 - Fi)\n",
    "    return XI,Fi,TR\n",
    "\n",
    "\n",
    "def remove_missing_years(df, nmin):\n",
    "    '''\n",
    "    # input has to be a pandas data frame df\n",
    "    # including the variables YEAR, PRCP\n",
    "    # returns the same dataset after removing all years with less of nmin\n",
    "    days of data missing\n",
    "    # (accounts for missing entries, negative values)\n",
    "    # the number of years remaining (nyears2)\n",
    "    # and the original number of years (nyears1)\n",
    "    '''\n",
    "    years_all  = df['YEAR']\n",
    "    years      = pd.Series.unique(years_all)\n",
    "    nyears1    = np.size(years)\n",
    "    for jj in range(nyears1):\n",
    "        dfjj      = df[ df['YEAR'] == years[jj] ]\n",
    "        my_year   = dfjj.PRCP[ dfjj['PRCP'] >= 0 ] # remove -9999 V\n",
    "        my_year2  = my_year[ np.isfinite(my_year) ] # remove  nans - infs V\n",
    "        my_length = len(my_year2)\n",
    "        if my_length < 366-nmin:\n",
    "            df    = df[df.YEAR != years[jj]] # remove this year from the data frame\n",
    "    # then remove NaNs and -9999 from the record\n",
    "    # df.dropna(subset=['PRCP'], inplace = True)\n",
    "    df = df.dropna(subset=['PRCP'])\n",
    "    # df = df.ix[df['PRCP'] >= 0]\n",
    "    df = df[df['PRCP'] >= 0].copy()\n",
    "    # check how many years remain\n",
    "    years_all_2 = df['YEAR']\n",
    "    nyears2 = np.size(pd.Series.unique(years_all_2))\n",
    "    return df, nyears2, nyears1\n",
    "\n",
    "\n",
    "def wei_fit(sample):\n",
    "    ''' fit a 2-parameters Weibull distribution to a sample\n",
    "    by means of Probability Weighted Moments (PWM) matching (Greenwood 1979)\n",
    "    using only observations larger than a value 'threshold' are used for the fit\n",
    "    -- threshold without renormalization -- it assumes the values below are\n",
    "    not present. Default threshold = 0\n",
    "    INPUT:: sample (array with observations)\n",
    "            threshold (default is = 0)\n",
    "    OUTPUT::\n",
    "    returns dimension of the sample (n) (only values above threshold)\n",
    "    N represent the number of observations > threshold\n",
    "    Weibull scale (c) and shape (w) parameters '''\n",
    "    sample = np.asarray(sample) # from list to Numpy array\n",
    "    wets   = sample[sample > 0.0]\n",
    "    x      = np.sort(wets) # sort ascend by default\n",
    "    M0hat  = np.mean(x)\n",
    "    M1hat  = 0.0\n",
    "    n      = x.size # sample size\n",
    "    for ii in range(n):\n",
    "        real_ii = ii + 1\n",
    "        M1hat   = M1hat + x[ii]*(n - real_ii)\n",
    "    M1hat = M1hat/(n*(n-1))\n",
    "    c     = M0hat/gamma( np.log(M0hat/M1hat)/np.log(2)) # scale par\n",
    "    w     = np.log(2)/np.log(M0hat/(2*M1hat)) # shape par\n",
    "    return  n, c, w\n",
    "\n",
    "\n",
    "def mev_fit(df, thresh=1):\n",
    "    '''--------------------------------------------------------------------\n",
    "    fit MEV to a dataframe of daily rainfall observations df - with PRCP, YEAR\n",
    "    years with too many missing data must already have been removed\n",
    "    -----------------------------------------------------------------'''\n",
    "    years   = np.unique(df.YEAR)\n",
    "    nyears  = np.size(years)\n",
    "    N = np.zeros(nyears)\n",
    "    C = np.zeros(nyears)\n",
    "    W = np.zeros(nyears)\n",
    "    for iy, year in enumerate(years):\n",
    "        sample = df['PRCP'].values[df['YEAR']==year]\n",
    "        excesses = sample[sample > thresh] - thresh\n",
    "        Ni = np.size(excesses)\n",
    "        if Ni == 0:\n",
    "            N[iy] = 0\n",
    "            C[iy] = 1e-9\n",
    "            W[iy] = 1.0\n",
    "        elif Ni == 1:\n",
    "            N[iy] = 1\n",
    "            W[iy] = 0.7\n",
    "            C[iy] = excesses[0]/gamma(1 + 1/W[iy])\n",
    "        else:\n",
    "            N[iy], C[iy], W[iy] = wei_fit(excesses)\n",
    "    return N,C,W\n",
    "\n",
    "\n",
    "def mev_fun(y, pr, N, C, W):\n",
    "    ''' MEV distribution function, to minimize numerically\n",
    "    for computing quantiles\n",
    "    Updated version, to include accounting for dry years with 0 events'''\n",
    "    nyears = N.size\n",
    "    # mev0f = numzero + np.sum( ( 1-np.exp(-(y/Cn)**Wn ))**Nn) - nyears*pr\n",
    "    mev0f = np.sum( ( 1-np.exp(-(y/C)**W ))**N) - nyears*pr\n",
    "    return mev0f\n",
    "\n",
    "\n",
    "def mev_quant(Fi, x0, N, C, W, thresh=1):\n",
    "    '''--------------------------------------------------------------------\n",
    "    computes the MEV quantile for given non exceedance prob. in Fi\n",
    "    arguments:\n",
    "    Fi: non exceedance probability (either scalar or array of values)\n",
    "    x0: starting guess for numerical solution\n",
    "    N, C, W: Yearly parameters of MEV distribution\n",
    "    potmode: if True, considers the distributions of value above threshold (default is False)\n",
    "    (In practice if potmode=True, the distribution of excesses over threshold is computed\n",
    "    and then from it the cdf is computed for the effective quantile = quant - thresh)\n",
    "    thresh: threshold for defining ordinary events (default is zero)\n",
    "    returns:\n",
    "    quant -> single quantile, or array of quantiles\n",
    "    flags -> flag = 0 if everything is ok, = 1 if convergence problems\n",
    "    when It happens, a different x0 should be used.\n",
    "    ---------------------------------------------------------------------'''\n",
    "    Fi = np.asarray(Fi)\n",
    "    is_scalar = False if Fi.ndim > 0 else True\n",
    "    Fi.shape = (1,)*(1-Fi.ndim) + Fi.shape\n",
    "    m = np.size(Fi)\n",
    "    quant = np.zeros(m)\n",
    "    flags = np.zeros((m), dtype = bool) # flag for the convergence of numerical solver\n",
    "    for ii in range(m):\n",
    "        myfun     = lambda y: mev_fun(y,Fi[ii],N,C,W)\n",
    "        res       = sc.optimize.fsolve(myfun, x0, full_output = True)\n",
    "        quant[ii] = res[0]\n",
    "        info      = res[1]\n",
    "        fval      = info['fvec']\n",
    "        if fval > 1e-5:\n",
    "            print('mevd_quant:: ERROR - fsolve does not work -  change x0')\n",
    "            flags[ii] = 1\n",
    "    quant = quant + thresh\n",
    "    quant  = quant if not is_scalar else quant[0]\n",
    "    flags  = flags if not is_scalar else flags[0]\n",
    "    return quant, flags\n",
    "\n",
    "\n",
    "def fit_yearly_weibull(xdata, thresh=1, maxmiss=36):\n",
    "    nobsmin = 366 - maxmiss\n",
    "    yearsall = xdata.time.dt.year.values\n",
    "    years = np.unique(yearsall)\n",
    "    nyears0 = np.size(years)\n",
    "    NCW = np.zeros((nyears0, 3))\n",
    "    NOBS = np.zeros(nyears0)\n",
    "    YEARS = np.zeros(nyears0)\n",
    "    for i in range(nyears0):\n",
    "        sample = xdata.sel(time=str(years[i]))\n",
    "        NOBS[i] = np.size(sample)\n",
    "        excesses = sample[sample > thresh] - thresh\n",
    "        Ni = np.size(excesses)\n",
    "        if Ni == 0:\n",
    "            # NCW[i, :] = np.array([0, np.nan, np.nan])\n",
    "            NCW[i, :] = np.array([0, 1E-9, 1.0])\n",
    "        elif Ni == 1:\n",
    "            what = 0.7 # Prior belief on shape parameter\n",
    "            chat = excesses[0]/gamma(1 + 1/what)\n",
    "            NCW[i, :] = np.array([1.0, chat, what])\n",
    "        else:\n",
    "            NCW[i,:] = wei_fit(excesses)\n",
    "        YEARS[i] = years[i]\n",
    "    cond = NOBS > nobsmin\n",
    "    NCW = NCW[cond]\n",
    "    YEARS = YEARS[cond]\n",
    "    return NCW, YEARS\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
